@online{262588213843476FalsehoodsProgrammersBelieve,
  title = {Falsehoods Programmers Believe about Time, in a Single List},
  author = {262588213843476},
  url = {https://gist.github.com/timvisee/fcda9bbdff88d45cc9061606b4b923ca},
  urldate = {2022-06-16},
  abstract = {Falsehoods programmers believe about time, in a single list - falsehoods-programming-time-list.md},
  langid = {english},
  organization = {{Gist}},
  file = {C:\Users\ron\Zotero\storage\TGBBFZ3H\fcda9bbdff88d45cc9061606b4b923ca.html}
}

@incollection{abowdBetterUnderstandingContext1999,
  title = {Towards a Better Understanding of Context and Context-Awareness},
  booktitle = {Handheld and {{Ubiquitous Computing}}},
  author = {Abowd, G. and Dey, A. and Brown, P. and Davies, N. and Smith, M. and Steggles, P.},
  date = {1999},
  pages = {304--307},
  publisher = {{Springer}},
  url = {https://link.springer.com/book/10.1007/3-540-48157-5},
  isbn = {978-3-540-48157-7}
}

@article{abrialFaultlessSystemsYes2009,
  title = {Faultless {{Systems}}: {{Yes We Can}}!},
  shorttitle = {Faultless {{Systems}}},
  author = {Abrial, Jean-Raymond},
  date = {2009-09},
  journaltitle = {Computer},
  volume = {42},
  number = {9},
  pages = {30--36},
  issn = {1558-0814},
  doi = {10.1109/MC.2009.283},
  abstract = {This paper presents simple ideas that offer suggestions on how to improve the situation of computerized system development. Gradually introducing some simple features will eventually result in a global improvement in the software development.},
  eventtitle = {Computer},
  keywords = {Faultless software systems,Programming,Proofs,Refinement,Software development},
  file = {C\:\\Users\\ron\\Zotero\\storage\\9XIVT23U\\Abrial - 2009 - Faultless Systems Yes We Can!.pdf;C\:\\Users\\ron\\Zotero\\storage\\GANB82FX\\stamp.html}
}

@book{abrialModelingEventBSystem2010,
  title = {Modeling in {{Event-B}}: System and Software Engineering},
  author = {Abrial, Jean-Raymond},
  date = {2010},
  publisher = {{Cambridge University Press}},
  isbn = {978-0-521-89556-9}
}

@book{adamsElvishKlingonExploring2011,
  title = {From {{Elvish}} to {{Klingon}}: Exploring Invented Languages},
  shorttitle = {From {{Elvish}} to {{Klingon}}},
  author = {Adams, Michael},
  date = {2011},
  publisher = {{Oxford University Press}},
  location = {{Oxford ; New York}},
  isbn = {978-0-19-280709-0},
  pagetotal = {294},
  keywords = {{Languages, Artificial}},
  annotation = {OCLC: ocn713186702}
}

@inbook{adgerThreeConlangProjects2020,
  title = {Three Conlang Projects at Three Educational Levels},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Adger, David and Urk, Coppe Van},
  date = {2020-08-12},
  pages = {49--68},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0005},
  url = {https://academic.oup.com/book/31973/chapter/267717122},
  urldate = {2023-08-24},
  abstract = {This chapter reports on three distinct implementations of conlang projects: one for elementary school children aged 5–10 that was developed with the Centre for Literacy in Primary Education, one as a one-week summer program for secondary students aged about 15, as part of Queen Mary University’s Widening Participation initiative, and one for university students at Queen Mary College that was based on Adger’s experience creating languages for a television series. For each project, the development process, learning outcomes, and project mechanics are described. The projects vary considerably in structure and focus, and are shown to benefit students at all educational levels.},
  bookauthor = {Adger, David and Urk, Coppe Van},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@inproceedings{agrawalaModelsMemoryScheduling1975,
  title = {Models of Memory Scheduling},
  booktitle = {Proceedings of the Fifth {{ACM}} Symposium on {{Operating}} Systems Principles},
  author = {Agrawala, A. K. and Bryant, R. M.},
  date = {1975-11-01},
  series = {{{SOSP}} '75},
  pages = {217--222},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/800213.806540},
  url = {https://doi.org/10.1145/800213.806540},
  urldate = {2022-06-23},
  abstract = {Queueing theoretic models of single and multi-processor computer systems have received wide attention in the computer science literature. Few of these models consider the effect of finite memory size of a machine and its impact on the memory scheduling problem. In an effort to formulate an analytical model for memory scheduling we propose four simple models and examine their characteristics using simulation. In this paper, we discuss some interesting results of these simulations.},
  isbn = {978-1-4503-7863-5},
  keywords = {Analytical models,Computer system simulation,Dynamic memory allocation,First-fit,Memory fragmentation,Multiprogramming,Performance evaluation,Scheduling,Swapping systems},
  file = {C:\Users\ron\Zotero\storage\XP5UNCBW\800213.806540.pdf}
}

@inproceedings{al-hamouriMeasuringImpactsVirtualization2020,
  title = {Measuring the {{Impacts}} of {{Virtualization}} on the {{Performance}} of {{Thread-Based Applications}}},
  booktitle = {2020 {{Seventh International Conference}} on {{Software Defined Systems}} ({{SDS}})},
  author = {Al-hamouri, Rahaf and Al-Jarrah, Heba and Al-Sharif, Ziad A. and Jararweh, Yaser},
  date = {2020-04},
  pages = {131--138},
  doi = {10.1109/SDS49854.2020.9143884},
  abstract = {The rapid advancements in computer hardware and software facilitated the ability for conventional users and developers to use various virtualization techniques on their common personal computers. For example, users and developers are often using virtualization technologies such as VirtualBox, VMware, Docker, and WSL. These technologies enabled the ability to develop, test, and deploy software across various platforms and computing environments, each of which ensures isolation and security, but may often sacrifice performance due to the extra layer of communication. Thus, this paper presents a performance-based evaluation for thread-based applications that are hosted on different virtualization frameworks. Therefore, it evaluates the effects of virtualization techniques on sequential and multithreaded applications. The performance of the same application is measured on different environments, some of which are provided by a virtual machine that is powered by VirtualBox, while the others are contained in a lightweight virtualization that is provided by the WSL. This paper uses the same application on all environments and presents a performance wise comparison.},
  eventtitle = {2020 {{Seventh International Conference}} on {{Software Defined Systems}} ({{SDS}})},
  keywords = {Container,Containers,Hardware,Instruction sets,Linux,Microsoft Windows,Threads,Virtual Machine,Virtualization,Windows Subsystem for Linux},
  file = {C:\Users\ron\Zotero\storage\EVWIKXPH\Measuring_the_Impacts_of_Virtualization_on_the_Performance_of_Thread-Based_Applications.pdf}
}

@inproceedings{al-zawahrehProceduralModelRequirements2015,
  title = {Procedural {{Model}} of {{Requirements Elicitation Techniques}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Intelligent Information Processing}}, {{Security}} and {{Advanced Communication}}},
  author = {Al-Zawahreh, Hanan and Almakadmeh, Khaled},
  date = {2015-11-23},
  series = {{{IPAC}} '15},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2816839.2816902},
  url = {https://doi.org/10.1145/2816839.2816902},
  urldate = {2022-10-22},
  abstract = {Software requirement elicitation is one of the most sensitive phases in software requirements knowledge area; it extracts useful requirement and help software engineers to select the right elicitation techniques. This paper presents a proposed elicitation model to facilitate the selection requirement elicitation techniques on the basis of the previous published work in academia, including the combination of situational characteristics, in addition technique characteristics, project characteristic and use guidelines to avoid misunderstanding of the requirements.},
  isbn = {978-1-4503-3458-7},
  keywords = {Elicitation Techniques,Elicitation Techniques Selection,Requirement Engineering,Requirements Elicitation Process},
  file = {C:\Users\ron\Zotero\storage\QB8I9LI2\Al-Zawahreh and Almakadmeh - 2015 - Procedural Model of Requirements Elicitation Techn.pdf}
}

@inproceedings{alenaziNovelApproachTracing2020,
  title = {A Novel Approach to Tracing Safety Requirements and State-Based Design Models},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}},
  author = {Alenazi, Mounifah and Niu, Nan and Savolainen, Juha},
  date = {2020-06-27},
  series = {{{ICSE}} '20},
  pages = {848--860},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3377811.3380332},
  url = {https://doi.org/10.1145/3377811.3380332},
  urldate = {2022-10-22},
  abstract = {Traceability plays an essential role in assuring that software and systems are safe to use. Automated requirements traceability faces the low precision challenge due to a large number of false positives being returned and mingled with the true links. To overcome this challenge, we present a mutation-driven method built on the novel idea of proactively creating many seemingly correct tracing targets (i.e., mutants of a state machine diagram), and then exploiting model checking within process mining to automatically verify whether the safety requirement's properties hold in the mutants. A mutant is killed if its model checking fails; otherwise, it is survived. We leverage the underlying killed-survived distinction, and develop a correlation analysis procedure to identify the traceability links. Experimental evaluation results on two automotive systems with 27 safety requirements show considerable precision improvements compared with the state-of-the-art.},
  isbn = {978-1-4503-7121-6},
  keywords = {mutation analysis,process mining,requirements engineering,systems modeling language (SysML),traceability},
  file = {C:\Users\ron\Zotero\storage\PPG6YLIS\Alenazi et al. - 2020 - A novel approach to tracing safety requirements an.pdf}
}

@article{allmanViewingOpenSource2003,
  title = {Viewing {{Open Source}} with an {{Open Mind}}: {{Most}} Writing about Open Source Stresses the Goodness (or Lack of Goodness) of Open Source as a Software Development Model.},
  shorttitle = {Viewing {{Open Source}} with an {{Open Mind}}},
  author = {Allman, Eric and McKusick, Kirk},
  date = {2003-07},
  journaltitle = {Queue},
  shortjournal = {Queue},
  volume = {1},
  number = {5},
  pages = {6--7},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/945074.945128},
  url = {https://dl.acm.org/doi/10.1145/945074.945128},
  urldate = {2022-05-23},
  abstract = {Some of the more interesting articles discussing open source have been from the economic point of view. Since the dismal science of economics sees nothing as purely good or bad, these types of pieces generally make for interesting reading, in turn leading you to think a bit more carefully about the possibility that open source might be good for some things but less good for others.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\C99KIDXP\Allman and McKusick - 2003 - Viewing Open Source with an Open Mind Most writin.pdf}
}

@inproceedings{alrajehAdaptingRequirementsModels2020,
  title = {Adapting Requirements Models to Varying Environments},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}},
  author = {Alrajeh, Dalal and Cailliau, Antoine and van Lamsweerde, Axel},
  options = {useprefix=true},
  date = {2020-06-27},
  series = {{{ICSE}} '20},
  pages = {50--61},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3377811.3380927},
  url = {https://doi.org/10.1145/3377811.3380927},
  urldate = {2022-10-22},
  abstract = {The engineering of high-quality software requirements generally relies on properties and assumptions about the environment in which the software-to-be has to operate. Such properties and assumptions, referred to as environment conditions in this paper, are highly subject to change over time or from one software variant to another. As a consequence, the requirements engineered for a specific set of environment conditions may no longer be adequate, complete and consistent for another set. The paper addresses this problem through a tool-supported requirements adaptation technique. A goal-oriented requirements modelling framework is considered to make requirements' refinements and dependencies on environment conditions explicit. When environment conditions change, an adapted goal model is computed that is correct with respect to the new environment conditions. The space of possible adaptations is not fixed a priori; the required changes are expected to meet one or more environment-independent goal(s) to be satisfied in any version of the system. The adapted goal model is generated using a new counterexample-guided learning procedure that ensures the correctness of the updated goal model, and prefers more local adaptations and more similar goal models.},
  isbn = {978-1-4503-7121-6},
  keywords = {context-dependent requirements,formal verification,logic-based learning,requirements adaptation,requirements evolution},
  file = {C:\Users\ron\Zotero\storage\GI6R7LG2\Alrajeh et al. - 2020 - Adapting requirements models to varying environmen.pdf}
}

@inproceedings{alshareefValidationFrameworkAspectual2020,
  title = {Validation {{Framework}} for {{Aspectual Requirements Engineering}} ({{ValFAR}})},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Engineering}} \& {{MIS}} 2020},
  author = {Alshareef, Sohil F. and Maatuk, Abdelsalam M. and Abdelaziz, Tawfig M. and Hagal, Mohammed},
  date = {2020-09-14},
  series = {{{ICEMIS}}'20},
  pages = {1--7},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3410352.3410777},
  url = {https://doi.org/10.1145/3410352.3410777},
  urldate = {2022-10-22},
  abstract = {Aspect-Oriented Requirements Engineering (AORE) extends the existing requirements engineering approaches to support the identification and handling of crosscutting concerns. Crosscutting concerns are considered as potential aspects and can lead to the phenomenal "tyranny of the dominant decomposition". Requirements-level aspects are responsible for producing scattered and tangled descriptions of requirements in the requirements document. Requirements validation artifact is an essential task in software development. This task ensures that requirements are correct and valid in terms of completeness and consistency, hence, reducing the development cost, maintenance and establish an approximately correct estimate of effort and completion time of the project. In this paper, we present a validation framework for aspectual requirements that can be used with AORE approaches to facilitate the validation of the resulting crosscutting relationships and aspects. The proposed framework comprises a high-level and low-level validation. The high-level validation is to validate the concerns with stakeholders, whereas the low-level validation validates the aspectual requirement by developers using a checklist. The approach has been evaluated using a case study. The results demonstrate that the proposed framework is feasible and acceptable.},
  isbn = {978-1-4503-7736-2},
  keywords = {AORE,aspectual requirements,crosscutting concern,requirements engineering,validation and verification},
  file = {C:\Users\ron\Zotero\storage\R8DRQLZ3\Alshareef et al. - 2020 - Validation Framework for Aspectual Requirements En.pdf}
}

@article{alurAlgorithmicAnalysisHybrid1995,
  title = {The Algorithmic Analysis of Hybrid Systems},
  author = {Alur, R. and Courcoubetis, C. and Halbwachs, N. and Henzinger, T.A. and Ho, P.-H. and Nicollin, X. and Olivero, A. and Sifakis, J. and Yovine, S.},
  date = {1995-02},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  volume = {138},
  number = {1},
  pages = {3--34},
  issn = {03043975},
  doi = {10.1016/0304-3975(94)00202-T},
  url = {https://linkinghub.elsevier.com/retrieve/pii/030439759400202T},
  urldate = {2022-06-11},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\LBEF7ASX\Alur et al. - 1995 - The algorithmic analysis of hybrid systems.pdf}
}

@inbook{andersonLearningLanguageLanguage2020,
  title = {Learning about Language through Language Invention: “{{I}} Was Really Proud of the Language {{I}} Created”},
  shorttitle = {Learning about Language through Language Invention},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Anderson, Skye J. and Bischoff, Shannon T. and Punske, Jeffrey and Fountain, Amy V.},
  date = {2020-08-12},
  pages = {208--238},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0013},
  url = {https://academic.oup.com/book/31973/chapter/267718033},
  urldate = {2023-08-24},
  abstract = {This chapter analyzes the implementation of Invented Language Projects in different institutional and pedagogical contexts, focusing on introductory level linguistics. The data the chapter draws on come from students at three public universities, in a variety of class sizes, across undergraduate cohorts, and using a variety of integrations of language invention in course materials, over the last five years. The chapter identifies patterns of effectiveness in the use of language invention in the classroom by analyzing data collected inside and outside of these courses, assessing students’ mastery of core concepts in linguistics, their beliefs and attitudes about language, and their perception of the utility of language invention for their own learning. Though there is variation in the effectiveness of these strategies as they are instantiated in different contexts, the chapter shows that language invention is a promising way to engage with and effectively teach introductory students about the workings of natural human language.},
  bookauthor = {Anderson, Skye J. and Bischoff, Shannon T. and Punske, Jeffrey and Fountain, Amy V.},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@inproceedings{anishIdentifyingArchitecturallySignificant2015,
  title = {Identifying Architecturally Significant Functional Requirements},
  booktitle = {Proceedings of the {{Fifth International Workshop}} on {{Twin Peaks}} of {{Requirements}} and {{Architecture}}},
  author = {Anish, Preethu Rose and Balasubramaniam, Balaji and Cleland-Huang, Jane and Wieringa, Roel and Daneva, Maya and Ghaisas, Smita},
  date = {2015-05-16},
  series = {{{TwinPeaks}} '15},
  pages = {3--8},
  publisher = {{IEEE Press}},
  location = {{Florence, Italy}},
  abstract = {Failure to identify and analyze architecturally significant functional and non-functional requirements (NFRs) early on in the life cycle of a project can result in costly rework in later stages of software development. While NFRs indicate an explicit architectural impact, the impact that functional requirements may have on architecture is often implicit. The skills needed for capturing functional requirements are different than those needed for making architectural decisions. As a result, these two activities are often conducted by different teams in a project. Therefore it becomes necessary to integrate the knowledge gathered by people with different expertise to make informed architectural decisions. We present a study to bring out that functional requirements often have implicit architectural impact and do not always contain comprehensive information to aid architectural decisions. Further, we present our initial work on automating the identification of architecturally significant functional requirements from requirements documents and their classification into categories based on the different kinds of architectural impact they can have. We believe this to be a crucial precursor for recommending specific design decisions. We envisage ArcheR, a tool that (a) automates the identification of architecturally significant functional requirements from requirement specification documents, (b) classify them into categories based on the different kinds of architectural impact they can have, (c) recommend probing questions the business analyst should ask in order to produce a more complete requirements specification, and (d) recommend possible architectural solutions in response to the architectural impact.},
  keywords = {architectural decisions,architecturally significant functional requirements (ASFRs),requirements classification},
  file = {C:\Users\ron\Zotero\storage\D4QIBM7I\Anish et al. - 2015 - Identifying architecturally significant functional.pdf}
}

@inproceedings{ansaryDistributedOperatingSystem2017,
  title = {A {{Distributed Operating System Network Stack}} and {{Device Driver}} for {{Multicores}}},
  booktitle = {2017 {{IEEE}} 37th {{International Conference}} on {{Distributed Computing Systems}} ({{ICDCS}})},
  author = {Ansary, Saif and Barbalace, Antonio and Chuang, Ho-Ren and Lazor, Thomas and Ravindran, Binoy},
  date = {2017-06},
  pages = {2646--2649},
  issn = {1063-6927},
  doi = {10.1109/ICDCS.2017.90},
  abstract = {With the advances in network speeds a single processor cannot cope anymore with the growing number of data streams from a single network card. Multicore processors come at a rescue but traditional SMP OSes, which integrate the software network stack, scale only to a certain extent,limiting an application's ability to serve more connections while increasing the number of cores. On the other hand, kernel bypass solutions seem to scale better, but limit resource flexibility and control. We propose attacking these problems with a distributed OS design, using multiple network stacks (one per kernel) and relying on multi-queue hardware and hardware flow steering. This creates a single-socket abstraction among kernels while minimizing inter-core communication. We introduce our design, consisting of a distributed network stack, a distributed device driver, and a load-balancing algorithm. We compare our prototype, NetPopcorn, with Linux, Affinity Accept, FastSocket. NetPopcorn accepts between 5 to 8 times more connections and reduces the tail latency compared to these competitors. We also compare NetPopcorn with mTCP and observe that for high core counts, mTCP accepts only 18\% more connections yet with higher tail latency than NetPopcorn.},
  eventtitle = {2017 {{IEEE}} 37th {{International Conference}} on {{Distributed Computing Systems}} ({{ICDCS}})},
  keywords = {connections scalability,Hardware,Kernel,Linux,multicore,Multicore processing,network,OS,Popcorn Linux,Scalability,single IP,SMP},
  file = {C:\Users\ron\Zotero\storage\NPG3MIFC\A_Distributed_Operating_System_Network_Stack_and_Device_Driver_for_Multicores.pdf}
}

@inproceedings{anwerSystematicApproachIdentifying2019,
  title = {A {{Systematic Approach}} for {{Identifying Requirement Change Management Challenges}}: {{Preliminary Results}}},
  shorttitle = {A {{Systematic Approach}} for {{Identifying Requirement Change Management Challenges}}},
  booktitle = {Proceedings of the {{Evaluation}} and {{Assessment}} on {{Software Engineering}}},
  author = {Anwer, Sajid and Wen, Lian and Wang, Zhe},
  date = {2019-04-15},
  series = {{{EASE}} '19},
  pages = {230--235},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3319008.3319031},
  url = {https://doi.org/10.1145/3319008.3319031},
  urldate = {2022-10-22},
  abstract = {Requirement Change is one of the most challenging tasks in software development lifecycle, particularly in the complex context of Global Software Development (GSD). During the last decade, many studies are carried out to address these problems, however, careful examination of these works suggests that there's a potential research gap. This paper has performed a Systematic Literature Review (SLR) to identify the most significant/commonly studied challenges of requirement change management process and furthermore this process under GSD context. We identified ten challenges such as impact analysis, cost estimation, artifacts documents management, requirement traceability, requirements dependency, conflicts with existing requirements, time estimation, change prioritization, user involvement, and system destabilizing. Furthermore, three challenges such as communication and coordination, knowledge sharing, management, and Change Control Board (CCB) management are identified for globally distributed projects. We also mapped these identified challenges to Requirement Change Management Process (RCMP) outcomes proposed in our previous study. We believe that mapping between RCM challenges and RCMP outcomes will enhance the practical significance of this study results. Considering the systematic literature review results, we suggest that there is a need to develop a framework for requirement change management for quality software systems development.},
  isbn = {978-1-4503-7145-2},
  keywords = {Global software development,Requirement change management,Requirements evolution,Systematic literature review},
  file = {C:\Users\ron\Zotero\storage\I8I29Q8F\Anwer et al. - 2019 - A Systematic Approach for Identifying Requirement .pdf}
}

@article{apidianakiWordTypesTokens2023,
  title = {From {{Word Types}} to {{Tokens}} and {{Back}}: {{A Survey}} of {{Approaches}} to {{Word Meaning Representation}} and {{Interpretation}}},
  shorttitle = {From {{Word Types}} to {{Tokens}} and {{Back}}},
  author = {Apidianaki, Marianna},
  date = {2023-03-14},
  journaltitle = {Computational Linguistics},
  pages = {1--59},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00474},
  url = {https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00474/114225/From-Word-Types-to-Tokens-and-Back-A-Survey-of},
  urldate = {2023-08-14},
  abstract = {Abstract              Vector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\QPH88CZT\coli_a_00474.pdf}
}

@online{appleinc.AppleDeveloperDocumentation,
  title = {Apple {{Developer Documentation}}: {{UIKit}}},
  author = {{Apple, Inc.}},
  url = {https://developer.apple.com/documentation/uikit},
  organization = {{UIKit | Apple Developer Documentation}}
}

@online{appleinc.ButtonSwiftUIControls,
  title = {Button, {{SwiftUI Controls}} and Indicators},
  author = {{Apple, Inc.}},
  url = {https://developer.apple.com/documentation/swiftui/button},
  urldate = {2022-11-28},
  organization = {{Button | Apple Developer Documentation}}
}

@online{appleinc.CoreDataProgramming,
  title = {Core {{Data Programming Guide}}: {{What Is Core Data}}?},
  author = {{Apple, Inc.}},
  url = {https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/CoreData/index.html},
  urldate = {2022-11-27},
  file = {C:\Users\ron\Zotero\storage\XC3XSRR6\index.html}
}

@online{appleinc.IPadModelsCompatible,
  title = {{{iPad}} Models Compatible with {{iPadOS}} 16},
  author = {{Apple, Inc.}},
  url = {https://support.apple.com/guide/ipad/supported-models-ipad213a25b2/ipados},
  urldate = {2022-11-19},
  abstract = {Find out which iPad models support iPadOS 16.},
  langid = {english},
  organization = {{Apple Support}},
  file = {C:\Users\ron\Zotero\storage\9KYWBNA9\ipados.html}
}

@online{appleinc.Localization,
  title = {Localization},
  author = {{Apple, Inc.}},
  url = {https://developer.apple.com/localization/},
  urldate = {2022-11-20},
  abstract = {Learn how you can localize your apps, product pages, and marketing materials to better connect with customers in different locations.},
  langid = {english},
  organization = {{Apple Developer}},
  file = {C:\Users\ron\Zotero\storage\LFNVMN8G\localization.html}
}

@online{appleinc.Timer,
  title = {Timer},
  author = {{Apple, Inc.}},
  url = {https://developer.apple.com/documentation/foundation/timer},
  urldate = {2022-11-30},
  abstract = {A timer that fires after a certain time interval has elapsed, sending a specified message to a target object.},
  langid = {english},
  organization = {{Apple Developer Documentation}},
  file = {C:\Users\ron\Zotero\storage\NW6VD4GI\timer.html}
}

@article{apteAddingParityLinux2007,
  title = {Adding Parity to the {{Linux}} Ext3 File System},
  author = {Apte, Himani and Rungta, Meenali},
  date = {2007-01-01},
  journaltitle = {ACM SIGOPS Operating Systems Review},
  shortjournal = {SIGOPS Oper. Syst. Rev.},
  volume = {41},
  number = {1},
  pages = {56--65},
  issn = {0163-5980},
  doi = {10.1145/1228291.1228306},
  url = {https://doi.org/10.1145/1228291.1228306},
  urldate = {2022-06-24},
  abstract = {Modern disks no longer operate in a simple "fail-stop" manner, yet commodity operating systems assume they do. We design and implement a parity based approach to improve the robustness of journaling file systems. We modify the existing ext3 file system for data and ordered journaling modes to incorporate parity and call it the "Parity File System". Using PFS, we are able to recover from a single latent sector error or silent block corruption within a given file. We show that the performance overhead for PFS compared to ext3 is minimal while the robustness is significantly improved.},
  file = {C:\Users\ron\Zotero\storage\I85KZRZN\Apte and Rungta - 2007 - Adding parity to the Linux ext3 file system.pdf}
}

@inproceedings{ariyapalaContextOSContextAware2013,
  title = {{{ContextOS}}: {{A Context Aware Operating System}} for {{Mobile Devices}}},
  shorttitle = {{{ContextOS}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Green Computing}} and {{Communications}} and {{IEEE Internet}} of {{Things}} and {{IEEE Cyber}}, {{Physical}} and {{Social Computing}}},
  author = {Ariyapala, Kanishka and Conti, Mauro and Keppitiyagama, Chamath},
  date = {2013-08},
  pages = {976--984},
  publisher = {{IEEE}},
  location = {{Beijing, China}},
  doi = {10.1109/GreenCom-iThings-CPSCom.2013.168},
  url = {http://ieeexplore.ieee.org/document/6682182/},
  urldate = {2022-05-15},
  abstract = {The Operating System (OS) manages the hardware resources of a computer. For an OS, the knowledge about context is valuable information in optimizing its tasks. Recent mobile devices, such as smart-phones and tablets, are providing new avenues in context aware computing, because of the wide variety of sensors integrated into them.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Green Computing}} and {{Communications}} ({{GreenCom}}) and {{IEEE Internet}} of {{Things}}({{iThings}}) and {{IEEE Cyber}}, {{Physical}} and {{Social Computing}}({{CPSCom}})},
  isbn = {978-0-7695-5046-6},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\59HWZW4G\Ariyapala et al. - 2013 - ContextOS A Context Aware Operating System for Mo.pdf}
}

@book{arnoldMachineTranslationIntroductory1994,
  title = {Machine {{Translation}}: {{An Introductory Guide}}},
  shorttitle = {Machine {{Translation}}},
  author = {Arnold, Douglas and Balkan, Lorna and Meijer, Siety and Humphreys, R. Lee and Sadler, Louisa},
  date = {1994},
  publisher = {{NCC Blackwell Ltd.}},
  location = {{London}},
  url = {http://www.essex.ac.uk/linguistics/clmt/MTbook/},
  isbn = {185542-17x},
  file = {C:\Users\ron\Zotero\storage\8XTRIV92\Machine Translation An Introductory Guide.pdf}
}

@book{aronoffWhatMorphology2012,
  title = {What Is Morphology?},
  author = {Aronoff, Mark and Fudeman, Kirsten Anne},
  date = {2012},
  series = {Fundamentals of Linguistics},
  edition = {2. ed., [repr.]},
  publisher = {{Wiley-Blackwell}},
  location = {{Chichester}},
  isbn = {978-1-4051-9467-9},
  langid = {english},
  pagetotal = {290}
}

@inproceedings{asanoFeatureOrientedRefinement2019,
  title = {Feature {{Oriented Refinement}} from {{Requirements}} to {{System Decomposition}}: {{Quantitative}} and {{Accountable Approach}}},
  shorttitle = {Feature {{Oriented Refinement}} from {{Requirements}} to {{System Decomposition}}},
  booktitle = {Proceedings of the 23rd {{International Systems}} and {{Software Product Line Conference}} - {{Volume A}}},
  author = {Asano, Masaki and Nishiura, Yoichi and Nakanishi, Tsuneo and Fujiwara, Keiichi},
  date = {2019-09-09},
  pages = {195--205},
  publisher = {{ACM}},
  location = {{Paris France}},
  doi = {10.1145/3336294.3336314},
  url = {https://dl.acm.org/doi/10.1145/3336294.3336314},
  urldate = {2022-10-22},
  abstract = {This paper presents the revised domain engineering process to develop product lines of automotive body parts in Aisin Seiki Co., Ltd. In the process, feature analysis is conducted by a limited number of engineers with talent of abstraction and separation and other work including specifications and architecture design is conducted by average engineers who know the products. Feature analysis defines a hierarchy of abstraction, achieves separation of concerns, and disciplines other artifacts to follow the structure of abstraction and separation. Requirements and specifications are refined by the use case, use case scenario, and hierarchical tabular description (USDM) in a step-wise manner. The specification in USDM is refined to a system decomposition in a quantitative and accountable manner using the robustness diagram and design structure matrix. The revised domain engineering process reduced the issues pointed out in software reviews concerning errors on specifications and architecture design. Moreover, it reduced lead time for architecture design and produced the architecture tolerant to changes.},
  eventtitle = {{{SPLC}} 2019: 23rd {{International Systems}} and {{Software Product Line Conference}}},
  isbn = {978-1-4503-7138-4},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\QCMR4XDI\Asano et al. - 2019 - Feature Oriented Refinement from Requirements to S.pdf}
}

@article{ascherOpenSourceRight2004,
  title = {Is {{Open Source Right}} for {{You}}?: {{A}} Fictional Case Study of Open Source in a Commercial Software Shop},
  author = {Ascher, David},
  date = {2004-05},
  journaltitle = {Queue},
  volume = {2},
  number = {3},
  pages = {32--38},
  doi = {1005062.1005065},
  url = {https://doi.org/10.1145/1005062.1005065},
  abstract = {The media often present open source software as a direct competitor to commercial software. This depiction, usually pitting David (Linux) against Goliath (Microsoft), makes for fun reading in the weekend paper. However, it mostly misses the point of what open source means to a development organization. In this article, I use the experiences of GizmoSoft (a fictitious software company) to present some perspectives on the impact of open source software usage in a software development shop.},
  file = {C:\Users\ron\Zotero\storage\2H666HBF\1005062.1005065.pdf}
}

@inproceedings{aziziConcurrentHardwareSoftware2002,
  title = {Concurrent {{Hardware}}/{{Software Coverification}} with {{Java}} Threads},
  booktitle = {Proceedings. {{International Conference}} on {{Parallel Computing}} in {{Electrical Engineering}}},
  author = {Azizi, Mostafa},
  date = {2002},
  pages = {95--98},
  doi = {10.1109/PCEE.2002.1115211},
  eventtitle = {International {{Conference}} on {{Parallel Computing}} in {{Electrical Engineering}}},
  file = {C:\Users\ron\Zotero\storage\7AR8I3HX\Concurrent_hardware_software_coverification_with_Java_threads.pdf}
}

@standard{baggiaSpeechSynthesisMarkup2010,
  title = {Speech {{Synthesis Markup Language}} ({{SSML}}) {{Version}} 1.1},
  author = {Baggia, Paolo and Bagshaw, Paul and Bodell, Michael and {De Zhi Huang} and {Lou Xiaoyan} and McGlashan, Scott and {Jianhua Tao} and {Hu Fang} and {Helen Meng} and {Xia Hairong} and {Zhiyong Wu}},
  editora = {Burnett, Daniel and {Zhi Wei Shuang}},
  editoratype = {collaborator},
  date = {2010-09-07},
  url = {https://www.w3.org/TR/speech-synthesis11/},
  urldate = {2023-10-24},
  version = {1.1}
}

@standard{bagshawPronunciationLexiconSpecification2028,
  title = {Pronunciation {{Lexicon Specification}} ({{PLS}})},
  author = {Bagshaw, Paul and Burnett, Daniel C and Carter, Jerry and Scahill, Frank},
  editora = {Baggia, Paolo},
  editoratype = {collaborator},
  date = {2028-10-14},
  publisher = {{World Wide Web Consortium}},
  url = {https://www.w3.org/TR/pronunciation-lexicon/},
  version = {1.0}
}

@inproceedings{balikuddembePlanningPublicSector2018,
  title = {Planning for Public Sector Software Projects Using Value-Based Requirements Engineering Techniques: A Research Agenda},
  shorttitle = {Planning for Public Sector Software Projects Using Value-Based Requirements Engineering Techniques},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Software Engineering}} in {{Africa}}},
  author = {Balikuddembe, Joseph Kibombo and Nakirijja, Justine},
  date = {2018-05-27},
  series = {{{SEiA}} '18},
  pages = {50--54},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3195528.3195536},
  url = {https://doi.org/10.1145/3195528.3195536},
  urldate = {2022-10-22},
  abstract = {The introduction of e-Government enabled services has resulted in the public- sector wide integration of different software applications, often scaled up to a national level. Out of observation, the way these initiatives are handled differs in the way software-development projects are managed in the private sector. The anticipated value of these projects tends to differ significantly in the long run. We have particularly picked interest in the health sector in which e-Health initiatives have been defined. We aim at understanding how value proliferation can be understood and quantified from the onset on such large-scale projects using requirement engineering techniques. In this work we infer that effective planning of large scale ICT initiatives, such as e-Health, should be long term driven so as to ensure effective sector management. Novel approaches in this realm should strive at linking strategy, measurement and operational decisions from the onset. In here we examine what has been done, key opportunities, challenges and gaps that can be addressed by the research community. In bridging these gaps, we propose an agenda by formulating key research questions which both the industry and academia can address as future direction to align this view.},
  isbn = {978-1-4503-5719-7},
  keywords = {e-health based projects,project planning and success,value-based requirements engineering},
  file = {C:\Users\ron\Zotero\storage\4MVP89GM\Balikuddembe and Nakirijja - 2018 - Planning for public sector software projects using.pdf}
}

@inproceedings{bani-salamehComprehensiveSurveyRequirements2015,
  title = {Towards a {{Comprehensive Survey}} of the {{Requirements Elicitation Process Improvements}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Intelligent Information Processing}}, {{Security}} and {{Advanced Communication}}},
  author = {Bani-Salameh, Hani and Al jawabreh, Nadera},
  date = {2015-11-23},
  series = {{{IPAC}} '15},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2816839.2816872},
  url = {https://doi.org/10.1145/2816839.2816872},
  urldate = {2022-10-22},
  abstract = {Good quality of requirements is considered as one of the most critical parts of software development projects. The main objective of any project is to get the right requirements in order to be successful. Requirements elicitation is the process of gathering the right requirements from different sources (e.g. users, stakeholders) using the right techniques to achieve the users and system needs. There is not much attention and focus on the requirements elicitation process modeling in general. Most of existing models study the elicitation techniques in specific. This paper conducts a comprehensive survey of the requirements elicitation literature. We propose a model that illustrates the elicitation process activities. Features captured in this model: (1) cover almost all and the most important activities in the elicitation process, (2) concentrate on the improvement of the requirements quality by applying requirements tracking and refinement.},
  isbn = {978-1-4503-3458-7},
  keywords = {Quality,Requirements,Requirements Elicitation,Requirements Tracking},
  file = {C:\Users\ron\Zotero\storage\9ZP7IU3D\Bani-Salameh and Al jawabreh - 2015 - Towards a Comprehensive Survey of the Requirements.pdf}
}

@inproceedings{barbalacePopcornReplicatedkernelOS2014,
  title = {Popcorn: A Replicated-Kernel {{OS}} Based on {{Linux}}},
  booktitle = {Proceedings of the {{Linux Symposium}}},
  author = {Barbalace, Antonio and Ravindran, Binoy and Katz, David},
  date = {2014-07},
  pages = {123--138},
  location = {{Ottawa, Canada}},
  eventtitle = {Linux {{Symposium}}},
  file = {C:\Users\ron\Zotero\storage\J63HNM34\ols2014-barbalace.pdf}
}

@inproceedings{bartoliniAutonomicOperatingSystem2013,
  title = {The Autonomic Operating System Research Project: Achievements and Future Directions},
  shorttitle = {The Autonomic Operating System Research Project},
  booktitle = {Proceedings of the 50th {{Annual Design Automation Conference}}},
  author = {Bartolini, Davide B. and Cattaneo, Riccardo and Durelli, Gianluca C. and Maggio, Martina and Santambrogio, Marco D. and Sironi, Filippo},
  date = {2013-05-29},
  series = {{{DAC}} '13},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2463209.2488828},
  url = {https://doi.org/10.1145/2463209.2488828},
  urldate = {2022-06-07},
  abstract = {Traditionally, hypervisors, operating systems, and runtime systems have been providing an abstraction layer over the bare-metal hardware. Traditional abstractions, however, do not consider for non-functional requirements such as system-level constraints or users' objectives. As these requirements are gaining increasing importance, researchers are looking into making user-specified and system-level objectives first-class citizens in the computer systems' realm. This paper describes the Autonomic Operating System (AcOS) project; AcOS enhances commodity operating systems with an autonomic layer that enables self-* properties through adaptive resource allocation. With AcOS, we investigate intelligent resource allocation to achieve user-specified service-level objectives on application performance and to respect system-level thresholds on CPU temperature. We give a broad overview of AcOS, elaborate on its achievements, and discuss research perspectives.},
  isbn = {978-1-4503-2071-9},
  keywords = {autonomic computing,dynamic thermal management,operating systems,performance management,virtualization},
  file = {C:\Users\ron\Zotero\storage\NJWDCDBW\2463209.2488828.pdf}
}

@article{beanDefectPreventionDetection2008,
  title = {Defect {{Prevention}} and {{Detection}} in {{Software}} for {{Automated Test Equipment}}},
  author = {Bean, Eric},
  date = {2008},
  pages = {8},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\7L5HK6XI\Bean - 2008 - Defect Prevention and Detection in Software for Au.pdf}
}

@inproceedings{benderPIERESPlaygroundNetwork2021,
  title = {{{PIERES}}: {{A Playground}} for {{Network Interrupt Experiments}} on {{Real-Time Embedded Systems}} in the {{IoT}}},
  shorttitle = {{{PIERES}}},
  booktitle = {Companion of the {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Bender, Franz and Brune, Jan Jonas and Keutel, Nick Lauritz and Behnke, Ilja and Thamsen, Lauritz},
  date = {2021-04-19},
  pages = {81--84},
  publisher = {{ACM}},
  location = {{Virtual Event France}},
  doi = {10.1145/3447545.3451189},
  url = {https://dl.acm.org/doi/10.1145/3447545.3451189},
  urldate = {2022-06-11},
  eventtitle = {{{ICPE}} '21: {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  isbn = {978-1-4503-8331-8},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\TEFLBMR5\Bender et al. - 2021 - PIERES A Playground for Network Interrupt Experim.pdf}
}

@article{berardInterruptTimedAutomata2012,
  title = {Interrupt {{Timed Automata}}: Verification and Expressiveness},
  shorttitle = {Interrupt {{Timed Automata}}},
  author = {Bérard, Béatrice and Haddad, Serge and Sassolas, Mathieu},
  date = {2012-02},
  journaltitle = {Formal Methods in System Design},
  shortjournal = {Form Methods Syst Des},
  volume = {40},
  number = {1},
  pages = {41--87},
  issn = {0925-9856, 1572-8102},
  doi = {10.1007/s10703-011-0140-2},
  url = {http://link.springer.com/10.1007/s10703-011-0140-2},
  urldate = {2022-06-11},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\WEM8884V\Bérard et al. - 2012 - Interrupt Timed Automata verification and express.pdf}
}

@inproceedings{berardRealTimeProperties2010,
  title = {Real {{Time Properties}} for {{Interrupt Timed Automata}}},
  booktitle = {2010 17th {{International Symposium}} on {{Temporal Representation}} and {{Reasoning}}},
  author = {Berard, Beatrice and Haddad, Serge and Sassolas, Mathieu},
  date = {2010-09},
  pages = {69--76},
  publisher = {{IEEE}},
  location = {{Paris}},
  doi = {10.1109/TIME.2010.11},
  url = {http://ieeexplore.ieee.org/document/5601853/},
  urldate = {2022-06-11},
  abstract = {Interrupt Timed Automata (ITA) have been introduced to model multi-task systems with interruptions. They form a subclass of stopwatch automata, where the real valued variables (with rate 0 or 1) are organized along priority levels. While reachability is undecidable with usual stopwatches, the problem was proved decidable for ITA.},
  eventtitle = {2010 17th {{International Symposium}} on {{Temporal Representation}} and {{Reasoning}} ({{TIME}} 2010)},
  isbn = {978-1-4244-8014-2},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\PKYMLTW9\Berard et al. - 2010 - Real Time Properties for Interrupt Timed Automata.pdf}
}

@inbook{berryTeachingInventedLanguages2020,
  title = {Teaching Invented Languages as an Introductory Course: {{Unfamiliar}} Territory},
  shorttitle = {Teaching Invented Languages as an Introductory Course},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Berry, James A.},
  date = {2020-08-12},
  pages = {125--136},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0009},
  url = {https://academic.oup.com/book/31973/chapter/267717537},
  urldate = {2023-08-24},
  abstract = {This chapter focuses on the introduction of an invented languages course to the English Department of a comprehensive state university in the midwestern United States in Fall 2016. The department has limited offerings in linguistics, with no major or minor. The course acted primarily as an applied introductory class without prerequisites, open to students with varied academic backgrounds in linguistics. Because this course was the only exposure to linguistics some students would have in their academic careers, teaching it called for some constraints. An early-semester look at international auxiliary languages such as Esperanto inspired several students to create invented languages that were easy to learn. At the same time, those with a more extensive background created more complex structures. The resulting projects ranged from minimal to highly developed; overall, though, the students maintained a high level of enthusiasm and interest in their work.},
  bookauthor = {Berry, James A.},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@video{biblaridionConlangShowcaseNekachti2019,
  entrysubtype = {video},
  title = {Conlang {{Showcase}} - {{Nekāchti}}},
  editor = {{Biblaridion}},
  editortype = {director},
  date = {2019-11-13},
  url = {https://www.youtube.com/watch?v=da0Cq3enfXM},
  abstract = {One of my favorite conlangs I've ever created.}
}

@book{birdNaturalLanguageProcessing2009,
  title = {Natural Language Processing with {{Python}}},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  date = {2009},
  edition = {1st ed},
  publisher = {{O'Reilly}},
  location = {{Beijing ; Cambridge [Mass.]}},
  abstract = {This is an introduction to natural language processing, which supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation},
  isbn = {978-0-596-51649-9},
  pagetotal = {479},
  keywords = {Natural language processing (Computer science),Python (Computer program language),Python {$<$}Programmiersprache{$>$},Sprachverarbeitung},
  annotation = {OCLC: ocn301885973}
}

@article{boehmThreadsCannotBe,
  title = {Threads {{Cannot Be Implemented As}} a {{Library}}},
  author = {Boehm, Hans-J},
  pages = {14},
  abstract = {In many environments, multi-threaded code is written in a language that was originally designed without thread support (e.g. C), to which a library of threading primitives was subsequently added. There appears to be a general understanding that this is not the right approach. We provide specific arguments that a pure library approach, in which the compiler is designed independently of threading issues, cannot guarantee correctness of the resulting code.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\TRME3I5L\Boehm - Threads Cannot Be Implemented As a Library.pdf}
}

@inproceedings{bondiBestPracticesWriting2012,
  title = {Best Practices for Writing and Managing Performance Requirements: A Tutorial},
  shorttitle = {Best Practices for Writing and Managing Performance Requirements},
  booktitle = {Proceedings of the 3rd {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Bondi, André B.},
  date = {2012-04-22},
  series = {{{ICPE}} '12},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2188286.2188288},
  url = {https://doi.org/10.1145/2188286.2188288},
  urldate = {2022-06-09},
  abstract = {Performance requirements are one of the main drivers of architectural decisions. Because many performance problems have their roots in architectural decisions, and since poor performance is a principal cause of software project risk, it is essential that performance requirements be developed early in the software lifecycle, and that they be clearly formulated. In this tutorial, we shall look at criteria for high-quality performance requirements, including algebraic consistency, measurability, testability, and linkage to business and engineering needs. While focus of this tutorial is on practice, we shall show how the drafting of performance requirements can be aided by performance modeling. We shall show methods for presenting and managing performance requirements that will improve their chances of being accepted by architects, developers, testers, contract negotiators, and purchasers; and of their being successfully implemented and tested.},
  isbn = {978-1-4503-1202-8},
  keywords = {performance requirements,system performance},
  file = {C:\Users\ron\Zotero\storage\ZJS8HKA4\2188286.2188288.pdf}
}

@article{boscEmergenceArgumentStructure2022,
  title = {The {{Emergence}} of {{Argument Structure}} in {{Artificial Languages}}},
  author = {Bosc, Tom and Vincent, Pascal},
  date = {2022-12-23},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {1375--1391},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00524},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00524/114371/The-Emergence-of-Argument-Structure-in-Artificial},
  urldate = {2023-08-14},
  abstract = {Abstract              Computational approaches to the study of language emergence can help us understand how natural languages are shaped by cognitive and sociocultural factors. Previous work focused on tasks where agents refer to a single entity. In contrast, we study how agents predicate, that is, how they express that some relation holds between several entities. We introduce a setup where agents talk about a variable number of entities that can be partially observed by the listener. In the presence of a least-effort pressure, they tend to discuss only entities that are not observed by the listener. Thus we can obtain artificial phrases that denote a single entity, as well as artificial sentences that denote several entities. In natural languages, if we ignore the verb, phrases are usually concatenated, either in a specific order or by adding case markers to form sentences. Our setup allows us to quantify how much this holds in emergent languages using a metric we call concatenability. We also measure transitivity, which quantifies the importance of word order. We demonstrate the usefulness of this new setup and metrics for studying factors that influence argument structure. We compare agents having access to input representations structured into pre-segmented objects with properties, versus unstructured representations. Our results indicate that the awareness of object structure yields a more natural sentence organization.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\5CXXHGAX\tacl_a_00524.pdf}
}

@inproceedings{brandoleseMeasurementAnalysisModeling2008,
  title = {Measurement, {{Analysis}} and {{Modeling}} of {{RTOS System Calls Timing}}},
  booktitle = {2008 11th {{EUROMICRO Conference}} on {{Digital System Design Architectures}}, {{Methods}} and {{Tools}}},
  author = {Brandolese, Carlo and Fornaciari, William},
  date = {2008-09},
  pages = {618--625},
  doi = {10.1109/DSD.2008.86},
  abstract = {This paper presents a methodology for accurately characterizing the system calls of an operating system for embedded applications. Characterization consists of two phases: measurements and modeling. Measurements allow a coarse-grained quantitative comparison of different operating systems. Models, on the other hand, have been derived to gain a more detailed view of the behavior of a RTOS. Furthermore, they have been used within a source-level execution time estimation framework and their accuracy and usefulness proved through benchmarking. The measurement framework is based on two prototyping boards based on Xilinx and Altera devices and the timing characterization has been performed on two real-time operating systems: VxWorks and RTEMS.},
  eventtitle = {2008 11th {{EUROMICRO Conference}} on {{Digital System Design Architectures}}, {{Methods}} and {{Tools}}},
  keywords = {Design methodology,execution profiling,Operating systems,performance analysis,Power system management,Power system modeling,Predictive models,Proposals,Real time operating systems,Real time systems,Software performance,system calls,Time measurement,Timing,timing characterization},
  file = {C\:\\Users\\ron\\Zotero\\storage\\TL9NZI7F\\Brandolese and Fornaciari - 2008 - Measurement, Analysis and Modeling of RTOS System .pdf;C\:\\Users\\ron\\Zotero\\storage\\25WA6QYI\\4669292.html}
}

@standard{brayExtensibleMarkupLanguage2006,
  title = {Extensible {{Markup Language}} ({{XML}}) 1.1},
  author = {Bray, Tim and Paoli and Sperberg-McQueen, C.M. and Malar, Eve and Yergeau, François and Cowan, John},
  date = {2006-09-29},
  abstract = {The Extensible Markup Language (XML) is a subset of SGML that is completely described in this document. Its goal is to enable generic SGML to be served, received, and processed on the Web in the way that is now possible with HTML. XML has been designed for ease of implementation and for interoperability with both SGML and HTML.},
  version = {Second Editon}
}

@inproceedings{bulusuApplyingRequirementEngineering2018,
  title = {Applying a Requirement Engineering Based Approach to Evaluate the Security Requirements Engineering Methodologies},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Bulusu, Sravani Teja and Laborde, Romain and Wazan, Ahmad Samer and Barrere, Francois and Benzekri, Abdelmalek},
  date = {2018-04-09},
  pages = {1316--1318},
  publisher = {{ACM}},
  location = {{Pau France}},
  doi = {10.1145/3167132.3167417},
  url = {https://dl.acm.org/doi/10.1145/3167132.3167417},
  urldate = {2022-10-22},
  abstract = {Considering the multitude of security requirements engineering methodologies available today, selecting a security requirement engineering methodology that fits the security engineering context becomes a promising task. In previous work, we outlined a generic evaluation methodology to elicit and evaluate the anticipated characteristics of a security requirements engineering methodology according to the stakeholders’ working context. In this paper, we detail each step of our methodology using an example context of network security requirements engineering.},
  eventtitle = {{{SAC}} 2018: {{Symposium}} on {{Applied Computing}}},
  isbn = {978-1-4503-5191-1},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\SV2TFVC4\Bulusu et al. - 2018 - Applying a requirement engineering based approach .pdf}
}

@inproceedings{bulusuWhichSecurityRequirements2017,
  title = {Which {{Security Requirements Engineering Methodology Should I Choose}}? {{Towards}} a {{Requirements Engineering-based Evaluation Approach}}},
  shorttitle = {Which {{Security Requirements Engineering Methodology Should I Choose}}?},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Availability}}, {{Reliability}} and {{Security}}},
  author = {Bulusu, Sravani Teja and Laborde, Romain and Wazan, Ahmad Samer and Barrère, Francois and Benzekri, Abdelmalek},
  date = {2017-08-29},
  series = {{{ARES}} '17},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3098954.3098996},
  url = {https://doi.org/10.1145/3098954.3098996},
  urldate = {2022-10-22},
  abstract = {Since many decades, requirements engineering domain has seen significant enhancements towards adapting the security and risk analysis concepts. In this regard, there exist numerous security requirements engineering methodologies that support elicitation and evaluation of the security requirements. However, selecting a security requirements engineering methodology (SRE) for a given context of use often depends on a set of ad hoc criteria. In this paper, we propose a methodological evaluation methodology that helps in identifying the characteristics of a good SRE methodology.},
  isbn = {978-1-4503-5257-4},
  keywords = {evaluation methodology,Security requirements engineering},
  file = {C:\Users\ron\Zotero\storage\ANEVSLJC\Bulusu et al. - 2017 - Which Security Requirements Engineering Methodolog.pdf}
}

@book{burleyCybersecurityCurricula20172018,
  title = {Cybersecurity {{Curricula}} 2017: {{Curriculum Guidelines}} for {{Post-Secondary Degree Programs}} in {{Cybersecurity}}},
  shorttitle = {Cybersecurity {{Curricula}} 2017},
  author = {Burley, Diana and Bishop, Matt and Buck, Scott and Ekstrom, Joseph J. and Futcher, Lynn and Gibson, David and Hawthorne, Elizabeth K and Kaza, Siddharth and Levy, Yair and Mattord, Herbert and Parrish, Allen},
  date = {2018-01},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  isbn = {978-1-4503-8919-8},
  pagetotal = {123},
  file = {C:\Users\ron\Zotero\storage\7373KNA4\Joint Task Force on Cybersecurity Education - 2018 - Cybersecurity Curricula 2017 Curriculum Guideline.pdf}
}

@inbook{byrdTeachingProtoIndoEuropeanConstructed2020,
  title = {Teaching {{Proto-Indo-European}} as a Constructed Language},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Byrd, Brenna Reinhart and Byrd, Andrew Miles},
  date = {2020-08-12},
  pages = {186--207},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0012},
  url = {https://academic.oup.com/book/31973/chapter/267717924},
  urldate = {2023-08-24},
  abstract = {Traditionally, to even begin studying Proto-Indo-European (PIE), one must have many years of experience with one (or multiple) ancient Indo-European (IE) languages. Yet, the dwindling number of students who meet these expectations makes teaching PIE at the undergraduate level an increasingly difficult task. This chapter proposes a unique solution: instructors should teach students a constructed language (conlang) version of PIE as a precursor to discussions on the individual surviving branches and the methodologies behind historical reconstruction. This approach was developed through the synergy of the authors’ experiences teaching a conlang version of PIE to actors for the video game Far Cry Primal, teaching PIE to undergraduate students at the University of Kentucky, and previous training in language pedagogy and transformative learning.},
  bookauthor = {Byrd, Brenna Reinhart and Byrd, Andrew Miles},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@online{camfromvulgarlangCreatingLanguageHow2019,
  title = {Creating a {{Language}}, or {{How}} to {{Conlang}}},
  author = {{Cam from Vulgarlang}},
  date = {2019-12-09},
  url = {https://blog.worldanvil.com/worldbuilding/creating-a-language-or-how-to-conlang/},
  abstract = {Creating a language for your novel? Or maybe you’re conlanging – that’s creating a language – for your TTRPG game? Cam, linguistics specialist and creator of Vulgarlang Cam, takes us through how to get started!}
}

@software{camfromvulgarlangVulgarlangFantasyLanguage,
  title = {Vulgarlang, {{A Fantasy Language Generator}}},
  author = {{Cam from Vulgarlang}},
  url = {https://www.vulgarlang.com/}
}

@inbook{carpenterTeachingInventedLanguages2020,
  title = {Teaching Invented Languages to the Undergraduate Major: {{A}} Capstone Course},
  shorttitle = {Teaching Invented Languages to the Undergraduate Major},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Carpenter, Angela C.},
  date = {2020-08-12},
  pages = {107--124},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0008},
  url = {https://academic.oup.com/book/31973/chapter/267717440},
  urldate = {2023-08-24},
  abstract = {This chapter discusses how creating an invented language allows students to master critical reasoning skills and apply their linguistic knowledge to a creative language project by using the various strands of linguistic training they have received during their undergraduate years to produce their own invented language. The structure of the course, which includes weekly discussions and presentations, along with a grammar workshop that focuses on each of the elements needed to build the language, starting with phonetics and phonology and then continuing through various syntactic elements such as word order, case, and relative clause structure are detailed and discussed. Pedagogically, the course builds on four pillars: peer-to-peer learning, close and critical engagement with original source materials, problem-solving, and creative engagement with linguistic theory.},
  bookauthor = {Carpenter, Angela C.},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@article{changGuestEditorColumn2012,
  title = {Guest {{Editor}}'s {{Column}}},
  author = {Chang, Frederick},
  date = {2012},
  journaltitle = {The Next Wave: The National Security Agency's review of emerging technologies},
  volume = {19},
  number = {4},
  pages = {0--1},
  url = {https://www.nsa.gov/portals/75/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-19-4.pdf},
  file = {C:\Users\ron\Downloads\TNW-19-4.pdf}
}

@inproceedings{chenContemporaryRequirementsChallenges2019,
  title = {Contemporary Requirements Challenges and Issues: An Empirical Study in 11 Organizations},
  shorttitle = {Contemporary Requirements Challenges and Issues},
  booktitle = {Proceedings of the 34th {{ACM}}/{{SIGAPP Symposium}} on {{Applied Computing}}},
  author = {Chen, Feng and Power, Norah and Collins, J. J. and Ishikawa, Fuyuki},
  date = {2019-04-08},
  series = {{{SAC}} '19},
  pages = {1592--1599},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3297280.3297435},
  url = {https://doi.org/10.1145/3297280.3297435},
  urldate = {2022-10-22},
  abstract = {Requirements Engineering (RE) is regarded as key to software project success and has been researched and practiced for decades. With the growing maturity and complexity of software development, however, the contemporary RE environment has been changing so that the intertwining of requirements with implementation and organizational contexts has been observed and emphasized. What relevant RE challenges and issues exist in this contemporary RE environment merits investigation. This paper, therefore, reports an empirical study on the current requirements challenges and issues focusing on projects in enterprise software development environments that affect multiple (more than three) applications. Data were obtained by survey and interviews from a total of 26 practitioners in 11 organizations. Through analyzing the data, a collection of requirements challenges and issues were identified. The findings indicate that identifying the ramifications of changes and balancing the level of design and solutions in requirements are becoming important in contemporary RE for enterprise software development environment.},
  isbn = {978-1-4503-5933-7},
  keywords = {empirical study,requirements challenges,requirements engineering,requirements issues},
  file = {C:\Users\ron\Zotero\storage\DGFYGGAI\Chen et al. - 2019 - Contemporary requirements challenges and issues a.pdf}
}

@inproceedings{chengjunResearchMicrokernelTechnology2009,
  title = {Research on the {{Microkernel Technology}}},
  booktitle = {2009 {{Second International Workshop}} on {{Computer Science}} and {{Engineering}}},
  author = {Chengjun, Wang},
  date = {2009-10},
  volume = {2},
  pages = {199--202},
  doi = {10.1109/WCSE.2009.795},
  abstract = {Problems remain the layered approach. Each layer possesses considerable functionality. Major changes in one layer can have numerous effects, many difficult to trace, on code in adjacent layers. In this paper, we have introduced the microkernel architecture, studied the benefits of a microkernel organization and performance, point out the microkernel must include those functions, that depend directly on the hardware and those functions needed to support the servers and applications operating in user mode, and we have classified and designed them.},
  eventtitle = {2009 {{Second International Workshop}} on {{Computer Science}} and {{Engineering}}},
  keywords = {Communication system security,Computer architecture,Computer science,driver thread,File systems,Fuzzy systems,Hardware,Kernel,Memory management,Microkernel,Operating System,Operating systems,organization,Virtual memory,Yarn},
  file = {C:\Users\ron\Zotero\storage\KAM6A2MJ\Research_on_the_Microkernel_Technology.pdf}
}

@inproceedings{chenSystematicMethodologyOS2013,
  title = {A Systematic Methodology for {{OS}} Benchmark Characterization},
  booktitle = {Proceedings of the 2013 {{Research}} in {{Adaptive}} and {{Convergent Systems}} on - {{RACS}} '13},
  author = {Chen, Shuo-Hung and Lin, Hsiao-Mei and Chen, Kuo-Yi and Chang, Yuan-Hao and Yew, Pen-Chung and Ho, Chien-Chung},
  date = {2013},
  pages = {404--409},
  publisher = {{ACM Press}},
  location = {{Montreal, Quebec, Canada}},
  doi = {10.1145/2513228.2513234},
  url = {http://dl.acm.org/citation.cfm?doid=2513228.2513234},
  urldate = {2022-06-16},
  eventtitle = {The 2013 {{Research}} in {{Adaptive}} and {{Convergent Systems}}},
  isbn = {978-1-4503-2348-2},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\THDYGLVS\Chen et al. - 2013 - A systematic methodology for OS benchmark characte.pdf}
}

@article{chenTwoStageHypothesesGeneration2009,
  title = {Two-{{Stage Hypotheses Generation}} for {{Spoken Language Translation}}},
  author = {Chen, Boxing and Zhang, Min and Aw, Ai Ti},
  date = {2009-03-01},
  journaltitle = {ACM Transactions on Asian Language Information Processing},
  shortjournal = {ACM Transactions on Asian Language Information Processing},
  volume = {8},
  number = {1},
  pages = {4:1--4:22},
  issn = {1530-0226},
  doi = {10.1145/1482343.1482347},
  url = {https://doi.org/10.1145/1482343.1482347},
  urldate = {2023-08-14},
  abstract = {Spoken Language Translation (SLT) is the research area that focuses on the translation of speech or text between two spoken languages. Phrase-based and syntax-based methods represent the state-of-the-art for statistical machine translation (SMT). The phrase-based method specializes in modeling local reorderings and translations of multiword expressions. The syntax-based method is enhanced by using syntactic knowledge, which can better model long word reorderings, discontinuous phrases, and syntactic structure. In this article, we leverage on the strength of these two methods and propose a strategy based on multiple hypotheses generation in a two-stage framework for spoken language translation. The hypotheses are generated in two stages, namely, decoding and regeneration. In the decoding stage, we apply state-of-the-art, phrase-based, and syntax-based methods to generate basic translation hypotheses. Then in the regeneration stage, much more hypotheses that cannot be captured by the decoding algorithms are produced from the basic hypotheses. We study three regeneration methods: redecoding, n-gram expansion, and confusion network in the second stage. Finally, an additional reranking pass is introduced to select the translation outputs by a linear combination of rescoring models. Experimental results on the Chinese-to-English IWSLT-2006 challenge task of translating the transcription of spontaneous speech show that the proposed mechanism achieves significant improvements over the baseline of about 2.80 BLEU-score.},
  keywords = {hypotheses generation,Spoken language translation,statistical machine translation},
  file = {C:\Users\ron\Zotero\storage\P8V8K8KC\1482343.1482347.pdf}
}

@inproceedings{cheribiContextawarenessSecurityModel2017,
  title = {Context-Awareness Security Model Based on Multi-Agents System Operating in Complex Environment},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Internet}} of Things, {{Data}} and {{Cloud Computing}}},
  author = {Cheribi, Haoua and Amraoui, Noureddine and Bouramoul, Abdelkrim and Kholladi, Mohamed Khireddine},
  date = {2017-03-22},
  pages = {1--8},
  publisher = {{ACM}},
  location = {{Cambridge United Kingdom}},
  doi = {10.1145/3018896.3036382},
  url = {https://dl.acm.org/doi/10.1145/3018896.3036382},
  urldate = {2022-05-15},
  abstract = {In complex and pervasive environments, characterized by frequent and unpredictable changes, privacy and security are the most challenging tasks due to their importance and sensibility. Traditional Schemes of security seems inadequate for these novel challenges. The consideration of the execution context represents a key element for the efficiency of the security policies face to these circumstances. In other words, the system must be contextaware. This paper aims to respond to these novel challenges, by proposing a security policy, based on the „context-aware‟ notion. The system is conceived as a multi-agents system. The latter is chosen since its mobility and autonomy properties facilitate the achievement of desired objectives. Furthermore, the use of ontology permits to increase the objectivity and eliminate the eventual semantic heterogeneities. The paper presents firstly the general architecture before detailing each one of its components, as well as the distinguished consideration of the context-aware and semantic aspects.},
  eventtitle = {{{ICC}} '17: {{Second International Conference}} on {{Internet}} of {{Things}}, {{Data}} and {{Cloud Computing}}},
  isbn = {978-1-4503-4774-7},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\L2KQHXSA\Cheribi et al. - 2017 - Context-awareness security model based on multi-ag.pdf}
}

@article{cheyneCreatedLanguagesScience2008,
  title = {Created {{Languages}} in {{Science Fiction}}},
  author = {Cheyne, Ria},
  date = {2008},
  journaltitle = {Science Fiction Studies},
  volume = {35},
  number = {3},
  eprint = {25475175},
  eprinttype = {jstor},
  pages = {386--403},
  url = {http://www.jstor.org/stable/25475175},
  file = {C:\Users\ron\Zotero\storage\6USQM56W\Cheyne-CreatedLanguagesScience-2008.pdf}
}

@inproceedings{chongIntegratedTimingAnalysis2013,
  title = {Integrated {{Timing Analysis}} of {{Application}} and {{Operating Systems Code}}},
  booktitle = {2013 {{IEEE}} 34th {{Real-Time Systems Symposium}}},
  author = {Chong, Lee Kee and Ballabriga, Clement and Pham, Van-Thuan and Chattopadhyay, Sudipta and Roychoudhury, Abhik},
  date = {2013-12},
  pages = {128--139},
  publisher = {{IEEE}},
  location = {{Vancouver, BC, Canada}},
  doi = {10.1109/RTSS.2013.21},
  url = {http://ieeexplore.ieee.org/document/6728868/},
  urldate = {2022-06-21},
  abstract = {Real-time embedded software often runs on a supervisory operating system software layer on top of a modern processor. Thus, to give timing guarantees on the execution time and response time of such applications, one needs to consider the timing effects of the operating system, such as system calls and interrupts — over and above modeling the timing effects of micro-architectural features such as pipeline and cache. Previous works on Worst-case Execution Time (WCET) analysis have focused on micro-architectural modeling while ignoring the operating system’s timing effects. As a result, WCET analyzers only estimate the maximum un-interrupted execution time of a program. In this work, we present a framework for RTOS aware WCET analysis - where the timing effects of system calls and interrupts can be accounted for. The key observation behind our analysis is to capture the timing effects of system calls and/or interrupts, as well as their effect on the micro-architectural states, compositionally via a damage function. This damage function is then composed in a controlled fashion to result in a RTOS-aware, micro-architecture-aware timing analysis of an application. We show the use of our analysis to compute the worst-case response time for a real-life robot controller software which runs several tasks such as balancing and/or navigation on top of a real-time operating system running on a modern processor.},
  eventtitle = {2013 {{IEEE}} 34th {{Real-Time Systems Symposium}} ({{RTSS}})},
  isbn = {978-1-4799-2006-8},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\CJ4NWL28\Chong et al. - 2013 - Integrated Timing Analysis of Application and Oper.pdf}
}

@misc{CISC0680Research,
  title = {{{CISC}} 0680 {{Research Assignment}}},
  file = {C:\Users\ron\Zotero\storage\X7XN98WW\research_pa.pdf}
}

@article{clarkCaninePretrainingEfficient2022,
  title = {\textsc{Canine} : {{Pre-training}} an {{Efficient Tokenization-Free Encoder}} for {{Language Representation}}},
  author = {Clark, Jonathan H. and Garrette, Dan and Turc, Iulia and Wieting, John},
  date = {2022-01-31},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {73--91},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00448},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00448/109284/Canine-Pre-training-an-Efficient-Tokenization-Free},
  urldate = {2023-08-14},
  abstract = {Abstract              Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model’s ability to adapt. In this paper, we present Canine, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. Canine outperforms a comparable mBert model by 5.7 F1 on TyDi QA, a challenging multilingual benchmark, despite having fewer model parameters.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\RNUAVHAT\tacl_a_00448.pdf}
}

@inproceedings{claymanLessonsOperatingSystems2018,
  title = {Lessons from Operating Systems for Layering and Abstractions in {{5G}} Networks},
  booktitle = {{{NOMS}} 2018 - 2018 {{IEEE}}/{{IFIP Network Operations}} and {{Management Symposium}}},
  author = {Clayman, Stuart and Tuncer, Daphne},
  date = {2018-04},
  pages = {1--7},
  issn = {2374-9709},
  doi = {10.1109/NOMS.2018.8406276},
  abstract = {The trend of softwarization within networks, such as SDN, NFV, and 5G, has increased the need for a network operating system and all its associated elements. In this paper we discuss how the layering and abstractions that commonly appear within computer operating systems are missing within the domain of networking. As such, the lack of abstractions means that interacting with and managing networks has already become a difficult and sometimes cumbersome task. The softwarization process is magnifying this effect. The abstractions that appear in operating systems hide the underlying features of the hardware, presenting elements to programmers and system managers that are easier to understand and to interact with. In this sense, the networking world is far behind. We present some of the lessons learned from these operating system abstractions and consider what could appear in the networking world.},
  eventtitle = {{{NOMS}} 2018 - 2018 {{IEEE}}/{{IFIP Network Operations}} and {{Management Symposium}}},
  keywords = {5G mobile communication,Hardware,Instruction sets,Memory management,Operating systems,Task analysis},
  file = {C:\Users\ron\Zotero\storage\SPREQDVJ\Lessons_from_operating_systems_for_layering_and_abstractions_in_5G_networks.pdf}
}

@inproceedings{collardDefinitionInformationSecurity2017,
  title = {A Definition of {{Information Security Classification}} in Cybersecurity Context},
  booktitle = {2017 11th {{International Conference}} on {{Research Challenges}} in {{Information Science}} ({{RCIS}})},
  author = {Collard, Guillaume and Ducroquet, Stéphane and Disson, Eric and Talens, Guilaine},
  date = {2017-05},
  pages = {77--82},
  issn = {2151-1357},
  doi = {10.1109/RCIS.2017.7956520},
  abstract = {The concept of Information Security Classification is variable and sometimes uninformative. Most of definitions are coming from Standards and weren't updated for years even if the scope and the challenges in security are now becoming larger with Cybersecurity. Based on a literature review, we propose a new definition of Information Security Classification.},
  eventtitle = {2017 11th {{International Conference}} on {{Research Challenges}} in {{Information Science}} ({{RCIS}})},
  keywords = {Bibliographies,Computer security,Cybersecurity,Data Categorization,Data Classification,Data Security Categorization,Data Security Classification,Information Classification,Information security,Information Security Categorization,Information Security Classification,Law,Sensitivity},
  file = {C\:\\Users\\ron\\Zotero\\storage\\PGFEIHQ8\\Collard et al. - 2017 - A definition of Information Security Classificatio.pdf;C\:\\Users\\ron\\Zotero\\storage\\4L6HUV3P\\7956520.html}
}

@article{conradiVersionModelsSoftware1998,
  title = {Version Models for Software Configuration Management},
  author = {Conradi, Reidar and Westfechtel, Bernhard},
  date = {1998-06-01},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {30},
  number = {2},
  pages = {232--282},
  issn = {0360-0300},
  doi = {10.1145/280277.280280},
  url = {https://dl.acm.org/doi/10.1145/280277.280280},
  urldate = {2023-04-25},
  abstract = {After more than 20 years of research and practice in software configuration management (SCM), constructing consistent configurations of versioned software products still remains a challenge. This article focuses on the version models underlying both commercial systems and research prototypes. It provides an overview and classification of different versioning paradigms and defines and relates fundamental concepts such as revisions, variants, configurations, and changes. In particular, we focus on intensional versioning, that is, construction of versions based on configuration rules. Finally, we provide an overview of systems that have had significant impact on the development of the SCM discipline and classify them according to a detailed taxonomy.},
  keywords = {changes,configuration rules,configurations,revisions,variants,versions}
}

@inbook{coonLinguisticsArrivalHeptapods2020,
  title = {The Linguistics of {{Arrival}}: {{Heptapods}}, Field Linguistics, and {{Universal Grammar}}},
  shorttitle = {The Linguistics of {{Arrival}}},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Coon, Jessica},
  date = {2020-08-12},
  pages = {32--48},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0004},
  url = {https://academic.oup.com/book/31973/chapter/267717021},
  urldate = {2023-08-24},
  abstract = {If aliens arrived, could we communicate with them? What are the tools linguists use to decipher unknown languages? How different can languages be from one another? Do these differences have bigger consequences for how we see the world? This chapter addresses these questions through the lens of the 2016 science-fiction film               Arrival               and the real-life work of language documentation (in particular, the Mayan language Ch’ol). In               Arrival               , linguistics professor Dr. Louise Banks is recruited by the military to translate the language of the newly arrived alien heptapods. Her job is to find the answer to the question everyone is asking: why are they here? Language is a crucial piece of the answer. This chapter discusses the themes which               Arrival               has brought to the mainstream, including Universal Grammar, the Sapir-Whorf hypothesis, and the importance of linguistic fieldwork.},
  bookauthor = {Coon, Jessica},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@article{craigenDefiningCybersecurity2014,
  title = {Defining {{Cybersecurity}}},
  author = {Craigen, Dan and Diakun-Thibault, Nadia and Purse, Randy},
  date = {2014},
  journaltitle = {Technology Innovation Management Review},
  volume = {4},
  number = {10},
  pages = {13--21},
  publisher = {{Talent First Network}},
  location = {{Ottawa}},
  issn = {1927-0321},
  file = {C:\Users\ron\Zotero\storage\URMHDCD3\835.html}
}

@article{crowMSDOSMemoryEnvironment1996,
  title = {The {{MS-DOS}} Memory Environment},
  author = {Crow, Jerry},
  date = {1996-01-01},
  journaltitle = {ACM SIGICE Bulletin},
  shortjournal = {SIGICE Bull.},
  volume = {21},
  number = {3},
  pages = {2--16},
  issn = {1558-1144},
  doi = {10.1145/226036.226037},
  url = {https://doi.org/10.1145/226036.226037},
  urldate = {2022-06-23},
  abstract = {When I first started experimenting with and using MS-DOS in 1986, I was confused by the various types of memory available with the then contemporary PCs: expanded, extended, enhanced expanded, etc. I found that I was frequently asked to explain the concepts associated with these terms and, frankly, could not do so. I decided to remedy that situation by researching the subject and acquiring sufficient knowledge to permit me to understand what was happening from a hardware and software standpoint vis a vis the MS-DOS memory environment. When I had accomplished that task, I wrote an article for the Phoenix PC Users Group Newsletter documenting my new found knowledge. The original article was published in 1989.},
  file = {C:\Users\ron\Zotero\storage\64ECAUD4\226036.226037.pdf}
}

@article{dabreSurveyMultilingualNeural2021,
  title = {A {{Survey}} of {{Multilingual Neural Machine Translation}}},
  author = {Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
  date = {2021-09-30},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  number = {5},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3406095},
  url = {https://dl.acm.org/doi/10.1145/3406095},
  urldate = {2023-10-21},
  abstract = {We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart, because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and, hence, deserve further exploration. In this article, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues, and challenges. Wherever possible, we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions for MNMT. This article is aimed towards both beginners and experts in NMT. We hope this article will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT.},
  langid = {english}
}

@article{dardenneGoaldirectedRequirementsAcquisition1993,
  title = {Goal-Directed Requirements Acquisition},
  author = {Dardenne, Anne and van Lamsweerde, Axel and Fickas, Stephen},
  options = {useprefix=true},
  date = {1993-04-01},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  volume = {20},
  number = {1},
  pages = {3--50},
  issn = {0167-6423},
  doi = {10.1016/0167-6423(93)90021-G},
  url = {https://www.sciencedirect.com/science/article/pii/016764239390021G},
  urldate = {2022-10-27},
  abstract = {Requirements analysis includes a preliminary acquisition step where a global model for the specification of the system and its environment is elaborated. This model, called requirements model, involves concepts that are currently not supported by existing formal specification languages, such as goals to be achieved, agents to be assigned, alternatives to be negotiated, etc. The paper presents an approach to requirements acquisition which is driven by such higher-level concepts. Requirements models are acquired as instances of a conceptual meta-model. The latter can be represented as a graph where each node captures an abstraction such as, e.g., goal, action, agent, entity, or event, and where the edges capture semantic links between such abstractions. Well-formedness properties on nodes and links constrain their instances—that is, elements of requirements models. Requirements acquisition processes then correspond to particular ways of traversing the meta-model graph to acquire appropriate instances of the various nodes and links according to such constraints. Acquisition processes are governed by strategies telling which way to follow systematically in that graph; at each node specific tactics can be used to acquire the corresponding instances. The paper describes a significant portion of the meta-model related to system goals, and one particular acquisition strategy where the meta-model is traversed backwards from such goals. The meta-model and the strategy are illustrated by excerpts of a university library system.},
  langid = {english},
  keywords = {conceptual modeling,domain analysis,meta-level inference,nonfunctional requirements,Requirements engineering,specification acquisition,specification reuse},
  file = {C\:\\Users\\ron\\Zotero\\storage\\JHAPSLRK\\Dardenne et al. - 1993 - Goal-directed requirements acquisition.pdf;C\:\\Users\\ron\\Zotero\\storage\\7KGYZKH6\\016764239390021G.html}
}

@inreference{Declension2023,
  title = {Declension},
  booktitle = {Wikipedia},
  date = {2023-09-25T00:16:41Z},
  url = {https://en.wikipedia.org/w/index.php?title=Declension&oldid=1176949624},
  urldate = {2023-10-31},
  abstract = {In linguistics, declension (verb: to decline) is the changing of the form of a word, generally to express its syntactic function in the sentence, by way of some inflection. Declensions may apply to nouns, pronouns, adjectives, adverbs, and articles to indicate number (e.g. singular, dual, plural), case (e.g. nominative case, accusative case, genitive case, dative case), gender (e.g. masculine, neuter, feminine), and a number of other grammatical categories. Meanwhile, the inflectional change of verbs is called conjugation. Declension occurs in many of the world's languages. It is an important aspect of language families like Quechuan (i.e., languages native to the Andes), Indo-European (e.g. German, Icelandic, Lithuanian, Latvian, Slavic, Sanskrit, Latin, Ancient Greek, Modern Greek, Albanian, Classical Armenian and Modern Armenian and Kurdish), Bantu (e.g. Zulu, Kikuyu), Semitic (e.g. Modern Standard Arabic), Finno-Ugric (e.g. Hungarian, Finnish, Estonian), and Turkic (e.g. Turkish). Old English was an inflectional language, but largely abandoned inflectional changes as it evolved into Modern English. Though traditionally classified as synthetic, Modern English has moved towards a mostly analytic language.},
  langid = {english},
  annotation = {Page Version ID: 1176949624}
}

@online{departmentofhomlandsecurityGlossaryCommonCybersecurity2014,
  title = {A {{Glossary}} of {{Common Cybersecurity Words}} and {{Phrases}}},
  author = {{Department of Homland Security}},
  year = {October 1,k 2014}
}

@article{desantoFiniteStateTextProcessing2023,
  title = {Finite-{{State Text Processing}}},
  author = {De Santo, Aniello},
  date = {2023-03-01},
  journaltitle = {Computational Linguistics},
  volume = {49},
  number = {1},
  pages = {245--247},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_r_00466},
  url = {https://direct.mit.edu/coli/article/49/1/245/113644/Finite-State-Text-Processing},
  urldate = {2023-08-14},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\SCNCSS5A\coli_r_00466.pdf}
}

@inproceedings{devriesAutomaticDetectionIncomplete2016,
  title = {Automatic Detection of Incomplete Requirements via Symbolic Analysis},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 19th {{International Conference}} on {{Model Driven Engineering Languages}} and {{Systems}}},
  author = {DeVries, Byron and Cheng, Betty H. C.},
  date = {2016-10-02},
  pages = {385--395},
  publisher = {{ACM}},
  location = {{Saint-malo France}},
  doi = {10.1145/2976767.2976791},
  url = {https://dl.acm.org/doi/10.1145/2976767.2976791},
  urldate = {2022-10-22},
  abstract = {The usefulness of a system specification depends in part on the completeness of the requirements. However, enumerating all necessary requirements is difficult, especially when requirements interact with an unpredictable environment. A specification built with an idealized environmental view is incomplete if it does not include requirements to handle non-idealized behavior. Often incomplete requirements are not detected until implementation, testing, or worse, after deployment. Even when performed during requirements analysis, detecting incomplete requirements is typically an error prone, tedious, and manual task. This paper introduces Ares, a design-time approach for detecting incomplete requirements decomposition using symbolic analysis of hierarchical requirements models. We illustrate our approach by applying Ares to a requirements model of an industrybased automotive adaptive cruise control system. Ares is able to automatically detect specific instances of incomplete requirements decompositions at design-time, many of which are subtle and would be difficult to detect, either manually or with testing.},
  eventtitle = {{{MODELS}} '16: {{ACM}}/{{IEEE}} 19th {{International Conference}} on {{Model Driven Engineering Languages}} and {{Systems}}},
  isbn = {978-1-4503-4321-3},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\9BNMAWD2\DeVries and Cheng - 2016 - Automatic detection of incomplete requirements via.pdf}
}

@article{diamondGenlangsZipfLaw2023,
  title = {"{{Genlangs}}" and {{Zipf}}'s {{Law}}: {{Do}} Languages Generated by {{ChatGPT}} Statistically Look Human?},
  shorttitle = {"{{Genlangs}}" and {{Zipf}}'s {{Law}}},
  author = {Diamond, Justin},
  date = {2023},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2304.12191},
  url = {https://arxiv.org/abs/2304.12191},
  urldate = {2023-08-21},
  abstract = {OpenAI's GPT-4 is a Large Language Model (LLM) that can generate coherent constructed languages, or "conlangs," which we propose be called "genlangs" when generated by Artificial Intelligence (AI). The genlangs created by ChatGPT for this research (Voxphera, Vivenzia, and Lumivoxa) each have unique features, appear facially coherent, and plausibly "translate" into English. This study investigates whether genlangs created by ChatGPT follow Zipf's law. Zipf's law approximately holds across all natural and artificially constructed human languages. According to Zipf's law, the word frequencies in a text corpus are inversely proportional to their rank in the frequency table. This means that the most frequent word appears about twice as often as the second most frequent word, three times as often as the third most frequent word, and so on. We hypothesize that Zipf's law will hold for genlangs because (1) genlangs created by ChatGPT fundamentally operate in the same way as human language with respect to the semantic usefulness of certain tokens, and (2) ChatGPT has been trained on a corpora of text that includes many different languages, all of which exhibit Zipf's law to varying degrees. Through statistical linguistics, we aim to understand if LLM-based languages statistically look human. Our findings indicate that genlangs adhere closely to Zipf's law, supporting the hypothesis that genlangs created by ChatGPT exhibit similar statistical properties to natural and artificial human languages. We also conclude that with human assistance, AI is already capable of creating the world's first fully-functional genlang, and we call for its development.},
  version = {1},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\ron\\Downloads\\2304.12191.pdf;C\:\\Users\\ron\\Zotero\\storage\\PIAUNHSK\\2304.12191.pdf}
}

@inproceedings{dolan-gavittTappanZeeNorth2013,
  title = {Tappan {{Zee}} (North) Bridge: Mining Memory Accesses for Introspection},
  shorttitle = {Tappan {{Zee}} (North) Bridge},
  booktitle = {Proceedings of the 2013 {{ACM SIGSAC}} Conference on {{Computer}} \& Communications Security},
  author = {Dolan-Gavitt, Brendan and Leek, Tim and Hodosh, Josh and Lee, Wenke},
  date = {2013-11-04},
  series = {{{CCS}} '13},
  pages = {839--850},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2508859.2516697},
  url = {https://doi.org/10.1145/2508859.2516697},
  urldate = {2022-06-20},
  abstract = {The ability to introspect into the behavior of software at runtime is crucial for many security-related tasks, such as virtual machine-based intrusion detection and low-artifact malware analysis. Although some progress has been made in this task by automatically creating programs that can passively retrieve kernel-level information, two key challenges remain. First, it is currently difficult to extract useful information from user-level applications, such as web browsers. Second, discovering points within the OS and applications to hook for active monitoring is still an entirely manual process. In this paper we propose a set of techniques to mine the memory accesses made by an operating system and its applications to locate useful places to deploy active monitoring, which we call tap points. We demonstrate the efficacy of our techniques by finding tap points for useful introspection tasks such as finding SSL keys and monitoring web browser activity on five different operating systems (Windows 7, Linux, FreeBSD, Minix and Haiku) and two processor architectures (ARM and x86).},
  isbn = {978-1-4503-2477-9},
  keywords = {active monitoring,introspection,reverse engineering},
  file = {C:\Users\ron\Zotero\storage\HFQXF4ES\Dolan-Gavitt et al. - 2013 - Tappan Zee (north) bridge mining memory accesses .pdf}
}

@inproceedings{dovgalyukQEMUbasedFrameworkNonintrusive2017,
  title = {{{QEMU-based}} Framework for Non-Intrusive Virtual Machine Instrumentation and Introspection},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Dovgalyuk, Pavel and Fursova, Natalia and Vasiliev, Ivan and Makarov, Vladimir},
  date = {2017-08-21},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {944--948},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3106237.3122817},
  url = {https://doi.org/10.1145/3106237.3122817},
  urldate = {2022-06-21},
  abstract = {This paper presents the framework based on the emulator QEMU. Our framework provides set of multi-platform analysis tools for the virtual machines and mechanism for creating instrumentation and analysis tools. Our framework is based on a lightweight approach to dynamic analysis of binary code executed in virtual machines. This approach is non-intrusive and provides system-wide analysis capabilities. It does not require loading any guest agents and source code of the OS. Therefore it may be applied to ROM-based guest systems and enables using of record/replay of the system execution. We use application binary interface (ABI) of the platform to be analyzed for creating introspection tools. These tools recover the part of kernel-level information related to the system calls executed on the guest machine.},
  isbn = {978-1-4503-5105-8},
  keywords = {ABI,Dynamic analysis,Introspection,QEMU,Software instrumentation,Virtual machine},
  file = {C:\Users\ron\Zotero\storage\AZNP8UVD\Dovgalyuk et al. - 2017 - QEMU-based framework for non-intrusive virtual mac.pdf}
}

@online{droutKingAlfredGrammar,
  title = {King {{Alfred Grammar}}},
  author = {Drout, Michael D.C.},
  editora = {Kapelle, Rachel},
  editoratype = {collaborator},
  url = {https://people.umass.edu/sharris/in/gram/KingAlfredGrammar.html},
  urldate = {2023-10-31}
}

@dataset{dryerWALSOnlineV2020,
  title = {{{WALS Online}} (V2020.3)},
  author = {Dryer and Haspelmath, Martin},
  doi = {10.5281/zenodo.7385533},
  url = {https://wals.info/},
  abstract = {The World Atlas of Language Structures (WALS) is a large database of structural (phonological, grammatical, lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors.}
}

@online{duncanPCjsMSDOSEncyclopedia1988,
  title = {{{PCjs}}: {{The MS-DOS Encyclopedia}}: {{Section I}}: {{The Development}} of {{MS-DOS}}},
  shorttitle = {{{PCjs}}},
  author = {Duncan, Ray and Bostwick, Steve and Burgoyne, Keith},
  date = {1988},
  url = {https://web.archive.org/web/20190406161852/https://www.pcjs.org/pubs/pc/reference/microsoft/mspl13/msdos/encyclopedia/section1/},
  urldate = {2022-06-23},
  organization = {{The MS-DOS Encyclopedia: Section I: The Development of MS-DOS}}
}

@inproceedings{dunnExploringConstructiconLinguistic2023,
  title = {Exploring the {{Constructicon}}: {{Linguistic Analysis}} of a {{Computational CxG}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Construction Grammars}} and {{NLP}} ({{CxGs}}+{{NLP}}, {{GURT}}/{{SyntaxFest}} 2023)},
  author = {Dunn, Jonathan},
  date = {2023-03},
  pages = {1--11},
  publisher = {{Association for Computational Linguistics}},
  location = {{Washington DC}},
  url = {https://aclanthology.org/2023.cxgsnlp-1.1},
  abstract = {Recent work has formulated the task for computational construction grammar as producing a constructicon given a corpus of usage. Previous work has evaluated these unsupervised grammars using both internal metrics (for example, Minimum Description Length) and external metrics (for example, performance on a dialectology task). This paper instead takes a linguistic approach to evaluation, first learning a constructicon and then analyzing its contents from a linguistic perspective. This analysis shows that a learned constructicon can be divided into nine major types of constructions, of which Verbal and Nominal are the most common. The paper also shows that both the token and type frequency of constructions can be used to model variation across registers and dialects.},
  eventtitle = {First {{International Workshop}} on {{Construction Grammars}} and {{NLP}} ({{CxGs}}+{{NLP}}, {{GURT}}/{{SyntaxFest}} 2023)},
  file = {C\:\\Users\\ron\\Zotero\\storage\\I6AWRBWK\\2023.cxgsnlp-1.1.mp4;C\:\\Users\\ron\\Zotero\\storage\\XCGZQXXM\\2023.cxgsnlp-1.1.pdf}
}

@dataset{dunnReplicationDataExposure2022,
  title = {Replication {{Data}} for: {{Exposure}} and {{Emergence}} in {{Usage-Based Grammar}}: {{Computational Experiments}} in 35 {{Languages}}},
  shorttitle = {Replication {{Data}} For},
  author = {Dunn, Jonathan},
  editora = {Dunn, Jonathan and {University Of Canterbury}},
  editoratype = {collaborator},
  date = {2022},
  publisher = {{DataverseNO}},
  doi = {10.18710/CES0L8},
  url = {https://dataverse.no/citation?persistentId=doi:10.18710/CES0L8},
  urldate = {2023-09-01},
  abstract = {[article abstract:] This paper uses computational experiments to explore the role of exposure in the emergence of construction grammars. While usage-based grammars are hypothesized to depend on a learner’s exposure to actual language use, the mechanisms of such exposure have only been studied in a few constructions in isolation. This paper experiments with (i) the growth rate of the constructicon, (ii) the convergence rate of grammars exposed to independent registers, and (iii) the rate at which constructions are forgotten when they have not been recently observed. These experiments show that the lexicon grows more quickly than the grammar and that the growth rate of the grammar is not dependent on the growth rate of the lexicon. At the same time, register-specific grammars converge onto more similar constructions as the amount of exposure increases. This means that the influence of specific registers becomes less important as exposure increases. Finally, the rate at which constructions are forgotten when they have not been recently observed mirrors the growth rate of the constructicon. This paper thus presents a computational model of usage-based grammar that includes both the emergence and the unentrenchment of constructions. [dataset abstract:] This dataset consists of three zip folders containing the main analysis represented in the related publication as well as a number of separate corpus files that serve as the raw input to grammar learning.}
}

@inproceedings{easttomModifiedMcCumberCube2019,
  title = {A {{Modified McCumber Cube}} as a {{Basis}} for a {{Taxonomy}} of {{Cyber Attacks}}},
  booktitle = {2019 {{IEEE}} 9th {{Annual Computing}} and {{Communication Workshop}} and {{Conference}} ({{CCWC}})},
  author = {Easttom, Chuck and Butler, William},
  date = {2019-01},
  pages = {0943--0949},
  doi = {10.1109/CCWC.2019.8666559},
  abstract = {There currently exist taxonomies for malware, however malware is only one type of attack. Malware taxonomies are usually targeted at a specific scenario and are not easily translatable to other attacks that may not involve malware. There are other taxonomies that examine specific types of attacks such as attacks on embedded systems. Most current taxonomies do not cover all types of cyber-attacks. Those that do, don't examine all the dimensions of an attack. There is a need for a comprehensive attack taxonomy that explores all dimensions of any attack. The McCumber cube is a methodology for examining security from three dimensions. This provides a foundation for creating an attack taxonomy. The taxonomy views cyber-attacks as how they counter the security aspects of the McCumber cube.},
  eventtitle = {2019 {{IEEE}} 9th {{Annual Computing}} and {{Communication Workshop}} and {{Conference}} ({{CCWC}})},
  keywords = {attack taxonomy,cyber-attacks,Cyberattack,Data security,Embedded systems,malware,malware taxonomy,McCumber Cube,Taxonomy,Trojan horses},
  file = {C\:\\Users\\ron\\Zotero\\storage\\ENK3KKL8\\Easttom and Butler - 2019 - A Modified McCumber Cube as a Basis for a Taxonomy.pdf;C\:\\Users\\ron\\Zotero\\storage\\MN3DFVR4\\8666559.html}
}

@inproceedings{eckhardtAreNonfunctionalRequirements2016,
  title = {Are "Non-Functional" Requirements Really Non-Functional? An Investigation of Non-Functional Requirements in Practice},
  shorttitle = {Are "Non-Functional" Requirements Really Non-Functional?},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Eckhardt, Jonas and Vogelsang, Andreas and Fernández, Daniel Méndez},
  date = {2016-05-14},
  series = {{{ICSE}} '16},
  pages = {832--842},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2884781.2884788},
  url = {https://doi.org/10.1145/2884781.2884788},
  urldate = {2022-10-22},
  abstract = {Non-functional requirements (NFRs) are commonly distinguished from functional requirements by differentiating how the system shall do something in contrast to what the system shall do. This distinction is not only prevalent in research, but also influences how requirements are handled in practice. NFRs are usually documented separately from functional requirements, without quantitative measures, and with relatively vague descriptions. As a result, they remain difficult to analyze and test. Several authors argue, however, that many so-called NFRs actually describe behavioral properties and may be treated the same way as functional requirements. In this paper, we empirically investigate this point of view and aim to increase our understanding on the nature of NFRs addressing system properties. We report on the classification of 530 NFRs extracted from 11 industrial requirements specifications and analyze to which extent these NFRs describe system behavior. Our results suggest that most "non-functional" requirements are not non-functional as they describe behavior of a system. Consequently, we argue that many so-called NFRs can be handled similarly to functional requirements.},
  isbn = {978-1-4503-3900-1},
  keywords = {classification,empirical studies,model-based development,non-functional requirements},
  file = {C:\Users\ron\Zotero\storage\WW39DK3I\Eckhardt et al. - 2016 - Are non-functional requirements really non-funct.pdf}
}

@inproceedings{ederConfiguringLatentSemantic2015,
  title = {Configuring Latent Semantic Indexing for Requirements Tracing},
  booktitle = {Proceedings of the {{Second International Workshop}} on {{Requirements Engineering}} and {{Testing}}},
  author = {Eder, Sebastian and Femmer, Henning and Hauptmann, Benedikt and Junker, Maximilian},
  date = {2015-05-16},
  series = {{{RET}} '15},
  pages = {27--33},
  publisher = {{IEEE Press}},
  location = {{Florence, Italy}},
  abstract = {Latent Semantic Indexing (LSI) is an accepted technique for information retrieval that is used in requirements tracing to recover links between artifacts, e.g., between requirements documents and test cases. However, configuring LSI is difficult, because the number of possible configurations is huge. The configuration of LSI, which depends on the underlying dataset, greatly influences the accuracy of the results. Therefore, one of the key challenges for applying LSI is finding an appropriate configuration. Evaluating results for each configuration is time consuming, and therefore, automatically determining an appropriate configuration for LSI improves the applicability of LSI based methods. We propose a fully automated technique to determine appropriate configurations for LSI to recover links between requirements artifacts. We evaluate our technique on six sets of requirements artifacts from industry and academia and show that the configurations selected by our approach yield results that are almost as accurate as results from configurations based on a ground truth like known links or expert knowledge. Our approach improves the applicability of LSI in industry and academia, as researchers and practitioners do not need to determine appropriate configurations manually or provide a ground truth.},
  file = {C:\Users\ron\Zotero\storage\7ZAEJCM8\Eder et al. - 2015 - Configuring latent semantic indexing for requireme.pdf}
}

@inproceedings{elkhannoubiFundamentalPillarsEffective2015,
  title = {Fundamental Pillars for an Effective Cybersecurity Strategy},
  booktitle = {2015 {{IEEE}}/{{ACS}} 12th {{International Conference}} of {{Computer Systems}} and {{Applications}} ({{AICCSA}})},
  author = {Elkhannoubi, Hasna and Belaissaoui, Mustapha},
  date = {2015-11},
  pages = {1--2},
  issn = {2161-5330},
  doi = {10.1109/AICCSA.2015.7507241},
  abstract = {An effective national cybersecurity strategy conforms to international standards serves to preserve organization's information assets. The principle is to support a cybersecurity consistent approach including legal, technological and organizational dimensions. The promise of this research program is to identify the key areas for an effective cybersecurity strategy addressed to organizations (administration or enterprise) in order to develop a favorable recommendation framework for cybersecurity strategy implementation that ensure an acceptable information security level. Our main aim is to develop a platform for an effective cybersecurity strategy build on a successful experience of cybersecurity strategies and turn around international standard and referential such as ISO27002 and ITIL.},
  eventtitle = {2015 {{IEEE}}/{{ACS}} 12th {{International Conference}} of {{Computer Systems}} and {{Applications}} ({{AICCSA}})},
  keywords = {Computer crime,cybersecurity,Europe,information,Law,legal,organizational,strategy,technological},
  file = {C\:\\Users\\ron\\Zotero\\storage\\XJKV32BR\\Elkhannoubi and Belaissaoui - 2015 - Fundamental pillars for an effective cybersecurity.pdf;C\:\\Users\\ron\\Zotero\\storage\\6ARB7ELW\\7507241.html}
}

@inproceedings{elmalakiCAreDroidAdaptationFramework2015,
  title = {{{CAreDroid}}: {{Adaptation Framework}} for {{Android Context-Aware Applications}}},
  shorttitle = {{{CAreDroid}}},
  booktitle = {Proceedings of the 21st {{Annual International Conference}} on {{Mobile Computing}} and {{Networking}}},
  author = {Elmalaki, Salma and Wanner, Lucas and Srivastava, Mani},
  date = {2015-09-07},
  pages = {386--399},
  publisher = {{ACM}},
  location = {{Paris France}},
  doi = {10.1145/2789168.2790108},
  url = {https://dl.acm.org/doi/10.1145/2789168.2790108},
  urldate = {2022-05-15},
  abstract = {Context-awareness is the ability of software systems to sense and adapt to their physical environment. Many contemporary mobile applications adapt to changing locations, connectivity states, available computational and energy resources, and proximity to other users and devices. Nevertheless, there is little systematic support for context-awareness in contemporary mobile operating systems. Because of this, application developers must build their own context-awareness adaptation engines, dealing directly with sensors and polluting application code with complex adaptation decisions. In this paper, we introduce CAreDroid, which is a framework that is designed to decouple the application logic from the complex adaptation decisions in Android context-aware applications. In this framework, developers are required—only—to focus on the application logic by providing a list of methods that are sensitive to certain contexts along with the permissible operating ranges under those contexts. At run time, CAreDroid monitors the context of the physical environment and intercepts calls to sensitive methods, activating only the blocks of code that best fit the current physical context.},
  eventtitle = {{{MobiCom}}'15: {{The}} 21th {{Annual International Conference}} on {{Mobile Computing}} and {{Networking}}},
  isbn = {978-1-4503-3619-2},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\RZSM7L6N\Elmalaki et al. - 2015 - CAreDroid Adaptation Framework for Android Contex.pdf}
}

@incollection{ericraymondCathedralBazaar2001,
  title = {The {{Cathedral}} and the {{Bazaar}}},
  booktitle = {The {{Cathedral}} and the {{Bazaar}}: {{Musings}} on {{Linux}} and {{Open Source}} by an {{Accidental Revolutionary}}},
  author = {{Eric Raymond}},
  date = {2001-02},
  publisher = {{O'Reilly Media}},
  isbn = {1-56592-724-9},
  file = {C:\Users\ron\Zotero\storage\DADDE523\The Cathedral and the Bazzar.docx}
}

@online{EvolutionUnixSystem,
  title = {Evolution of the {{Unix System Architecture}}: {{An Exploratory Case Study}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/8704965},
  urldate = {2022-05-20},
  file = {C:\Users\ron\Zotero\storage\9BM2FKP8\8704965.html}
}

@online{ExocompilationProductiveProgramming,
  title = {Exocompilation for Productive Programming of Hardware Accelerators | {{Proceedings}} of the 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  url = {https://dl.acm.org/doi/abs/10.1145/3519939.3523446},
  urldate = {2022-07-15},
  file = {C\:\\Users\\ron\\Zotero\\storage\\MSZIK7Z2\\Exocompilation for productive programming of hardw.pdf;C\:\\Users\\ron\\Zotero\\storage\\754XR6J9\\3519939.html}
}

@online{ExploringAgile,
  title = {Exploring Agile},
  doi = {10.1145/1370143.1370144},
  url = {https://dl.acm.org/doi/epdf/10.1145/1370143.1370144},
  urldate = {2022-11-09},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\642AZ6ST\1370143.html}
}

@article{faganAdvancesSoftwareInspections1986,
  title = {Advances in Software Inspections},
  author = {Fagan, Michael E.},
  date = {1986-07},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {SE-12},
  number = {7},
  pages = {744--751},
  issn = {1939-3520},
  doi = {10.1109/TSE.1986.6312976},
  abstract = {Software inspection is a method of static testing to verify that software meets its requirements. It engages the developers and others in a formal process of investigation that usually detects more defects in the product-and at lower cost-than does machine testing. Studies and experiences are presented which enhance the use of the inspection process and improve its contribution to development of defect-free software on time and at lower cost. Examples of benefits are cited followed by descriptions of the inspection process and some methods of obtaining the enhanced results. Users of the method report very significant improvements in quality that are accompanied by lower development costs and greatly reduced maintenance efforts. Excellent results have been obtained by small and large organizations in all aspects of new development as well as in maintenance. There is some evidence that developers who participate in the inspection of their own product actually create fewer defects in subsequent work. Because inspections formalize the development process, productivity-enhancing and quality-enhancing tools can be adopted more easily and rapidly.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {Defect detection,inspection,Inspection,Maintenance engineering,Process control,project management,quality assurance,software development,software engineering,software quality,Software quality,Standards,testing,Testing,walkthru},
  file = {C\:\\Users\\ron\\Zotero\\storage\\4G42UMHH\\Fagan - 1986 - Advances in software inspections.pdf;C\:\\Users\\ron\\Zotero\\storage\\ZQSDB4BZ\\stamp.html}
}

@article{faganDesignCodeInspection1976,
  title = {Design and Code Inspection to Reduce Errors in Program Development},
  author = {Fagan, Michael E.},
  date = {1976},
  journaltitle = {IBM Systems Journal},
  volume = {15},
  number = {3},
  pages = {27},
  url = {http://www.mfagan.com/pdfs/ibmfagan.pdf},
  abstract = {Substantial net improvements in programming quality and productivity have been obtained through the use of formal inspections of design and of code.  Improvements are made possible by a systematic and efficient design and code verification process, within well-defined roles for inspection participants.  The manner in which inspection data is categorized and made suitable for process analysis is an important factor in attaining the improvements.  It is shown that by using inspection results, a mechanism for initial error reduction followed by ever-improving error rates can be achieved.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\PTF6KA5T\Lampson - 1983 - Hints for Computer System Design.pdf}
}

@incollection{faganHistorySoftwareInspections2002,
  title = {A {{History}} of {{Software Inspections}}},
  booktitle = {Software {{Pioneers}}},
  author = {Fagan, Michael},
  editor = {Broy, Manfred and Denert, Ernst},
  date = {2002},
  pages = {562--573},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-59412-0_34},
  url = {https://link.springer.com/10.1007/978-3-642-59412-0_34},
  urldate = {2022-11-03},
  isbn = {978-3-642-63970-8 978-3-642-59412-0},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\FUGT98AM\Fagan - 2002 - A History of Software Inspections.pdf}
}

@online{FalsehoodsProgrammersBelieve,
  title = {Falsehoods Programmers Believe about Time},
  url = {https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time},
  urldate = {2022-06-16},
  abstract = {Over the past couple of years [I have spent a lot of time][checklist] debugging other engineers' test code.  This was interesting work, occasionally frustrating but always informative. One might not...},
  langid = {american},
  organization = {{Infinite Undo}}
}

@inproceedings{fanEnhancedJournalingMethod2010,
  title = {An Enhanced Journaling Method for Clustered File System with Shared Storage},
  booktitle = {2010 3rd {{International Conference}} on {{Advanced Computer Theory}} and {{Engineering}}({{ICACTE}})},
  author = {Fan, Zhonglei and Zhao, Xiangmo},
  date = {2010-08},
  volume = {6},
  pages = {V6-78-V6-81},
  issn = {2154-7505},
  doi = {10.1109/ICACTE.2010.5579373},
  abstract = {Most modern file systems provide journaling method instead of traditional file system check routine for fast recovery and consistency guarantee from unexpected crashes. For clustered file systems with shared storage, to improve the performance of journaling, each node gets its own journal space and each journal can be on its own disk for greater parallelism. After a failure, file system consistency is restored quickly by simply re-applying all updates recorded in the failed node's journal. However, for file system operations involving multiple nodes, this method may not be efficient enough as lock mechanism has to be employed in most cases, which may cause poorer performance and more recovery time. By introducing specific journal storage to record the transactions involving multiple nodes, an enhanced lock-free journaling method is presented in this paper, which can improve the performance of journaling operations involving multiple nodes, and then shortens recovery time from crashes. A Linux-based concept-proofing implementation has been applied in self-developed clustered file system.},
  eventtitle = {2010 3rd {{International Conference}} on {{Advanced Computer Theory}} and {{Engineering}}({{ICACTE}})},
  keywords = {Clustered File Systems,Distributed Transactions,Journaling Method,Optimized production technology,Shared Storage},
  file = {C\:\\Users\\ron\\Zotero\\storage\\EEKN6GPS\\Fan and Zhao - 2010 - An enhanced journaling method for clustered file s.pdf;C\:\\Users\\ron\\Zotero\\storage\\V3SP99HQ\\5579373.html}
}

@online{fanpuzengArtLaTeXCommon2023,
  title = {The {{Art}} of {{LaTeX}}: {{Common Mistakes}}, and {{Advice}} for {{Typsetting Beautiful}}, {{Delightful Proofs}}},
  author = {{Fan Pu Zeng}},
  date = {2023-01-02},
  url = {https://fanpu.io/blog/2023/latex-tips/},
  urldate = {2023-01-09},
  abstract = {A comments widget built on GitHub Discussions.},
  langid = {english},
  organization = {{The Art of LaTeX: Common Mistakes, and Advice for Typsetting Beautiful, Delightful Proofs}},
  file = {C:\Users\ron\Zotero\storage\B3E4S4XZ\widget.html}
}

@inproceedings{farrarComparisonStemmingTechniques2019,
  title = {A Comparison of Stemming Techniques in Tracing},
  booktitle = {Proceedings of the 10th {{International Workshop}} on {{Software}} and {{Systems Traceability}}},
  author = {Farrar, David and Hayes, Jane Huffman},
  date = {2019-05-27},
  series = {{{SST}} '19},
  pages = {37--44},
  publisher = {{IEEE Press}},
  location = {{Montreal, Quebec, Canada}},
  doi = {10.1109/SST.2019.00017},
  url = {https://doi.org/10.1109/SST.2019.00017},
  urldate = {2022-10-22},
  abstract = {We examine the effects of stemming on the tracing of software engineering artifacts. We compare two common stemming algorithms to each other as well as to a baseline of no stemming. We evaluate the algorithms on eight tracing datasets. We run the experiment using the TraceLab experimental framework to allow for ease of repeatability and knowledge sharing among the tracing community. We compare the algorithms on precision at recall levels of [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], as well as on mean average precision values. The experiment indicated that neither the Porter stemmer nor the Krovetz stemmer outperformed the other on all datasets tested.},
  keywords = {empirical research,stemming,traceability},
  file = {C:\Users\ron\Zotero\storage\R9DSGLRX\Farrar and Hayes - 2019 - A comparison of stemming techniques in tracing.pdf}
}

@article{fernandezNamingPainRequirements2017,
  title = {Naming the Pain in Requirements Engineering: {{Contemporary}} Problems, Causes, and Effects in Practice},
  shorttitle = {Naming the Pain in Requirements Engineering},
  author = {Fernández, D. Méndez and Wagner, S. and Kalinowski, M. and Felderer, M. and Mafra, P. and Vetrò, A. and Conte, T. and Christiansson, M.-T. and Greer, D. and Lassenius, C. and Männistö, T. and Nayabi, M. and Oivo, M. and Penzenstadler, B. and Pfahl, D. and Prikladnicki, R. and Ruhe, G. and Schekelmann, A. and Sen, S. and Spinola, R. and Tuzcu, A. and de la Vara, J. L. and Wieringa, R.},
  options = {useprefix=true},
  date = {2017-10},
  journaltitle = {Empirical Software Engineering},
  shortjournal = {Empir Software Eng},
  volume = {22},
  number = {5},
  pages = {2298--2338},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-016-9451-7},
  url = {http://link.springer.com/10.1007/s10664-016-9451-7},
  urldate = {2022-10-27},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\5H8LI7JL\Fernández et al. - 2017 - Naming the pain in requirements engineering Conte.pdf}
}

@inproceedings{ferrariNaturalLanguageRequirements2018,
  title = {Natural Language Requirements Processing: From Research to Practice},
  shorttitle = {Natural Language Requirements Processing},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}: {{Companion Proceeedings}}},
  author = {Ferrari, Alessio},
  date = {2018-05-27},
  series = {{{ICSE}} '18},
  pages = {536--537},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3183440.3183467},
  url = {https://doi.org/10.1145/3183440.3183467},
  urldate = {2022-10-22},
  abstract = {Automated manipulation of natural language requirements, for classification, tracing, defect detection, information extraction, and other tasks, has been pursued by requirements engineering (RE) researchers for more than two decades. Recent technological advancements in natural language processing (NLP) have made it possible to apply this research more widely within industrial settings. This technical briefing targets researchers and practitioners, and aims to give an overview of what NLP can do today for RE problems, and what could do if specific research challenges, also emerging from practical experiences, are addressed. The talk will: survey current research on applications of NLP to RE problems; present representative industrially-ready techniques, with a focus on defect detection and information extraction problems; present enabling technologies in NLP that can play a role in RE research, including distributional semantics representations; discuss criteria for evaluation of NLP techniques in the RE context; outline the main challenges for a systematic application of the techniques in industry. The crosscutting topics that will permeate the talk are the need for domain adaptation, and the essential role of the human-in-the-loop.},
  isbn = {978-1-4503-5663-3},
  keywords = {NLP,requirements engineering},
  file = {C:\Users\ron\Zotero\storage\XXQTCDMA\Ferrari - 2018 - Natural language requirements processing from res.pdf}
}

@inproceedings{ferrariNLPRequirementsEngineering2021,
  title = {{{NLP}} for Requirements Engineering: Tasks, Techniques, Tools, and Technologies},
  shorttitle = {{{NLP}} for Requirements Engineering},
  booktitle = {Proceedings of the 43rd {{International Conference}} on {{Software Engineering}}: {{Companion Proceedings}}},
  author = {Ferrari, Alessio and Zhao, Liping and Alhoshan, Waad},
  date = {2021-05-25},
  series = {{{ICSE}} '21},
  pages = {322--323},
  publisher = {{IEEE Press}},
  location = {{Virtual Event, Spain}},
  doi = {10.1109/ICSE-Companion52605.2021.00137},
  url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00137},
  urldate = {2022-10-22},
  abstract = {Requirements engineering (RE) is one of the most natural language-intensive fields within the software engineering area. Therefore, several works have been developed across the years to automate the analysis of natural language artifacts that are relevant for RE, including requirements documents, but also app reviews, privacy policies, and social media content related to software products. Furthermore, the recent diffusion of game-changing natural language processing (NLP) techniques and platforms has also boosted the interest of RE researchers. However, a reference framework to provide a holistic understanding of the field of NLP for RE is currently missing. Based on the results of a recent systematic mapping study, and stemming from a previous ICSE tutorial by one of the authors, this technical briefing gives an overview of NLP for RE tasks, available techniques, supporting tools and NLP technologies. It is oriented to both researchers and practitioners, and will gently guide the audience towards a clearer view of how NLP can empower RE, providing pointers to representative works and specialised tools.},
  file = {C:\Users\ron\Zotero\storage\4CY9IWVL\Ferrari et al. - 2021 - NLP for requirements engineering tasks, technique.pdf}
}

@standard{froumentinW3CSpeechInterface2005,
  type = {Informational},
  title = {The {{W3C Speech Interface Framework Media Types}}:   Application/Voicexml+xml, Application/Ssml+xml, Application/Srgs,  Application/Srgs+xml, Application/Ccxml+xml, and Application/Pls+xml},
  shorttitle = {{{RFC4267}}},
  author = {Froumentin, Max},
  date = {2005-11},
  number = {4267},
  url = {https://www.ietf.org/rfc/rfc4267.txt},
  urldate = {2023-10-24}
}

@misc{fukiuTIPAManual2004,
  title = {{{TIPA Manual}}},
  author = {Fukiu, Rei},
  date = {2004-03-25},
  url = {http://www.l.u-tokyo.ac.jp/~fkr/tipa/tipaman.pdf},
  organization = {{The University of Tokyo}},
  file = {C:\Users\ron\Zotero\storage\UGRGTHT3\Fukiu - 2004 - TIPA Manual.pdf}
}

@inproceedings{gaborConstructionsCollocationsPatterns2023,
  title = {Constructions, {{Collocations}}, and {{Patterns}}: {{Alternative Ways}} of {{Construction Identification}} in a {{Usage-based}}, {{Corpus-driven Theoretical Framework}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Construction Grammars}} and {{NLP}} ({{CxGs}}+{{NLP}}, {{GURT}}/{{SyntaxFest}} 2023)},
  author = {G'abor, Simon},
  date = {2023-03},
  pages = {12--20},
  publisher = {{Association for Computational Linguistics}},
  location = {{Washington DC}},
  url = {https://aclanthology.org/2023.cxgsnlp-1.2},
  abstract = {There is a serious theoretical and methodological dilemma in usage-based construction grammar: how to identify constructions based on corpus pattern analysis. The present paper provides an overview of this dilemma, focusing on argument structure constructions (ASCs) in general. It seeks to answer the question of how a data-driven construction grammatical description can be built on the collocation data extracted from corpora. The study is of meta-scientific interest: it compares theoretical proposals in construction grammar regarding how they handle co-occurrences emerging from a corpus. Discussing alternative bottom-up approaches to the notion of construction, the paper concludes that there is no one-to-one correspondence between corpus patterns and constructions. Therefore, a careful analysis of the former can empirically ground both the identification and the description of constructions.},
  eventtitle = {The {{First International Workshop}} on {{Construction Grammars}} and {{NLP}} ({{CxGs}}+{{NLP}}, {{GURT}}/{{SyntaxFest}} 2023)},
  file = {C:\Users\ron\Zotero\storage\DMCMVJET\2023.cxgsnlp-1.2.pdf}
}

@online{gabrielRiseWorseBetter,
  title = {The {{Rise}} of ``{{Worse}} Is {{Better}}''},
  author = {Gabriel, Richard},
  url = {https://www.jwz.org/doc/worse-is-better.html},
  urldate = {2022-05-20},
  file = {C:\Users\ron\Zotero\storage\9GCCCFMY\worse-is-better.html}
}

@article{gallagherSourceForgeGrabsGIMP,
  title = {{{SourceForge}} Grabs {{GIMP}} for {{Windows}}’ Account, Wraps Installer in Bundle-Pushing Adware [{{Updated}}]},
  author = {Gallagher, Sean},
  journaltitle = {Ars Technica},
  pages = {8},
  url = {https://arstechnica.com/information-technology/2015/05/sourceforge-grabs-gimp-for-windows-account-wraps-installer-in-bundle-pushing-adware/},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\B3HATY5X\Gallagher - SourceForge grabs GIMP for Windows’ account, wraps.pdf}
}

@book{gauchatSwiftUIMasterminds2022,
  title = {SwiftUI for Masterminds},
  author = {Gauchat, John D},
  date = {2022},
  series = {For Masterminds},
  edition = {3rd Edition 2022},
  abstract = {How to take advantage of Swift and SwiftUI to create insanely grate apps for iPhones, iPads, and Macs.},
  isbn = {1-77797-822-X},
  langid = {Engish}
}

@inproceedings{gelperinExploringAgile2008,
  title = {Exploring Agile},
  booktitle = {Proceedings of the 2008 International Workshop on {{Scrutinizing}} Agile Practices or Shoot-out at the Agile Corral},
  author = {Gelperin, David},
  date = {2008-05-10},
  series = {{{APOS}} '08},
  pages = {1--3},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1370143.1370144},
  url = {https://doi.org/10.1145/1370143.1370144},
  urldate = {2022-11-09},
  abstract = {Bob attempts to figure out exactly what Agile development is and where it works by analyzing the Agile Manifesto and its supporting principles. He proposes changes to some troubling statements.},
  isbn = {978-1-60558-021-0},
  keywords = {agile development,agile manifesto},
  file = {C:\Users\ron\Zotero\storage\IQVGP28A\Gelperin - 2008 - Exploring agile.pdf}
}

@inproceedings{ghanimSpecializedQualityManagement2016,
  title = {Toward a {{Specialized Quality Management Maturity Assessment Model}}},
  booktitle = {Proceedings of the 2nd {{Africa}} and {{Middle East Conference}} on {{Software Engineering}} - {{AMECSE}} '16},
  author = {Ghanim, Yasser},
  date = {2016},
  pages = {1--8},
  publisher = {{ACM Press}},
  location = {{Cairo, Egypt}},
  doi = {10.1145/2944165.2944166},
  url = {http://dl.acm.org/citation.cfm?doid=2944165.2944166},
  urldate = {2022-12-08},
  abstract = {SW Quality Assessment models are either too broad such as CMMI-DEV and SPICE that cover the full software development life cycle (SDLC), or too narrow such as TMMI and TPI that focus on testing. Quality Management as a main concern within the software industry is broader than the concept of testing. The V-Model sets a broader view with the concepts of Verification and Validation. Quality Assurance (QA) is another broader term that includes quality of processes. Configuration audits add more scope. In parallel there are some less visible dimensions in quality not often addressed in traditional models such as business alignment of QA efforts. This paper compares the commonly accepted models related to software quality management and proposes a model that fills an empty space in this area. The paper provides some analysis of the concepts of maturity and capability levels and provides some proposed adaptations for quality management assessment.},
  eventtitle = {The 2nd {{Africa}} and {{Middle East Conference}}},
  isbn = {978-1-4503-4293-3},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\MNGHVPEV\Ghanim - 2016 - Toward a Specialized Quality Management Maturity A.pdf}
}

@inbook{gillonBringingLanguageConstruction2020,
  title = {Bringing Language Construction from the Classroom to the Community},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Gillon, Carrie and Delmonico, Edward and Martinez, Randi and Morrell, Spencer},
  date = {2020-08-12},
  pages = {137--168},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0010},
  url = {https://academic.oup.com/book/31973/chapter/267717622},
  urldate = {2023-08-24},
  abstract = {This chapter discusses the connection between constructed language courses and opportunities to connect with the wider community. Conlang courses are a fruitful way to engage and attract students into linguistics, but they can also be used to bring linguistics into the world at large. The focus of this chapter is a Special Topics in Linguistics class at Arizona State University. This chapter describes the course structure, discusses the pros and cons of this particular structure, and provides examples of the languages created in the class. This course also led to unexpected opportunities to bring linguistics into the larger community, in the form of panels at two different fan conventions. During each panel, the students described their own languages, and the choices they made in creating them. This chapter provides example slides from these panels and discusses the process of creating the panels, the outcomes, and the resulting interactions with the public.},
  bookauthor = {Gillon, Carrie and Delmonico, Edward and Martinez, Randi and Morrell, Spencer},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@book{goldbergConstructionsWorkNature2006,
  title = {Constructions at Work: The Nature of Generalization in Language},
  shorttitle = {Constructions at Work},
  author = {Goldberg, Adele E.},
  date = {2006},
  series = {Oxford Linguistics},
  publisher = {{Oxford University press}},
  location = {{Oxford}},
  isbn = {978-0-19-926852-8},
  langid = {english}
}

@book{goldbergExplainMeThis2019a,
  title = {Explain Me This: Creativity, Competition, and the Partial Productivity of Constructions},
  shorttitle = {Explain Me This},
  author = {Goldberg, Adele E.},
  date = {2019},
  publisher = {{Princeton University Press}},
  location = {{Princeton, New Jersey}},
  abstract = {Our use of language is highly creative yet also constrained, we use words and phrases creatively to express ourselves in ever-changing contexts, readily extending language constructions in new ways. Yet native speakers also implicitly know when a creative and easily interpretable formulation-such as "Explain me this" or "She considered to go"-doesn't sound quite right. In this incisive book, Adele Goldberg explores how these creative but constrained language skills emerge from a combination of general cognitive mechanisms and experience.Shedding critical light on an enduring linguistic paradox, Goldberg demonstrates how words and abstract constructions are generalized and constrained in the same ways. When learning language, we record partially abstracted tokens of language within the high-dimensional conceptual space that is used when we speak or listen. Our implicit knowledge of language includes dimensions related to form, function, and social context. At the same time, abstract memory traces of linguistic usage-events cluster together on a subset of dimensions, with overlapping aspects strengthened via repetition. In this way, dynamic categories that correspond to words and abstract constructions emerge from partially overlapping memory traces, and as a result, distinct words and constructions compete with one another each time we select them to express our intended messages.While much of the research on this puzzle has favored semantic or functional explanations over statistical ones, Goldberg's approach stresses that both the functional and statistical aspects of constructions emerge from the same learning mechanisms},
  isbn = {978-0-691-17425-9 978-0-691-17426-6},
  pagetotal = {195},
  keywords = {Discourse analysis,Psycholinguistics},
  annotation = {OCLC: on1032723071}
}

@article{gonzalez-rodriguezSelfOrganizedLinguisticSystems2018,
  title = {Self-{{Organized Linguistic Systems}}: {{From}} Traditional {{AI}} to Bottom-up Generative Processes},
  shorttitle = {Self-{{Organized Linguistic Systems}}},
  author = {Gonzalez-Rodriguez, Diego and Hernandez-Carrion, Jose Rodolfo},
  date = {2018-10},
  journaltitle = {Futures},
  shortjournal = {Futures},
  volume = {103},
  pages = {27--34},
  issn = {00163287},
  doi = {10.1016/j.futures.2018.03.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0016328717302161},
  urldate = {2023-10-03},
  langid = {english},
  file = {C\:\\Users\\ron\\Zotero\\storage\\FRB9PS3K\\Gonzalez-Rodriguez and Hernandez-Carrion - 2018 - Self-Organized Linguistic Systems From traditiona.pdf;C\:\\Users\\ron\\Zotero\\storage\\IKK9YPJH\\1-s2.0-S0016328717302161-main.pdf}
}

@article{gonzalezComputationalLinguisticApproach2022,
  title = {A {{Computational Linguistic Approach}} to {{Conlang Research Literature}}},
  author = {Gonzalez, Simon},
  date = {2022-12-31},
  journaltitle = {RiCOGNIZIONI. Rivista di Lingue e Letterature straniere e Culture moderne},
  pages = {V. 9 N. 18 (2022)},
  publisher = {{RiCOGNIZIONI. Rivista di Lingue e Letterature straniere e Culture moderne}},
  doi = {10.13135/2384-8987/7090},
  url = {https://www.ojs.unito.it/index.php/ricognizioni/article/view/7090},
  urldate = {2023-08-21},
  abstract = {The field of Conlang has evidenced an important growth in the last decades. This has been the product of a wide interest on the use and study of conlangs for artistic purposes. However, one important question is what it is happening with conlang in the academic world. This paper aims to have an overall understanding of the literature on conlang research. With this we aim to give a realistic picture of the field in the current age. We have implemented a computational linguistic approach, combining bibliometrics and network analysis to examine all publications available in the Scopus database. Analysing over 2300 academic publications since 1927 until 2022, we have found that Esperanto is by far the most documented conlang. Three main authors have contributed to this: Garvía R., Fiedler S., and Blanke D. The 1970s and 1980s have been the decades where the foundations of current research have been built. In terms of methodologies, language learning and experimental linguistics are the ones contributing to most as the preferred approaches of study in the field. We present the results and discuss our limitations and future work.},
  langid = {english},
  file = {C\:\\Users\\ron\\Downloads\\7090-Article Text-23244-2-10-20230605.pdf;C\:\\Users\\ron\\Zotero\\storage\\K74P2UR4\\7090-Article Text-23244-2-10-20230605.pdf}
}

@inproceedings{gonzalezCybersecurityPracticesInitial2021,
  title = {Cybersecurity {{Practices At The Initial Stages Of The Software Engineering Process}}},
  author = {Gonzalez, Hugo and Llamas-Contreras, Rafael and Guerra-García, César},
  date = {2021-10-25},
  pages = {219--226},
  publisher = {{IEEE}},
  location = {{San Diego, CA, USA}},
  doi = {10.1109/CONISOFT52520.2021.00037},
  eventtitle = {2021 9th {{International Conference}} in {{Software Engineering Research}} and {{Innovation}} ({{CONISOFT}})},
  file = {C:\Users\ron\Zotero\storage\A2STSH6C\Cybersecurity_Practices_At_The_Initial_Stages_Of_The_Software_Engineering_Process.pdf}
}

@inbook{goodallDesignIngLanguage2020,
  title = {The Design(Ing) of Language},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Goodall, Grant},
  date = {2020-08-12},
  pages = {69--85},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0006},
  url = {https://academic.oup.com/book/31973/chapter/267717231},
  urldate = {2023-08-24},
  abstract = {Courses on invented languages can do much more than just introduce students to linguistics. Through three case studies, it is shown that as students learn how to design a language, they also learn about the design of human language in a way that is unlikely to occur in other courses. The first case study involves the creation of a lexicon, in relation to John Wilkins’ invented language of 1668 and to Saussurean arbitrariness, commonly regarded as a fundamental design property of human language. The second case study concerns phonemic inventories. By designing their own from scratch, students see the competing pressures that phonemic inventories must satisfy in all languages. The third case study concerns inflectional morphology and the pressures that determine the form of particular morphemes. All of these case studies are accessible to students and help them engage with important aspects of the design properties of human language.},
  bookauthor = {Goodall, Grant},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@book{gormanFinitestateTextProcessing2022,
  title = {Finite-State Text Processing},
  author = {Gorman, Kyle and Sproat, Richard},
  date = {2022},
  publisher = {{Springer Nature Switzerland AG}},
  location = {{Cham, Switzerland}},
  isbn = {978-3-031-02179-4},
  langid = {english},
  annotation = {OCLC: 1331557791},
  file = {C:\Users\ron\Zotero\storage\2C33VEJ5\Gorman and Sproat - 2022 - Finite-state text processing.pdf}
}

@inproceedings{gormanPyniniPythonLibrary2016,
  title = {Pynini: {{A Python}} Library for Weighted Finite-State Grammar Compilation},
  shorttitle = {Pynini},
  booktitle = {Proceedings of the {{SIGFSM Workshop}} on {{Statistical NLP}} and {{Weighted Automata}}},
  author = {Gorman, Kyle},
  date = {2016},
  pages = {75--80},
  publisher = {{Association for Computational Linguistics}},
  location = {{Berlin, Germany}},
  doi = {10.18653/v1/W16-2409},
  url = {http://aclweb.org/anthology/W16-2409},
  urldate = {2023-10-09},
  eventtitle = {Proceedings of the {{SIGFSM Workshop}} on {{Statistical NLP}} and {{Weighted Automata}}},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\G8LTZ8M4\Gorman - 2016 - Pynini A Python library for weighted finite-state.pdf}
}

@inproceedings{guoDesignOptimizedLowlatency2013,
  title = {Design of an Optimized Low-Latency Interrupt Controller for {{IMS-DPU}}},
  booktitle = {2013 {{IEEE}} 10th {{International Conference}} on {{ASIC}}},
  author = {Guo, Zijia and Wang, Teng and Wang, Xin-An and Hu, Ziyi},
  date = {2013-10},
  pages = {1--4},
  issn = {2162-755X},
  doi = {10.1109/ASICON.2013.6811840},
  abstract = {Interrupt handling mechanism is an important function for multi-core system to work collaboratively. In this paper, an optimized low-latency interrupt controller is proposed to support a multi-core system IMS-DPU for high performance medical electronics equipment. Utilizing two interrupt models, the interrupt controller implements three different kinds of interrupts, shared peripheral interrupt (SPI), private peripheral interrupt (PPI) and software generated interrupt (SGI). The main feature of the controller is to distribute multiple interrupts across the cores of a multi-core system. In addition, our architecture supports several advanced features like interrupt pending and active state, interrupt preemption and nesting, interrupt grouping and security extension. The design in this study puts forward special optimization for performance enhancement in hardware structures, with the aim to support the combination of software stack and hardware stack, tail chaining and later arrivals. FPGA prototyping results justify the design. It is finally implemented with CSMC 180nm technology with 6.01K logic gates at working frequency of 200MHz.},
  eventtitle = {2013 {{IEEE}} 10th {{International Conference}} on {{ASIC}}},
  keywords = {Hardware,hardware stack,interrupt handling,later arrivals,Logic gates,low-latency interrupt controller,Multicore processing,Registers,Security,Software,software stack,tail chaining},
  file = {C:\Users\ron\Zotero\storage\V6ZNGTZ2\Design_of_an_optimized_low-latency_interrupt_controller_for_IMS-DPU.pdf}
}

@inproceedings{guptaAnsweringRequirementsTraceability2018,
  title = {Answering the Requirements Traceability Questions},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}: {{Companion Proceeedings}}},
  author = {Gupta, Arushi and Wang, Wentao and Niu, Nan and Savolainen, Juha},
  date = {2018-05-27},
  series = {{{ICSE}} '18},
  pages = {444--445},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3183440.3195049},
  url = {https://doi.org/10.1145/3183440.3195049},
  urldate = {2022-10-22},
  abstract = {To understand requirements traceability in practice, we present a preliminary study of identifying questions from requirements repositories and examining their answering status. Investigating four open-source projects results in 733 requirements questions, among which 43\% were answered successfully, 35\% were answered unsuccessfully, and 22\% were not answered at all. We evaluate the accuracy of using a state-of-the-art natural language processing tool to identify the requirements questions and illuminate automated ways to classify their answering status.},
  isbn = {978-1-4503-5663-3},
  keywords = {answering status,requirements questions,traceability},
  file = {C:\Users\ron\Zotero\storage\B6D44WYN\Gupta et al. - 2018 - Answering the requirements traceability questions.pdf}
}

@online{hammarströmGlottolog,
  title = {Glottolog 4.8},
  author = {Hammarström, Harald and Forkel, Robert and Haspelmath, Martin and Nordhoff, Sebastain and Bank, Sebastain},
  url = {https://glottolog.org/},
  abstract = {Comprehensive reference information for the world's languages, especially the lesser known languages.},
  organization = {{Glottolog 4.8}}
}

@inproceedings{hammoudiEffectIncompletenessCheck2021,
  title = {On the Effect of Incompleteness to Check Requirement-to-Method Traces},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Hammoudi, Mouna and Mayr-Dorn, Christoph and Mashkoor, Atif and Egyed, Alexander},
  date = {2021-03-22},
  series = {{{SAC}} '21},
  pages = {1465--1474},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3412841.3442021},
  url = {https://doi.org/10.1145/3412841.3442021},
  urldate = {2022-10-22},
  abstract = {Requirement-to-method traces reveal the code location(s) where a requirement is implemented. This is helpful to software engineers when they have to perform tasks such as software maintenance or bug fixing. Indeed, being aware of the method(s) that implement a requirement saves engineers' time, as it pinpoints the exact code region that needs to be edited to perform a bug fix or a maintenance task. Engineers produce traces manually as well as automatically. Nevertheless, traces are incomplete. This limits the amount of information that could be used by an automated technique to check further traces. Therefore, since traces are incomplete, we would like to study the effect of incompleteness on the automated assessment of requirement-to-method traces. In this paper, we apply machine learning on either incomplete or complete tracing information and we evaluate the effect of incompleteness on checking trace information. We demonstrate that the use of complete traces might yield a higher precision but yields a lower recall. Also, the use of incomplete traces yields a higher recall but a lower precision.},
  isbn = {978-1-4503-8104-8},
  keywords = {machine learning,requirement-to-method traces,traceability},
  file = {C:\Users\ron\Zotero\storage\X4Q9P7GL\Hammoudi et al. - 2021 - On the effect of incompleteness to check requireme.pdf}
}

@inproceedings{hauserUsingThreadsInteractive1994,
  title = {Using {{Threads}} in {{Interactive Systems}}: {{A Case Study}}},
  booktitle = {Proceedings of the Fourteenth {{ACM}} Symposium on {{Operating}} Systems Principles},
  author = {Hauser, Carl and Jacobi, Christian and Theimer, Marvin and Welch, Brent and Weiser, Mark},
  date = {1994-01},
  pages = {94--105},
  doi = {168619.168627},
  url = {https://doi.org/10.1145/168619.168627},
  eventtitle = {{{SOSP}} '93},
  file = {C:\Users\ron\Zotero\storage\GUU5I2T6\173668.168627.pdf}
}

@inproceedings{hendersonMakeItWork2014,
  title = {Make It Work, Make It Right, Make It Fast: Building a Platform-Neutral Whole-System Dynamic Binary Analysis Platform},
  shorttitle = {Make It Work, Make It Right, Make It Fast},
  booktitle = {Proceedings of the 2014 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Henderson, Andrew and Prakash, Aravind and Yan, Lok Kwong and Hu, Xunchao and Wang, Xujiewen and Zhou, Rundong and Yin, Heng},
  date = {2014-07-21},
  series = {{{ISSTA}} 2014},
  pages = {248--258},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2610384.2610407},
  url = {https://doi.org/10.1145/2610384.2610407},
  urldate = {2022-06-20},
  abstract = {Dynamic binary analysis is a prevalent and indispensable technique in program analysis. While several dynamic binary analysis tools and frameworks have been proposed, all suffer from one or more of: prohibitive performance degradation, semantic gap between the analysis code and the program being analyzed, architecture/OS specificity, being user-mode only, lacking APIs, etc. We present DECAF, a virtual machine based, multi-target, whole-system dynamic binary analysis framework built on top of QEMU. DECAF provides Just-In-Time Virtual Machine Introspection combined with a novel TCG instruction-level tainting at bit granularity, backed by a plugin based, simple-to-use event driven programming interface. DECAF exercises fine control over the TCG instructions to accomplish on-the-fly optimizations. We present 3 platform-neutral plugins - Instruction Tracer, Keylogger Detector, and API Tracer, to demonstrate the ease of use and effectiveness of DECAF in writing cross-platform and system-wide analysis tools. Implementation of DECAF consists of 9550 lines of C++ code and 10270 lines of C code and we evaluate DECAF using CPU2006 SPEC benchmarks and show average overhead of 605\% for system wide tainting and 12\% for VMI.},
  isbn = {978-1-4503-2645-2},
  keywords = {Dynamic binary analysis,dynamic taint analysis,virtual machine introspection},
  file = {C:\Users\ron\Zotero\storage\YQKJSD5B\Henderson et al. - 2014 - Make it work, make it right, make it fast buildin.pdf}
}

@thesis{heyerGeneratingImmersiveConlangs2021,
  type = {bathesis},
  title = {Generating {{Immersive Conlangs}}},
  author = {Heyer, Fabian},
  date = {2021-10-14},
  institution = {{Christian-Albrecht University of Kiel}},
  url = {https://www.researchgate.net/profile/Fabian_Heyer/publication/358746891_Generating_Immersive_Conlangs/links/62138b41f02286737cb258d3/Generating-Immersive-Conlangs.pdf},
  abstract = {Conlangs, constructed languages like Elvish or Klingon, are a key contribution to the quality of fictional works like movies and video games. Good Conlangs promote the immersion potential in fictional settings because they provide a relatable context. To make Conlangs plausible, however, is challenging and requires consideration of several aspects of natural languages. This thesis presents a Conlang generator architecture that derives new Conlangs from natural languages. It examines existing Conlang generators and discusses current methods in Natural Language Processing that are useful for Conlang generation. Several existing concepts, namely n-gram feature analysis, text generation with Recurrent Neural Networks, and attention-based translator models are combined with a novel Selective Replacement process for parallel corpora to yield a customizable Conlang build pipeline. By using Machine Learning tools, a reference implementation provides the foundation for experiments on the quality of the generated Conlangs. Several key characteristics in the Conlang generator’s behavior are examined, including the influence of modeling parameters on the properties of resulting languages and the role of word replacement strategies concerning semantic structure and context preservation.  To conclusively rate the generator’s performance, a user study on plausibility and immersiveness is proposed, along with possible future generator features.},
  langid = {english},
  pagetotal = {26},
  file = {C:\Users\ron\Zotero\storage\JMQMEZDE\Heyer - 2021 - Generating Immersive Conlangs.pdf}
}

@inproceedings{hochmullerAgileProcessMyths2008,
  title = {Agile {{Process Myths}}},
  booktitle = {Proceedings of the 2008 International Workshop on {{Scrutinizing}} Agile Practices or Shoot-out at the Agile Corral},
  author = {Hochmüller, Elke and Mittermeir, Roland T.},
  date = {2008-05-10},
  series = {{{APOS}} '08},
  pages = {5--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1370143.1370145},
  url = {https://doi.org/10.1145/1370143.1370145},
  urldate = {2022-11-09},
  abstract = {Based on the introduction of some more or less common myths concerning expected benefits of agile practices, this paper features some challenging topics in agile software development. These topics still need thorough discussion when characterizing agile practices and are particularly relevant for deciding on the applicability of agile practices for a particular project.},
  isbn = {978-1-60558-021-0},
  keywords = {agile practices,challenges,development processes,myths},
  file = {C:\Users\ron\Zotero\storage\NIP7IEFH\Hochmüller and Mittermeir - 2008 - Agile Process Myths.pdf}
}

@inproceedings{holtmannIntegratedSystemsEngineering2015,
  title = {Integrated Systems Engineering and Software Requirements Engineering for Technical Systems},
  booktitle = {Proceedings of the 2015 {{International Conference}} on {{Software}} and {{System Process}}},
  author = {Holtmann, Jörg and Bernijazov, Ruslan and Meyer, Matthias and Schmelter, David and Tschirner, Christian},
  date = {2015-08-24},
  series = {{{ICSSP}} 2015},
  pages = {57--66},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2785592.2785597},
  url = {https://doi.org/10.1145/2785592.2785597},
  urldate = {2022-10-22},
  abstract = {The development of software-intensive technical systems (e.g., within the automotive industry) involves several engineering disciplines like mechanical, electrical, control, and software engineering. Model-based Systems Engineering (MBSE) coordinates these disciplines throughout the development by means of discipline-spanning processes and system models. Such a system model provides a common understanding of the system under development and serves as a starting point for the discipline-specific development. An integral part of MBSE is the requirements engineering on the system level. However, for the discipline-specific development to start, these requirements need to be refined, e.g., into specific requirements for the embedded software. Since existing MBSE approaches lack support for this refinement step, we conceived a systematic transition from MBSE to model-based software requirements engineering, which we present in this paper. We automated the steps of the transition where possible, in order to avoid error-prone and time-consuming manual tasks. We illustrate the approach with an example of an automotive embedded system.},
  isbn = {978-1-4503-3346-7},
  keywords = {Requirements Engineering,Systems Engineering},
  file = {C:\Users\ron\Zotero\storage\W6UZVCMI\Holtmann et al. - 2015 - Integrated systems engineering and software requir.pdf}
}

@inproceedings{huangProbabilisticDenotationalSemantics2015,
  title = {Probabilistic {{Denotational Semantics}} for an {{Interrupt Modelling Language}}},
  booktitle = {2015 20th {{International Conference}} on {{Engineering}} of {{Complex Computer Systems}} ({{ICECCS}})},
  author = {Huang, Yanhong and Zhao, Yongxin and Qin, Shengchao and He, Jifeng},
  date = {2015-12},
  pages = {160--169},
  publisher = {{IEEE}},
  location = {{Gold Coast, Australia}},
  doi = {10.1109/ICECCS.2015.35},
  url = {http://ieeexplore.ieee.org/document/7384240/},
  urldate = {2022-06-12},
  abstract = {Interrupts play an important role in real time and embedded systems. It is purposely designed to handle unexpected and emergent issues. However, the randomicity of interrupts brings some potential safety problems, i.e., too frequently interrupt handling would cause the interrupted program to miss its deadline. It is therefore difficult to precisely predict and formally reason about a program’s behavior in the presence of interrupts. In this paper, we move one step forward by proposing a probabilistic denotational model for an interrupt modeling language that is capable of describing programs with nested interrupts, to characterise the formal semantics of such programs from a quantitative perspective under Hoare and He’s UTP framework. On top of the denotational model, we also present a set of algebraic laws involving distinct features. Our model sets up a semantic foundation for the analysis and reasoning about programs with nested interrupts for embedded systems.},
  eventtitle = {2015 20th {{International Conference}} on {{Engineering}} of {{Complex Computer Systems}} ({{ICECCS}})},
  isbn = {978-1-4673-8581-7},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\2HKPW8MS\Huang et al. - 2015 - Probabilistic Denotational Semantics for an Interr.pdf}
}

@report{huAssessmentAccessControl2006,
  title = {Assessment of {{Access Control Systems}}},
  author = {Hu, Vincent and Ferraiolo, David and Kuhn, Richard},
  date = {2006-09-29},
  number = {NIST Internal or Interagency Report (NISTIR) 7316},
  institution = {{National Institute of Standards and Technology}},
  doi = {10.6028/NIST.IR.7316},
  url = {https://csrc.nist.gov/publications/detail/nistir/7316/final},
  urldate = {2022-06-20},
  abstract = {Access control is perhaps the most basic aspect of computer security. Nearly all applications that deal with financial, privacy, safety, or defense include some form of access control. In many systems access control takes the form of a simple password mechanism, but many require more sophisticated and complex control. In addition to the authentication mechanism (such as a password), access control is concerned with how authorizations are structured. In some cases, authorization may mirror the structure of the organization, while in others it may be based on the sensitivity level of various documents and the security level of the user accessing those documents. This publication explains some of the most commonly used access control services available in information technology systems, their structure, where they are likely to be used, and advantages and disadvantages of each.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\8HFKD84E\Assessment_ACS.pdf}
}

@article{hubbardOpenSourceCore2004,
  title = {Open {{Source}} to the {{Core}}: {{Using}} Open Source in Real-World Software Products: {{The}} Good, the Bad and the Ugly},
  author = {Hubbard, Jordan},
  date = {2004-05},
  journaltitle = {Queue},
  volume = {2},
  number = {3},
  pages = {24--31},
  url = {https://doi.org/10.1145/1005062.1005064},
  abstract = {The open source development model is not exactly new. Individual engineers have been using open source as a collaborative development methodology for decades. Now that it has come to the attention of upper and middle management, however, it’s finally being openly acknowledged as a commercial engineering force-multiplier and important option for avoiding significant software development costs.},
  file = {C:\Users\ron\Zotero\storage\Q37ARRH9\Open Source to the Core.pdf}
}

@inproceedings{hviidServiceAbstractionLayer2018,
  title = {Service {{Abstraction Layer}} for {{Building Operating Systems}}: {{Enabling}} Portable Applications and Improving System Resilience},
  shorttitle = {Service {{Abstraction Layer}} for {{Building Operating Systems}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Communications}}, {{Control}}, and {{Computing Technologies}} for {{Smart Grids}} ({{SmartGridComm}})},
  author = {Hviid, Jakob and Kjærgaard, Mikkel Baun},
  date = {2018-10},
  pages = {1--6},
  doi = {10.1109/SmartGridComm.2018.8587543},
  abstract = {For large scale implementation of Demand Response programs, applications enabling Demand Response would need to be portable between buildings. Building Operating Systems are an essential strategic piece for enabling portable applications, which work with several hardware implementations from different vendors. These building operating systems can effectively be used to implement large-scale Demand Response implementations. Currently, though, the service layer of building operating systems requires direct integration with specific services, and thereby have requirements for specific implementations of services to be present in a specific building. This paper proposes the introduction of a Service Abstraction Layer to decouple the application from the specific implementations of services, as well as to introduce the concept of redundancy in service responsibility areas. These changes would allow for application portability between buildings but also allow for building operating system resiliency.A prototype abstraction layer is implemented and tested. Results show the introduction of a Service Abstraction Layer has promising benefits for building operating systems, and successfully decouples the applications from the building operating system implementation, as well as improving system reliability and resiliency. For smart grid applications, the addition of a service abstraction layer allows for large-scale portable applications, but would still require some form of standardization of communication protocols for different service types.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Communications}}, {{Control}}, and {{Computing Technologies}} for {{Smart Grids}} ({{SmartGridComm}})},
  keywords = {Buildings,Hardware,Load management,Ontologies,Operating systems,Resilience,Smart grids},
  file = {C:\Users\ron\Zotero\storage\NCEJVIDK\Service_Abstraction_Layer_for_Building_Operating_Systems_Enabling_portable_applications_and_improving_system_resilience.pdf}
}

@article{IEEERecommendedPractice1998,
  title = {{{IEEE Recommended Practice}} for {{Software Requirements Specifications}}},
  date = {1998-10},
  journaltitle = {IEEE Std 830-1998},
  pages = {1--40},
  doi = {10.1109/IEEESTD.1998.88286},
  abstract = {Replaced by ISO/IEC/IEEE 29148:2011. The content and qualities of a good software requirements specification (SRS) are described and several sample SRS outlines are presented. This recommended practice is aimed at specifying requirements of software to be developed but also can be applied to assist in the selection of in-house and commercial software products. Guidelines for compliance with IEEE/ EIA12207.1- 1997 are also provided.},
  keywords = {contract,customer,prototyping,Software requirements and specifications,software requirements specification,supplier,system requirements specifications},
  file = {C:\Users\ron\Zotero\storage\EDZ86WGP\830-1998.pdf}
}

@article{IEEEStandardHigher2003,
  title = {{{IEEE Standard For Higher Performance Protocol}} for the {{Standard Digital Interface}} for {{Programmable Instrumentation}}},
  date = {2003-12},
  journaltitle = {IEEE Std 488.1-2003 (Revision of IEEE Std 488.1-1987)},
  pages = {1--140},
  doi = {10.1109/IEEESTD.2003.94413},
  abstract = {This standard applies to interface systems used to interconnect both programmable and nonprogrammable electronic measuring apparatus with other apparatus and accessories necessary to assemble instrumentation systems. It applies to the interface of instrumentation systems, or portions of them, in which the a) Data exchanged among the interconnected apparatus is digital (as distinct from analog); b) Number of devices that may be interconnected by one contiguous bus does not exceed 15; c) Total transmission path lengths over the interconnecting cables does not exceed 20 m; d) Data rate among devices does not exceed 8 000 000 B/s. The basic functional specifications of this standard may be used in digital interface applications that require longer distances, more devices, increased noise immunity, or combinations of these. Different electrical and mechanical specifications may be required (for example, symmetrical circuit configurations, high threshold logic, special connectors, or cable configurations) for these extended applications.},
  eventtitle = {{{IEEE Std}} 488.1-2003 ({{Revision}} of {{IEEE Std}} 488.1-1987)},
  keywords = {GPIB,HPIB,HS488,IEEE Standards,Instruments,non-interlocked handshake,Patents,Protocols,SCPI,three-wire handshake,Trademarks},
  file = {C\:\\Users\\ron\\Zotero\\storage\\RVTJI54B\\2003 - IEEE Standard For Higher Performance Protocol for .pdf;C\:\\Users\\ron\\Zotero\\storage\\UNYFEXY5\\1258186.html}
}

@report{ieeestandardsboardIEEEStandardCodes1992,
  title = {IEEE Standard Codes, Formats, Protocols, and Common Commands for Use With IEEE Std 488.1-1987, IEEE Standard Digital Interface for Programmable Instrumentation},
  author = {{ΙΕΕΕ Standards Board}},
  date = {1992-06-18},
  institution = {{IEEE}},
  doi = {10.1109/IEEESTD.1992.114468},
  url = {http://ieeexplore.ieee.org/document/213762/},
  urldate = {2022-06-10},
  isbn = {9780738106656},
  langid = {greek},
  file = {C:\Users\ron\Zotero\storage\5AESN4C6\IEEE Standard Codes, Formats, Protocols, and Commo.pdf}
}

@misc{InternationalPhoneticAlphabet2020,
  title = {The {{International Phonetic Alphabet}}},
  date = {2020},
  url = {https://www.internationalphoneticassociation.org/IPAcharts/IPA_chart_orig/IPA_charts_E.html},
  organization = {{International Phonetic Association}},
  file = {C:\Users\ron\Zotero\storage\QIHXBUKI\IPA_Kiel_2020_tooltips.pdf}
}

@book{internationalphoneticassociationHandbookInternationalPhonetic1999,
  title = {Handbook of the {{International Phonetic Association}}: A Guide to the Use of the {{International Phonetic Alphabet}}},
  shorttitle = {Handbook of the {{International Phonetic Association}}},
  editor = {International Phonetic Association},
  date = {1999},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, U.K. ; New York, NY}},
  isbn = {978-0-521-65236-0 978-0-521-63751-0},
  pagetotal = {204},
  keywords = {{Handbooks, manuals, etc},Phonetic alphabet}
}

@online{internationalstandardsorganizationISOIEC9293,
  title = {{{ISO}}/{{IEC}} 9293:1994},
  shorttitle = {{{ISO}}/{{IEC}} 9293},
  author = {{International Standards Organization}},
  url = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/02/12/21273.html},
  urldate = {2022-06-23},
  abstract = {Information technology — Volume and file structure of disk cartridges for information interchange},
  langid = {english},
  organization = {{ISO}}
}

@book{internationalstandardsorganizationQualityManagementPrinciples2015,
  title = {Quality {{Management Principles}}},
  author = {{International Standards Organization}},
  date = {2015},
  series = {{{ISO}} 9000 {{Family}}},
  publisher = {{ISO}},
  isbn = {978-92-67-10650-2},
  file = {C:\Users\ron\Zotero\storage\A67ZXZN8\PUB100080.pdf}
}

@inproceedings{irigoyenferreiroferreiraApplyingISO90012007,
  title = {Applying {{ISO}} 9001:2000, {{MPS}}.{{BR}} and {{CMMI}} to {{Achieve Software Process Maturity}}: {{BL Informatica}}'s {{Pathway}}},
  shorttitle = {Applying {{ISO}} 9001},
  booktitle = {29th {{International Conference}} on {{Software Engineering}} ({{ICSE}}'07)},
  author = {Irigoyen Ferreiro Ferreira, Analia and Santos, Gleison and Cerqueira, Roberta and Montoni, Mariano and Barreto, Ahilton and Soares Barreto, Andrea O. and Rocha, Ana Regina},
  date = {2007-05},
  pages = {642--651},
  publisher = {{IEEE}},
  location = {{Minneapolis, MN}},
  doi = {10.1109/ICSE.2007.15},
  url = {http://ieeexplore.ieee.org/document/4222625/},
  urldate = {2022-12-05},
  eventtitle = {29th {{International Conference}} on {{Software Engineering}} ({{ICSE}}'07)},
  isbn = {978-0-7695-2828-1},
  file = {C:\Users\ron\Zotero\storage\QCQCE3Z9\ICSE.2007.15.pdf}
}

@online{JavaPathfinder,
  title = {Java {{Pathfinder}}},
  url = {https://github.com/javapathfinder},
  urldate = {2022-06-18},
  abstract = {Java Pathfinder has 8 repositories available. Follow their code on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {C:\Users\ron\Zotero\storage\66TR4IDS\javapathfinder.html}
}

@inproceedings{jiMicroTEEDesigningTEE2019,
  title = {{{MicroTEE}}: {{Designing TEE OS Based}} on the {{Microkernel Architecture}}},
  shorttitle = {{{MicroTEE}}},
  booktitle = {2019 18th {{IEEE International Conference On Trust}}, {{Security And Privacy In Computing And Communications}}/13th {{IEEE International Conference On Big Data Science And Engineering}} ({{TrustCom}}/{{BigDataSE}})},
  author = {Ji, Dongxu and Zhang, Qianying and Zhao, Shijun and Shi, Zhiping and Guan, Yong},
  date = {2019-08},
  pages = {26--33},
  publisher = {{IEEE}},
  location = {{Rotorua, New Zealand}},
  doi = {10.1109/TrustCom/BigDataSE.2019.00014},
  url = {https://ieeexplore.ieee.org/document/8887405/},
  urldate = {2022-05-20},
  abstract = {ARM TrustZone technology is widely used to provide Trusted Execution Environments (TEE) for mobile devices. However, most TEE OSes are implemented as monolithic kernels. In such designs, device drivers, kernel services and kernel modules all run in the kernel, which results in large size of the kernel. It is difficult to guarantee that all components of the kernel have no security vulnerabilities in the monolithic kernel architecture, such as the integer overflow vulnerability in Qualcomm QSEE TrustZone and the TZDriver vulnerability in HUAWEI Hisilicon TEE architecture. This paper presents MicroTEE, a TEE OS based on the microkernel architecture. In MicroTEE, the microkernel provides strong isolation for TEE OS’s basic services, such as crypto service and platform key management service. The kernel is only responsible for providing core services such as address space management, thread management, and inter-process communication. Other fundamental services, such as crypto service and platform key management service are implemented as applications at the user layer. Crypto Services and Key Management are used to provide Trusted Applications (TAs) with sensitive information encryption, data signing, and platform attestation functions. Our design avoids the compromise of the whole TEE OS if only one kernel service is vulnerable. A monitor has also been added to perform the switch between the secure world and the normal world. Finally, we implemented a MicroTEE prototype on the Freescale i.MX6Q Sabre Lite development board and tested its performance. Evaluation results show that the performance of cryptographic operations in MicroTEE is better than it in Linux when the size of data is small.},
  eventtitle = {2019 18th {{IEEE International Conference On Trust}}, {{Security And Privacy In Computing And Communications}}/13th {{IEEE International Conference On Big Data Science And Engineering}} ({{TrustCom}}/{{BigDataSE}})},
  isbn = {978-1-72812-777-4},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\SKXJEMVM\Ji et al. - 2019 - MicroTEE Designing TEE OS Based on the Microkerne.pdf}
}

@inproceedings{jinConstructionApplicationKnowledge2021,
  title = {Construction and Application of Knowledge Graph of Domestic Operating System Testing},
  booktitle = {2021 4th {{International Conference}} on {{Computer Science}} and {{Software Engineering}} ({{CSSE}} 2021)},
  author = {Jin, Dongsheng and Wang, Zhi and Li, Mingyang and Zhu, Xinjie},
  date = {2021-10-22},
  series = {{{CSSE}} 2021},
  pages = {268--273},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3494885.3494933},
  url = {https://doi.org/10.1145/3494885.3494933},
  urldate = {2022-06-07},
  abstract = {Aiming at the problems of poor reusability of domestic operating system test cases and insufficient sharing of test case design experience at this stage, a method for constructing knowledge graphs in the field of domestic operating system testing is proposed, and ontology construction and natural language processing technologies are applied to the field of software testing. Use the strong correlation of the knowledge graph to mine the experience knowledge in the design of historical test cases, select and reuse test cases that meet the test requirements for testers, and help them design test cases more efficiently. Through empirical research, this method gives full play to the advantages of knowledge graphs in relational network analysis and retrieval, and the coverage rate of reused test cases reaches 71\%, which can greatly save test costs and improve test efficiency, and has strong engineering application value.},
  isbn = {978-1-4503-9067-5},
  keywords = {Domestic operating system,Knowledge graph,Ontology construction,Reuse of test cases,Software testing},
  file = {C:\Users\ron\Zotero\storage\N9QBG2KT\3494885.3494933.pdf}
}

@inproceedings{jiResearchKeyTechnologies2021,
  title = {Research on {{Key Technologies}} of {{Software Configuration Management}} in {{Development}} of {{Large-scale Software System}}},
  booktitle = {2021 3rd {{Asia Pacific Information Technology Conference}}},
  author = {Ji, Wenjun},
  date = {2021-01-15},
  pages = {54--58},
  publisher = {{ACM}},
  location = {{Bangkok Thailand}},
  doi = {10.1145/3449365.3449374},
  url = {https://dl.acm.org/doi/10.1145/3449365.3449374},
  urldate = {2023-04-25},
  eventtitle = {{{APIT}} 2021: 2021 3rd {{Asia Pacific Information Technology Conference}}},
  isbn = {978-1-4503-8810-8},
  langid = {english}
}

@inproceedings{jonsonGrammarbasedContextspecificStatistical2007,
  title = {Grammar-Based Context-Specific Statistical Language Modelling},
  booktitle = {Proceedings of the {{Workshop}} on {{Grammar-Based Approaches}} to {{Spoken Language Processing}}},
  author = {Jonson, Rebecca},
  date = {2007-06-29},
  series = {{{SLP}} '07},
  pages = {25--32},
  publisher = {{Association for Computational Linguistics}},
  location = {{USA}},
  abstract = {This paper shows how we can combine the art of grammar writing with the power of statistics by bootstrapping statistical language models (SLMs) for Dialogue Systems from grammars written using the Grammatical Framework (GF) (Ranta, 2004). Furthermore, to take into account that the probability of a user's dialogue moves is not static during a dialogue we show how the same methodology can be used to generate dialogue move specific SLMs where certain dialogue moves are more probable than others. These models can be used at different points of a dialogue depending on contextual constraints. By using grammar generated SLMs we can improve both recognition and understanding performance considerably over using the original grammar. With dialogue move specific SLMs we would be able to get a further improvement if we had an optimal way of predicting the correct language model.},
  file = {C:\Users\ron\Zotero\storage\XQ5AQPH6\1626333.1626339.pdf}
}

@standard{JSONDataInterchange2017,
  title = {The {{JSON Data Interchange Syntax}}},
  shorttitle = {{{ECMA-404}}},
  date = {2017-12},
  publisher = {{ECMA Interenational}},
  url = {https://ecma-international.org/publications-and-standards/standards/ecma-404/},
  abstract = {JSON is a lightweight, text-based, language-independent syntax for defining data interchange formats. It was derived from the ECMAScript programming language, but is programming language independent. JSON defines a small set of structuring rules for the portable representation of structured data. The goal of this specification is only to define the syntax of valid JSON texts. Its intent is not to provide any semantics or interpretation of text conforming to that syntax. It also intentionally does not define how a valid JSON text might be internalized into the data structures of a programming language. There are many possible semantics that could be applied to the JSON syntax and many ways that a JSON text can be processed or mapped by a programming language. Meaningful interchange of information using JSON requires agreement among the involved parties on the specific semantics to be applied. Defining specific semantic interpretations of JSON is potentially a topic for other specifications. Similarly, language mappings of JSON can also be independently specified. For example, ECMA-262 defines mappings between valid JSON texts and ECMAScript’s runtime data structures.},
  file = {C:\Users\ron\Zotero\storage\77GFI9VY\2017 - The JSON Data Interchange Syntax.pdf}
}

@book{jurafskySpeechLanguageProcessing2009,
  title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  shorttitle = {Speech and Language Processing},
  author = {Jurafsky, Dan and Martin, James H.},
  date = {2009},
  series = {Prentice {{Hall}} Series in Artificial Intelligence},
  edition = {2nd ed},
  publisher = {{Pearson Prentice Hall}},
  location = {{Upper Saddle River, N.J}},
  isbn = {978-0-13-187321-6},
  pagetotal = {988},
  keywords = {Automatic speech recognition,Computational linguistics},
  annotation = {OCLC: 213375806}
}

@article{kallasPracticallyCorrectJustinTime,
  title = {Practically {{Correct}}, {{Just-in-Time Shell Script Parallelization}}},
  author = {Kallas, Konstantinos and Mustafa, Tammam and Bielak, Jan and Karnikis, Dimitris and Dang, Thurston H Y and Greenberg, Michael and Vasilakis, Nikos},
  pages = {17},
  abstract = {Recent shell-script parallelization systems enjoy mostly automated speedups by parallelizing scripts ahead-of-time. Unfortunately, such static parallelization is hampered by dynamic behavior pervasive in shell scripts—e.g., variable expansion and command substitution—which often requires reasoning about the current state of the shell and filesystem.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\Z55VR7GR\Kallas et al. - Practically Correct, Just-in-Time Shell Script Par.pdf}
}

@inproceedings{khanIdentificationAnalysisPerformance2009,
  title = {Identification and Analysis of Performance Metrics for Real Time Operating System},
  booktitle = {2009 {{International Conference}} on {{Emerging Technologies}}},
  author = {Khan, Zahid and Hussain, Khalid and Khan, Zafrullah and Mir, Sana Ahmed},
  date = {2009-10},
  pages = {183--187},
  doi = {10.1109/ICET.2009.5353177},
  abstract = {RTOS performance analysis is critical during the design and integration of embedded software to guarantee that application time constraints will be met at run time. To select an appropriate Operating System for an embedded system for a specific application, OS services needs to be analyzed. These OS services are identified by parameters to form Performance Metrics. The Performance Metrics selected include Context switching time, Preemption time, Interrupt Latency, Semaphore Shuffling time. In this research the Performance Metrics is analyzed in order to select right OS for the an embedded system for a specific application.},
  eventtitle = {2009 {{International Conference}} on {{Emerging Technologies}}},
  keywords = {Application software,Embedded software,Embedded system,Kernel,Measurement,Operating systems,Performance analysis,Processor scheduling,Real time systems,Resource management},
  file = {C:\Users\ron\Zotero\storage\XG3Z9SPR\Identification_and_analysis_of_performance_metrics_for_real_time_operating_system.pdf}
}

@inproceedings{khanjaniAspectsChoosingOpen2011,
  title = {The Aspects of Choosing Open Source versus Closed Source},
  booktitle = {2011 {{IEEE Symposium}} on {{Computers}} \& {{Informatics}}},
  author = {Khanjani, Atieh and Sulaiman, Riza},
  date = {2011-03},
  pages = {646--649},
  publisher = {{IEEE}},
  location = {{Kuala Lumpur, Malaysia}},
  doi = {10.1109/ISCI.2011.5958992},
  url = {http://ieeexplore.ieee.org/document/5958992/},
  urldate = {2022-05-23},
  abstract = {Closed source software, is a type of software that is licensed under the exclusive legal right of its owner. It is also purchasable by users by paying amount of money. Open Source Software (OSS) is software available with its source code under an open source license to study and modify the code. Open Source Software Development (OSSD) is the process to develop OSS. Many industries try using OSSD as they see the advantages of open source compared to closed source software development. This research presents the reasons of recently using OSSD model rather than traditional closed source approach. The result is to show the differences between closed source and open source process and how open source can effect on quality through its particular features. It also identifies and addresses the challenges and benefits faced by the users against traditional closed source model.},
  eventtitle = {Informatics ({{ISCI}})},
  isbn = {978-1-61284-689-7},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\2BDQ7HCQ\Khanjani and Sulaiman - 2011 - The aspects of choosing open source versus closed .pdf}
}

@inproceedings{kimThreadClusterMemory2010,
  title = {Thread {{Cluster Memory Scheduling}}: {{Exploiting Differences}} in {{Memory Access Behavior}}},
  shorttitle = {Thread {{Cluster Memory Scheduling}}},
  booktitle = {Proceedings of the 2010 43rd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Kim, Yoongu and Papamichael, Michael and Mutlu, Onur and Harchol-Balter, Mor},
  date = {2010-12-04},
  series = {{{MICRO}} '43},
  pages = {65--76},
  publisher = {{IEEE Computer Society}},
  location = {{USA}},
  doi = {10.1109/MICRO.2010.51},
  url = {https://doi.org/10.1109/MICRO.2010.51},
  urldate = {2022-06-23},
  abstract = {In a modern chip-multiprocessor system, memory is a shared resource among multiple concurrently executing threads. The memory scheduling algorithm should resolve memory contention by arbitrating memory access in such a way that competing threads progress at a relatively fast and even pace, resulting in high system throughput and fairness. Previously proposed memory scheduling algorithms are predominantly optimized for only one of these objectives: no scheduling algorithm provides the best system throughput and best fairness at the same time. This paper presents a new memory scheduling algorithm that addresses system throughput and fairness separately with the goal of achieving the best of both. The main idea is to divide threads into two separate clusters and employ different memory request scheduling policies in each cluster. Our proposal, Thread Cluster Memory scheduling (TCM), dynamically groups threads with similar memory access behavior into either the latency-sensitive (memory-non-intensive) or the bandwidth-sensitive (memory-intensive) cluster. TCM introduces three major ideas for prioritization: 1) we prioritize the latency-sensitive cluster over the bandwidth-sensitive cluster to improve system throughput, 2) we introduce a ``niceness'' metric that captures a thread's propensity to interfere with other threads, 3) we use niceness to periodically shuffle the priority order of the threads in the bandwidth-sensitive cluster to provide fair access to each thread in a way that reduces inter-thread interference. On the one hand, prioritizing memory-non-intensive threads significantly improves system throughput without degrading fairness, because such ``light'' threads only use a small fraction of the total available memory bandwidth. On the other hand, shuffling the priority order of memory-intensive threads improves fairness because it ensures no thread is disproportionately slowed down or starved. We evaluate TCM on a wide variety of multiprogrammed workloads and compare its performance to four previously proposed scheduling algorithms, finding that TCM achieves both the best system throughput and fairness. Averaged over 96 workloads on a 24-core system with 4 memory channels, TCM improves system throughput and reduces maximum slowdown by 4.6\%/38.6\% compared to ATLAS (previous work providing the best system throughput) and 7.6\%/4.6\% compared to PAR-BS (previous work providing the best fairness).},
  isbn = {978-0-7695-4299-7},
  keywords = {fairness,memory access behavior,memory scheduling,niceness,system throughput,thread cluster},
  file = {C:\Users\ron\Zotero\storage\2F6RW43U\MICRO.2010.51.pdf}
}

@inproceedings{kinebuchiHardwareAbstractionLayer2009,
  title = {A {{Hardware Abstraction Layer}} for {{Integrating Real-Time}} and {{General-Purpose}} with {{Minimal Kernel Modification}}},
  booktitle = {2009 {{Software Technologies}} for {{Future Dependable Distributed Systems}}},
  author = {Kinebuchi, Yuki and Kanda, Wataru and Yumura, Yu and Makijima, Kazuo and Nakajima, Tatsuo},
  date = {2009-03},
  pages = {112--116},
  doi = {10.1109/STFSSD.2009.39},
  abstract = {Integrating real-time and general-purpose operating systems into a single embedded device is promising scheme to support both real-time responsiveness and rich functionality with reasonable engineering cost. However, even if the modification for a single combination of real-time and general-purpose operating systems is small, it would be problematic when considering combinations of various operating systems. It is usual for manufacturers to leverage diverse operating systems, depending on real-time constraint, application set, software property they own, etc. In this paper, we introduce a thin virtualization layer for integrating real-time and general-purpose operating systems with a minimal modification to each operating system kernel. Using our virtualization layer, we integrated TOPPERS RTOS and Linux on the SH-4A processor platform with a few dozen lines of modification to each kernel, while introducing sufficiently-small overhead to both real-time responsiveness and system throughput.},
  eventtitle = {2009 {{Software Technologies}} for {{Future Dependable Distributed Systems}}},
  keywords = {Embedded system,embedded systems,Kernel,Linux,Operating systems,Real time systems,Registers,Software,system integration,virtualization},
  file = {C:\Users\ron\Zotero\storage\T443CRK8\A_Hardware_Abstraction_Layer_for_Integrating_Real-Time_and_General-Purpose_with_Minimal_Kernel_Modification.pdf}
}

@article{kiyavitskayaRequirementsToolsAmbiguity2008,
  title = {Requirements for Tools for Ambiguity Identification and Measurement in Natural Language Requirements Specifications},
  author = {Kiyavitskaya, Nadzeya and Zeni, Nicola and Mich, Luisa and Berry, Daniel M.},
  date = {2008-08-21},
  journaltitle = {Requirements Engineering},
  shortjournal = {Requir. Eng.},
  volume = {13},
  number = {3},
  pages = {207--239},
  issn = {0947-3602},
  doi = {10.1007/s00766-008-0063-7},
  url = {https://doi.org/10.1007/s00766-008-0063-7},
  urldate = {2022-06-09},
  abstract = {This paper proposes a two-step approach to identifying ambiguities in natural language (NL) requirements specifications (RSs). In the first step, a tool would apply a set of ambiguity measures to a RS in order to identify potentially ambiguous sentences in the RS. In the second step, another tool would show what specifically is potentially ambiguous about each potentially ambiguous sentence. The final decision of ambiguity remains with the human users of the tools. The paper describes several requirements-identification experiments with several small NL RSs using four prototypes of the first tool based on linguistic instruments and resources of different complexity and a manual mock-up of the second tool.},
  keywords = {Ambiguity identification,Disambiguation,Natural language processing,Natural langugage,Prototype tools,Requirements specification tools,Requirements-identification experiments,Requirements-identification prototype},
  file = {C:\Users\ron\Zotero\storage\26FSNMGM\Requirements_for_tools_for_ambiguity_ide.pdf}
}

@inproceedings{klein-etal-2020-opennmt,
  title = {The {{OpenNMT}} Neural Machine Translation Toolkit: 2020 Edition},
  booktitle = {Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: {{Research}} Track)},
  author = {Klein, Guillaume and Hernandez, François and Nguyen, Vincent and Senellart, Jean},
  date = {2020-10},
  pages = {102--109},
  publisher = {{Association for Machine Translation in the Americas}},
  location = {{Virtual}},
  url = {https://aclanthology.org/2020.amta-research.9},
  abstract = {OpenNMT is a multi-year open-source ecosystem for neural machine translation (NMT) and natural language generation (NLG). The toolkit consists of multiple projects to cover the complete machine learning workflow: from data preparation to inference acceleration. The systems prioritize efficiency, modularity, and extensibility with the goal of supporting research into model architectures, feature representations, and source modalities, while maintaining API stability and competitive performance for production usages. OpenNMT has been used in several production MT systems and cited in more than 700 research papers.},
  file = {C:\Users\ron\Zotero\storage\ARXZJSWX\Klein et al. - 2020 - The OpenNMT neural machine translation toolkit 20.pdf}
}

@article{kleinOpenNMTNeuralMachine2018,
  title = {{{OpenNMT}}: {{Neural Machine Translation Toolkit}}},
  shorttitle = {{{OpenNMT}}},
  author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Nguyen, Vincent and Senellart, Jean and Rush, Alexander M.},
  date = {2018},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1805.11462},
  url = {https://arxiv.org/abs/1805.11462},
  urldate = {2023-10-21},
  abstract = {OpenNMT is an open-source toolkit for neural machine translation (NMT). The system prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques. OpenNMT has been used in several production MT systems, modified for numerous research papers, and is implemented across several deep learning frameworks.},
  version = {1},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C:\Users\ron\Zotero\storage\V86X8PRQ\Klein et al. - 2018 - OpenNMT Neural Machine Translation Toolkit.pdf}
}

@inproceedings{krizCybersecurityPrinciplesIndustry2011,
  title = {Cybersecurity Principles for Industry and Government: {{A}} Useful Framework for Efforts Globally to Improve Cybersecurity},
  shorttitle = {Cybersecurity Principles for Industry and Government},
  booktitle = {2011 {{Second Worldwide Cybersecurity Summit}} ({{WCS}})},
  author = {Kriz, Danielle},
  date = {2011-06},
  pages = {1--3},
  abstract = {To better inform the public cybersecurity discussion, in January 2011 the Information Technology Industry Council (ITI) developed a comprehensive set of cybersecurity principles for industry and government [1]. ITI's six principles aim to provide a useful and important lens through which any efforts to improve cybersecurity should be viewed.},
  eventtitle = {2011 {{Second Worldwide Cybersecurity Summit}} ({{WCS}})},
  keywords = {Computer security,cybersecurity,Cyberspace,framework,government,Government,Industries,industry,Information Technology Industry Council,ITI,principles,Risk management},
  file = {C:\Users\ron\Zotero\storage\BRNTLHTF\Kriz - 2011 - Cybersecurity principles for industry and governme.pdf}
}

@inproceedings{laadanOperatingSystemVirtualization2010,
  title = {Operating System Virtualization: Practice and Experience},
  shorttitle = {Operating System Virtualization},
  booktitle = {Proceedings of the 3rd {{Annual Haifa Experimental Systems Conference}} on - {{SYSTOR}} '10},
  author = {Laadan, Oren and Nieh, Jason},
  date = {2010},
  pages = {1},
  publisher = {{ACM Press}},
  location = {{Haifa, Israel}},
  doi = {10.1145/1815695.1815717},
  url = {http://portal.acm.org/citation.cfm?doid=1815695.1815717},
  urldate = {2022-06-06},
  eventtitle = {The 3rd {{Annual Haifa Experimental Systems Conference}}},
  isbn = {978-1-60558-908-4},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\J96WD7US\1815695.1815717.pdf}
}

@book{langackerCognitiveGrammarBasic2008,
  title = {Cognitive {{Grammar}}: {{A Basic Introduction}}},
  shorttitle = {Cognitive {{Grammar}}},
  author = {Langacker, Ronald},
  date = {2008-02-14},
  edition = {1},
  publisher = {{Oxford University PressNew York}},
  doi = {10.1093/acprof:oso/9780195331967.001.0001},
  url = {https://academic.oup.com/book/10750},
  urldate = {2023-09-01},
  abstract = {Abstract             Cognitive Grammar is a radical alternative to the formalist theories that have dominated linguistic theory during the last half century. Instead of an objectivist semantics based on truth conditions or logical deduction, it adopts a conceptualist semantics based on human experience, our capacity to construe situations in alternate ways, and processes of imagination and mental construction. A conceptualist semantics makes possible an account of grammar which views it as being inherently meaningful (rather than an autonomous formal system). Grammar forms a continuum with lexicon, residing in assemblies of symbolic structures, i.e. pairings of conceptual structures and symbolizing phonological structures. Thus all grammatical elements are meaningful. It is shown in detail how Cognitive Grammar handles the major problems a theory of grammar has to deal with: grammatical classes, constructions, the relationship of grammar and lexicon, the capturing of regularities, and imposition of the proper restrictions. It is further shown how the framework applies to central domains of language structure: deixis, nominal structure, clausal structure, and complex sentences. Consideration is also given to discourse, the temporal dimension of grammar, and what it reveals about cognitive processes and the construction of our mental world.},
  isbn = {978-0-19-533196-7 978-0-19-986820-9},
  langid = {english}
}

@inproceedings{langleySpokenLanguageParsing2002,
  title = {Spoken Language Parsing Using Phrase-Level Grammars and Trainable Classifiers},
  booktitle = {Proceedings of the {{ACL-02}} Workshop on {{Speech-to-speech}} Translation: Algorithms and Systems - {{Volume}} 7},
  author = {Langley, Chad and Lavie, Alon and Levin, Lori and Wallace, Dorcas and Gates, Donna and Peterson, Kay},
  date = {2002-07-11},
  series = {{{S2S}} '02},
  pages = {15--22},
  publisher = {{Association for Computational Linguistics}},
  location = {{USA}},
  doi = {10.3115/1118656.1118659},
  url = {https://dl.acm.org/doi/10.3115/1118656.1118659},
  urldate = {2023-08-14},
  abstract = {In this paper, we describe a novel approach to spoken language analysis for translation, which uses a combination of grammar-based phrase-level parsing and automatic classification. The job of the analyzer is to produce a shallow semantic interlingua representation for spoken task-oriented utterances. The goal of our hybrid approach is to provide accurate real-time analyses while improving robustness and portability to new domains and languages.},
  file = {C:\Users\ron\Zotero\storage\WXKIWXB6\1118656.1118659.pdf}
}

@article{lapouchnianGoalOrientedRequirementsEngineering,
  title = {Goal-{{Oriented Requirements Engineering}}: {{An Overview}} of the {{Current Research}}},
  author = {Lapouchnian, Alexei},
  pages = {32},
  url = {http://www.cs.utoronto.ca/~alexei/pub/Lapouchnian-Depth.pdf},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\KLV6NX9X\Lapouchnian - Goal-Oriented Requirements Engineering An Overvie.pdf}
}

@article{lawtonOpenSourceSecurity2002,
  title = {Open Source Security: Opportunity or Oxymoron?},
  shorttitle = {Open Source Security},
  author = {Lawton, G.},
  date = {2002-03},
  journaltitle = {Computer},
  shortjournal = {Computer},
  volume = {35},
  number = {3},
  pages = {18--21},
  issn = {00189162},
  doi = {10.1109/2.989921},
  url = {http://ieeexplore.ieee.org/document/989921/},
  urldate = {2022-05-23},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\T2XFPRVQ\Lawton - 2002 - Open source security opportunity or oxymoron.pdf}
}

@article{leaCOOLArchitectureAbstractions,
  title = {The {{COOL}} Architecture and Abstractions for Object-Oriented Distributed Operating Systems},
  author = {Lea, Rodger and Jacquemot, Christian},
  pages = {8},
  abstract = {Building distributed operating systems benefits from the micro-kernel approach by allowing better support for modularization. However, we believe that we need to take this support a step further. A more modular, or object oriented approach is needed if we wish to cross the barrier of complezity that is holding back distributed operating system development. The Chorus Object Oriented Layer (COOL) is a layer built above the Chorus micro-kernel designed to eztend the micro-kernel abstractions with support for object oriented systems. COOL v2, the second iteration of this layer provides generic support for clusters of objects, in a distributed virtual memory model. It is built as a layered system where the lowest layer support only clusters and the upper layers support objects.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\XHCU897X\Lea and Jacquemot - The COOL architecture and abstractions for object-.pdf}
}

@online{leeReducingJournalingHarm2016,
  title = {Reducing {{Journaling Harm}} on {{Virtualized I}}/{{O Systems}} | {{Proceedings}} of the 9th {{ACM International}} on {{Systems}} and {{Storage Conference}}},
  author = {Lee, Eunji and Bahn, Hyokyung and Jeong, Minseong and Kim, Sunghwan and Yeon, Jesung and Yoo, Seunghoon and Noh, Sam H and Shin, Kang G.},
  date = {2016-06},
  url = {https://dl.acm.org/doi/abs/10.1145/2928275.2928289},
  urldate = {2022-06-24},
  abstract = {This paper analyzes the host cache effectiveness in full virtualization, particularly associated with journaling of guests. We observe that the journal access of guests degrades cache performance largely due to the write-once access pattern and the frequent sync operations. To remedy this problem, we design and implement a novel caching policy, called PDC (Pollution Defensive Caching), that detects the journal accesses and prevents them from entering the host cache. The proposed PDC is implemented in QEMU-KVM 2.1 on Linux 4.14 and provides 3-32\% performance improvement for various file and I/O benchmarks.},
  langid = {english},
  organization = {{ACM Conferences}},
  file = {C\:\\Users\\ron\\Zotero\\storage\\599YVBQI\\Reducing Journaling Harm on Virtualized IO System.pdf;C\:\\Users\\ron\\Zotero\\storage\\GUWL9EYV\\2928275.html}
}

@inproceedings{leVerifyingEventualityProperties2015,
  title = {Verifying Eventuality Properties of Imprecise System Requirements Using Event-{{B}}},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Le, Hong Anh and Truong, Ninh Thuan and Nakajima, Shin},
  date = {2015-04-13},
  series = {{{SAC}} '15},
  pages = {1651--1653},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2695664.2696038},
  url = {https://doi.org/10.1145/2695664.2696038},
  urldate = {2022-10-22},
  abstract = {Formally checking properties of functional requirements needs precise descriptions. However, the stakeholders sometimes describe the system with ambiguous, vague or fuzzy terms, hence formal frameworks for modeling and verifying such requirements are desirable. As one such, the Fuzzy If-Then rules have been used, but analyzing their functional properties still needs new methods. In this paper, we propose a refinement-based modeling approach for specification and verification of such requirements which are represented by Fuzzy If-Then rules. First, we make use of Event-B refinement where we provide a set of translation rules from Fuzzy If-Then rules to Event-B notations, then show how to verify both safety and eventuality properties with RODIN/Event-B. Finally, we illustrate the proposed method on an example case of Crane Controller.},
  isbn = {978-1-4503-3196-8},
  keywords = {eventuality,fuzzy set,refinement-based modeling},
  file = {C:\Users\ron\Zotero\storage\9P8HEYTT\Le et al. - 2015 - Verifying eventuality properties of imprecise syst.pdf}
}

@article{levyQuantificationCybersecurityFootprint2022,
  title = {Towards the Quantification of Cybersecurity Footprint for {{SMBs}} Using the {{CMMC}} 2.0},
  author = {Levy, Yair and Gafni, Ruti},
  date = {2022},
  journaltitle = {Online Journal of Applied Knowledge Management (OJAKM)},
  volume = {10},
  number = {1},
  pages = {43--61},
  doi = {10.36965/OJAKM.2022.10(1)43-61},
  url = {https://www.iiakm.org/ojakm/articles/2022/OJAKM_volume10_1pp43-61.php},
  urldate = {2023-05-25},
  abstract = {Organizations, small and big, are faced with major cybersecurity challenges over the past several decades, as the proliferation of information systems and mobile devices expand. While larger organizations invest significant efforts in developing approaches to deal with cybersecurity incidents, Small and Medium Businesses (SMBs) are still struggling with ways to both keep their businesses alive and secure their systems to the best of their abilities. When it comes to critical systems, such as defense industries, the interconnectivities of organizations in the supply-chain have demonstrated to be problematic given the depth required to provide a high-level cybersecurity posture. The United States (U.S.) Department of Defense (DoD) with the partnership of the Defense Industry Base (DIB) have developed the Cybersecurity Maturity Model Certification (CMMC) in 2020 with a third-party mandate for Level 1 certification. Following an outcry from many DIB organizations, a newly revised CMMC 2.0 was introduced in late 2021 where Level 1 (Fundamental) was adjusted for annual self-assessment. CMMC 2.0 provides the 17 practices that organizations should self-assess. While these 17 practices provide initial guidance for assessment, the specific level of measurement and how it impacts their overall cybersecurity posture is vague. Specifically, many of these practices use non-quantifiable terms such as “limit”, “verify”, “control”, “identify”, etc. The focus of this work is to provide SMBs with a quantifiable method to self-assess their Cybersecurity Footprint following the CMMC 2.0 Level 1 practices. This paper outlines the foundational literature work conducted in support of the proposed quantification Cybersecurity Footprint Index (CFI) using 26 elements that correspond to the relevant CMMC 2.0 Level 1 practices.},
  file = {C:\Users\ron\Zotero\storage\HETI9NMU\Levy_Gafni_2021.pdf}
}

@article{leyva-del-foyoIntegratedTaskInterrupt2012,
  title = {Integrated {{Task}} and {{Interrupt Management}} for {{Real-Time Systems}}},
  author = {Leyva-del-Foyo, Luis E. and Mejia-Alvarez, Pedro and de Niz, Dionisio},
  options = {useprefix=true},
  date = {2012-07-01},
  journaltitle = {ACM Transactions on Embedded Computing Systems},
  shortjournal = {ACM Trans. Embed. Comput. Syst.},
  volume = {11},
  number = {2},
  pages = {32:1--32:31},
  issn = {1539-9087},
  doi = {10.1145/2220336.2220344},
  url = {https://doi.org/10.1145/2220336.2220344},
  urldate = {2022-06-09},
  abstract = {Real-time scheduling algorithms like RMA or EDF and their corresponding schedulability test have proven to be powerful tools for developing predictable real-time systems. However, the traditional interrupt management model presents multiple inconsistencies that break the assumptions of many of the real-time scheduling tests, diminishing its utility. In this article, we analyze these inconsistencies and present a model that resolves them by integrating interrupts and tasks in a single scheduling model. We then use the RMA theory to calculate the cost of the model and analyze the circumstances under which it can provide the most value. This model was implemented in a kernel module. The portability of the design of our module is discussed in terms of its independence from both the hardware and the kernel. We also discuss the implementation issues of the model over conventional PC hardware, along with its cost and novel optimizations for reducing the overhead. Finally, we present our experimental evaluation to show evidence of its temporal determinism and overhead.},
  keywords = {interrupt scheduling,Predictability,programmable interrupt controller},
  file = {C:\Users\ron\Zotero\storage\BRNDRFPC\2220336.2220344.pdf}
}

@article{liChronosTimingAnalyzer2007,
  title = {Chronos: {{A}} Timing Analyzer for Embedded Software},
  shorttitle = {Chronos},
  author = {Li, Xianfeng and Liang, Yun and Mitra, Tulika and Roychoudhury, Abhik},
  date = {2007-12-01},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  series = {Special Issue on {{Experimental Software}} and {{Toolkits}}},
  volume = {69},
  number = {1},
  pages = {56--67},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2007.01.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0167642307001633},
  urldate = {2022-06-21},
  abstract = {Estimating the Worst Case Execution Time (WCET) of real-time embedded software is an important problem. WCET is defined as the upper bound b on the execution time of a program P on a processor X such that for any input the execution time of P on X is guaranteed to not exceed b. Such WCET estimates are crucial for schedulability analysis of real-time systems. In this paper, we present Chronos, a static analysis tool for generating WCET estimates of C programs. It performs detailed micro-architectural modeling to capture the timing effects of the underlying processor platform. Consequently, we can provide safe but tight WCET estimate of a given C program running on a complex modern processor. Chronos is an open-source distribution specifically suited to the needs of the research community. We support processor models captured by the popular SimpleScalar architectural simulator rather than targeting specific commercial processors. This makes the Chronos flexible, extensible and easily accessible to the researcher.},
  langid = {english},
  keywords = {Worst Case Execution Time (WCET) analysis},
  file = {C\:\\Users\\ron\\Zotero\\storage\\HTL7TNMQ\\Li et al. - 2007 - Chronos A timing analyzer for embedded software.pdf;C\:\\Users\\ron\\Zotero\\storage\\VLX87GHM\\S0167642307001633.html}
}

@article{liedtkeRealMicrokernels1996,
  title = {Toward Real Microkernels},
  author = {Liedtke, Jochen},
  date = {1996-09-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {39},
  number = {9},
  pages = {70--77},
  issn = {0001-0782},
  doi = {10.1145/234215.234473},
  url = {https://doi.org/10.1145/234215.234473},
  urldate = {2022-06-15},
  file = {C:\Users\ron\Zotero\storage\N656HRR8\234215.234473.pdf}
}

@online{lineroPronouncingThingsAmazon2018,
  title = {Pronouncing {{Things}} with {{Amazon}}’s {{Polly}}},
  author = {Linero, Katie},
  date = {2018-09-13},
  url = {https://cuttlesoft.com/blog/2018/09/13/pronouncing-things-with-amazons-polly/}
}

@inproceedings{liPIMLInterruptProgram2014,
  title = {{{pIML}} – {{An Interrupt Program Modelling Language}} for {{Real-Time}} and {{Embedded Systems}}},
  booktitle = {2014 21st {{Asia-Pacific Software Engineering Conference}}},
  author = {Li, Xin and Huang, Yanhong and Shi, Jianqi and Guo, Jian and Zhu, Huibiao and Xu, Yuanmin},
  date = {2014-12},
  volume = {1},
  pages = {78--85},
  issn = {1530-1362},
  doi = {10.1109/APSEC.2014.21},
  abstract = {In the design of dependable software for real-time and embedded systems, the quantitative analysis of program behavior and system performance is a crucial but extremely difficult issue, the challenge of which is exacerbated due to the random city and nondeterminism of interrupt events and the corresponding handling behaviors. Moreover, time analysis is also need to be taken into account for such kinds of systems. Thus the research on a theory which integrates interrupt behaviors and time analysis seems to be important and challenging. In this paper, we propose an interrupt modeling language pIML including the probabilistic feature to describe the programs with interrupts. We explore a probabilistic operational semantics to depict the actions of pIML. Meanwhile, we also implement this operational semantics we proposed on Maude platform, which fill the gap between the theory and practice. Maude supports rewriting logic, equational logic, and etc. The rewrite rules of rewriting logic can very well implement the transition rules of probabilistic operational semantics. Based on this implementation, it is very convenient to simulate the program written in pIML and analyze the behaviors of program in the presence of interrupts quantitatively.},
  eventtitle = {2014 21st {{Asia-Pacific Software Engineering Conference}}},
  keywords = {Chaos,Embedded systems,Probabilistic logic,Real-time systems,Semantics,Syntactics,Time factors},
  file = {C:\Users\ron\Zotero\storage\6BCM3367\Li et al. - 2014 - pIML – An Interrupt Program Modelling Language for.pdf}
}

@inproceedings{liTracingRequirementsSoftware2017,
  title = {Tracing Requirements in Software Design},
  booktitle = {Proceedings of the 2017 {{International Conference}} on {{Software}} and {{System Process}}},
  author = {Li, Zeheng and Chen, Mingrui and Huang, LiGuo and Ng, Vincent and Geng, Ruili},
  date = {2017-07-05},
  series = {{{ICSSP}} 2017},
  pages = {25--29},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3084100.3084102},
  url = {https://doi.org/10.1145/3084100.3084102},
  urldate = {2022-10-22},
  abstract = {Software requirement analysis is an essential step in software development process, which defines what is to be built in a project. Requirements are mostly written in text and will later evolve to fine-grained and actionable artifacts with details about system configurations, technology stacks, etc. Tracing the evolution of requirements enables stakeholders to determine the origin of each requirement and understand how well the software's design reflects to its requirements. Reckoning requirements traceability is not a trivial task, we focus on applying machine learning approach to classify traceability between various associated requirements. In particular, we investigate a 2-learner, ontology-based approach, where we train two classifiers to separately exploit two types of features, lexical features and features derived from a hand-built ontology. In comparison to a supervised baseline system that uses only lexical features, our approach yields a relative error reduction of 25.9\%. Most interestingly, results do not deteriorate when the hand-built ontology is replaced with its automatically constructed counterpart.},
  isbn = {978-1-4503-5270-3},
  keywords = {Machine Learning,Requirements Traceability,Software Design},
  file = {C:\Users\ron\Zotero\storage\W94NHGS2\Li et al. - 2017 - Tracing requirements in software design.pdf}
}

@inproceedings{liuGeneratingVisualizingTrace2022,
  title = {Generating and Visualizing Trace Link Explanations},
  booktitle = {Proceedings of the 44th {{International Conference}} on {{Software Engineering}}},
  author = {Liu, Yalin and Lin, Jinfeng and Anuyah, Oghenemaro and Metoyer, Ronald and Cleland-Huang, Jane},
  date = {2022-05-21},
  series = {{{ICSE}} '22},
  pages = {1033--1044},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3510003.3510129},
  url = {https://doi.org/10.1145/3510003.3510129},
  urldate = {2022-10-22},
  abstract = {Recent breakthroughs in deep-learning (DL) approaches have resulted in the dynamic generation of trace links that are far more accurate than was previously possible. However, DL-generated links lack clear explanations, and therefore non-experts in the domain can find it difficult to understand the underlying semantics of the link, making it hard for them to evaluate the link's correctness or suitability for a specific software engineering task. In this paper we present a novel NLP pipeline for generating and visualizing trace link explanations. Our approach identifies domain-specific concepts, retrieves a corpus of concept-related sentences, mines concept definitions and usage examples, and identifies relations between cross-artifact concepts in order to explain the links. It applies a post-processing step to prioritize the most likely acronyms and definitions and to eliminate non-relevant ones. We evaluate our approach using project artifacts from three different domains of interstellar telescopes, positive train control, and electronic healthcare systems, and then report coverage, correctness, and potential utility of the generated definitions. We design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales, and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface. Results show that the explanations presented in the interface helped non-experts to understand the underlying semantics of a trace link and improved their ability to vet the correctness of the link.},
  isbn = {978-1-4503-9221-1},
  keywords = {concept mining,explanation interface,software traceability},
  file = {C:\Users\ron\Zotero\storage\V8696VNK\Liu et al. - 2022 - Generating and visualizing trace link explanations.pdf}
}

@inproceedings{liuIDolaBridgeModeling2014,
  title = {{{iDola}}: {{Bridge Modeling}} to {{Verification}} and {{Implementation}} of {{Interrupt-Driven Systems}}},
  shorttitle = {{{iDola}}},
  booktitle = {2014 {{Theoretical Aspects}} of {{Software Engineering Conference}}},
  author = {Liu, Han and Zhang, Hehua and Jiang, Yu and Song, Xiaoyu and Gu, Ming and Sun, Jiaguang},
  date = {2014-09},
  pages = {193--200},
  doi = {10.1109/TASE.2014.33},
  abstract = {In real-time embedded applications, interrupt-driven systems are widely adopted due to strict timing requirements. However, development of interrupt-driven systems is time-consuming and error-prone. To conveniently ensure a trustworthy system design and implementation is a challenging problem, especially in complex applications. In this paper, we present a novel domain-specific language called iDola to model interrupt-driven systems declaratively and concisely. A major strength of iDola is the feasibility to capture complex interrupt handling mechanism in real-time operating systems and target platforms, such as delayed service and buffered processing. We also propose the formal operational semantics and code generation algorithm of iDola, so that iDola models can be transformed to timed automata for verification and loaded to generate platform-specific codes. We apply iDola on the modeling of an industrial interrupt-driven system, multifunction vehicle bus controller which runs in an embedded environment with eCos operating system. Based on iDola, the system is modeled with a dispatcher which embodies advanced interrupt handling in eCos, including buffered interrupt service routine and deferred service routine. Through transformation, the system design is verified and design bugs are detected. Code generation is also executed using the proposed algorithm. Generated codes display comparatively equal performance in the real system. We believe iDola can facilitate building a trustworthy interrupt-driven system.},
  eventtitle = {2014 {{Theoretical Aspects}} of {{Software Engineering Conference}}},
  keywords = {Analytical models,Automata,DH-HEMTs,domain-specific language,interrupt handling,Interrupt-driven system,Load modeling,Real-time systems,Semantics,Timing},
  file = {C:\Users\ron\Zotero\storage\N873DJ8W\Liu et al. - 2014 - iDola Bridge Modeling to Verification and Impleme.pdf}
}

@article{liuImprovingRealTimeInterrupt2011,
  title = {On {{Improving Real-Time Interrupt Latencies}} of {{Hybrid Operating Systems}} with {{Two-Level Hardware Interrupts}}},
  author = {Liu, Miao and Liu, Duo and Wang, Yi and Wang, Meng and Shao, Zili},
  date = {2011-07},
  journaltitle = {IEEE Transactions on Computers},
  volume = {60},
  number = {7},
  pages = {978--991},
  issn = {1557-9956},
  doi = {10.1109/TC.2010.119},
  abstract = {In this paper, we propose to implement hybrid operating systems based on two-level hardware interrupts. We analyze and model the worst-case real-time interrupt latency for RTAI and identify the key component for its optimization. Then, we propose our methodology to implement hybrid operating systems with two-level hardware interrupts by combining the real-time kernel and the time sharing OS (Operating System) kernel. Based on the methodology, we discuss the important issues for the implementation. Finally, we implement a hybrid system called RTLinux-THIN (Real-Time LINUX with Two-level Hardware INterrupts) on the ARM architecture by combining ARM Linux kernel 2.6.9 and μC/OS-II. We conduct experiments on a set of real application programs including mplayer, Bonnie, and iperf, and compare the interrupt latency and interrupt task distributions for RTLinux-THIN (with and without cache locking), RTAI, Linux, and Linux with RT patch on a hardware platform based on Intel PXA270 processor. The results show that our scheme not only provides an easy method for implementing hybrid systems but also achieves the performance improvement for both the time sharing and real-time subsystems.},
  keywords = {Emulation,Hardware,Hybrid operating systems,Kernel,Linux,Real time systems,real-time interrupt latency,RTAI,two-level hardware interrupts.},
  file = {C:\Users\ron\Zotero\storage\6TPPACEW\On_Improving_Real-Time_Interrupt_Latencies_of_Hybrid_Operating_Systems_with_Two-Level_Hardware_Interrupts.pdf}
}

@article{lopezStatisticalMachineTranslation2008,
  title = {Statistical Machine Translation},
  author = {Lopez, Adam},
  date = {2008-08},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {40},
  number = {3},
  pages = {1--49},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/1380584.1380586},
  url = {https://dl.acm.org/doi/10.1145/1380584.1380586},
  urldate = {2023-10-21},
  abstract = {Statistical machine translation (SMT) treats the translation of natural language as a machine learning problem. By examining many samples of human-produced translation, SMT algorithms automatically learn how to translate. SMT has made tremendous strides in less than two decades, and new ideas are constantly introduced. This survey presents a tutorial overview of the state of the art. We describe the context of the current research and then move to a formal problem description and an overview of the main subproblems: translation modeling, parameter estimation, and decoding. Along the way, we present a taxonomy of some different approaches within these areas. We conclude with an overview of evaluation and a discussion of future directions.},
  langid = {english}
}

@inproceedings{loucksMicrokernelbasedOperatingSystem1993,
  title = {A Microkernel-Based Operating System for Personal Digital Assistants},
  booktitle = {Proceedings of {{IEEE}} 4th {{Workshop}} on {{Workstation Operating Systems}}. {{WWOS-III}}},
  author = {Loucks, L. and Manikundalam, R. and Rawson, F.L.},
  date = {1993},
  pages = {9--13},
  publisher = {{IEEE Comput. Soc. Press}},
  location = {{Napa, CA, USA}},
  doi = {10.1109/WWOS.1993.348180},
  url = {http://ieeexplore.ieee.org/document/348180/},
  urldate = {2022-05-20},
  eventtitle = {{{IEEE}} 4th {{Workshop}} on {{Workstation Operating Systems}}. {{WWOS-III}}},
  isbn = {978-0-8186-4000-1},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\EZX35GU2\Loucks et al. - 1993 - A microkernel-based operating system for personal .pdf}
}

@video{lucasUltimateComputer1968,
  entrysubtype = {tvbroadcast},
  title = {The {{Ultimate Computer}}},
  booktitle = {Star {{Trek}}},
  editor = {Lucas, John Meredyth},
  editortype = {director},
  editora = {Fontana, D.C. and Wolfe, Laurence N.},
  editoratype = {scriptwriter},
  date = {1968-03-08},
  number = {Season 2, Episode 24},
  publisher = {{NBC}},
  abstract = {The Enterprise tests a computer that, if successful, could replace Kirk as the captain.},
  langid = {english}
}

@inproceedings{luGenerativeModelParsing2008,
  title = {A Generative Model for Parsing Natural Language to Meaning Representations},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lu, Wei and Ng, Hwee Tou and Lee, Wee Sun and Zettlemoyer, Luke S.},
  date = {2008-10-25},
  series = {{{EMNLP}} '08},
  pages = {783--792},
  publisher = {{Association for Computational Linguistics}},
  location = {{USA}},
  abstract = {In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.},
  file = {C:\Users\ron\Zotero\storage\W6CTQAMZ\1613715.1613815.pdf}
}

@inproceedings{luNaturalLanguageGeneration2009,
  title = {Natural Language Generation with Tree Conditional Random Fields},
  booktitle = {Proceedings of the 2009 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{Volume}} 1 - {{Volume}} 1},
  author = {Lu, Wei and Ng, Hwee Tou and Lee, Wee Sun},
  date = {2009-08-06},
  series = {{{EMNLP}} '09},
  pages = {400--409},
  publisher = {{Association for Computational Linguistics}},
  location = {{USA}},
  abstract = {This paper presents an effective method for generating natural language sentences from their underlying meaning representations. The method is built on top of a hybrid tree representation that jointly encodes both the meaning representation as well as the natural language in a tree structure. By using a tree conditional random field on top of the hybrid tree representation, we are able to explicitly model phrase-level dependencies amongst neighboring natural language phrases and meaning representation components in a simple and natural way. We show that the additional dependencies captured by the tree conditional random field allows it to perform better than directly inverting a previously developed hybrid tree semantic parser. Furthermore, we demonstrate that the model performs better than a previous state-of-the-art natural language generation model. Experiments are performed on two benchmark corpora with standard automatic evaluation metrics.},
  isbn = {978-1-932432-59-6},
  file = {C:\Users\ron\Zotero\storage\M65NHTEI\1699510.1699563.pdf}
}

@article{lynchOperatingSystemPerformance1972,
  title = {Operating System Performance},
  author = {Lynch, W. C.},
  date = {1972-07},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {15},
  number = {7},
  pages = {579--585},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/361454.361476},
  url = {https://dl.acm.org/doi/10.1145/361454.361476},
  urldate = {2022-06-06},
  abstract = {An overview of the current and future positions with respect to operating system performance is given. While a great deal of information and a large number of models for subsystems have been developed, gaps still exist in our knowledge. Because of the severe interactions between the various subsystems of an operating system, an overall model of the total system must be developed to be able to analyze and design the performance aspects of an operating system Although such total system designs are exceptional today, it is projected that they will become increasingly more common and necessary in the near future. Such a design philosophy will clearly have a severe impact on the way we go about modularizing operating and computer systems.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\ECE8KBZ5\361454.361476.pdf}
}

@article{maaoFfsck2014,
  title = {Ffsck},
  author = {MaAo and DraggaChris and C, Arpaci-DusseauAndrea and H, Arpaci-DusseauRemzi and Kirk, MckusickMarshall},
  date = {2014-01-01},
  journaltitle = {ACM Transactions on Storage (TOS)},
  publisher = {{ACM}},
  doi = {10.1145/2560011},
  url = {https://dl.acm.org/doi/abs/10.1145/2560011},
  urldate = {2022-06-24},
  abstract = {Failures, errors, and bugs can corrupt file systems and cause data loss, despite the presence of journals and similar preventive techniques. While consistency checkers such as fsck can detect corruption and repair a damaged image, they are generally ...},
  langid = {english},
  annotation = {PUB27 		New York, NY, USA},
  file = {C\:\\Users\\ron\\Zotero\\storage\\M4XCNF9Q\\MaAo et al. - 2014 - Ffsck.pdf;C\:\\Users\\ron\\Zotero\\storage\\YH9KRVJ9\\2560011.html}
}

@inproceedings{madrilesBoostingSinglethreadPerformance2009,
  title = {Boosting Single-Thread Performance in Multi-Core Systems through Fine-Grain Multi-Threading},
  booktitle = {{{ISCA}} '09},
  author = {Madriles, Carlos and López, Pedro and Codina, Josep M. and Gibert, Enric and Latorre, Fernando and Martínez, Alejandro and Martínez, Raúl and González, Antonio},
  date = {2009-06-20/2009-06-24},
  pages = {474--483},
  location = {{Austin Texas}},
  doi = {1555754.1555813},
  url = {https://dl.acm.org/doi/10.1145/1555754.1555813},
  eventtitle = {International {{Symposium}} on {{Computer Architecture}}},
  file = {C:\Users\ron\Zotero\storage\PS7TIT4R\1555815.1555813.pdf}
}

@article{maFfsckFastFileSystem2014,
  title = {Ffsck: {{The Fast File-System Checker}}},
  shorttitle = {Ffsck},
  author = {Ma, Ao and Dragga, Chris and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H. and Mckusick, Marshall Kirk},
  date = {2014-01},
  journaltitle = {ACM Transactions on Storage},
  shortjournal = {ACM Trans. Storage},
  volume = {10},
  number = {1},
  pages = {1--28},
  issn = {1553-3077, 1553-3093},
  doi = {10.1145/2560011},
  url = {https://dl.acm.org/doi/10.1145/2560011},
  urldate = {2022-06-24},
  abstract = {Crash failures, hardware errors, and file system bugs can corrupt file systems and cause data loss, despite the presence of journals and similar preventive techniques. While consistency checkers such as fsck can detect this corruption and restore a damaged image to a usable state, they are generally created as an afterthought, to be run only at rare intervals. Thus, checkers operate slowly, causing significant downtime for large scale storage systems when they are needed.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\63RK386Z\Ma et al. - 2014 - Ffsck The Fast File-System Checker.pdf}
}

@inproceedings{makiahoRequirementsManagementStudents2017,
  title = {Requirements {{Management}} in {{Students}}' {{Software Development Projects}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Computer Systems}} and {{Technologies}}},
  author = {Mäkiaho, Pekka and Poranen, Timo and Zhang, Zheying},
  date = {2017-06-23},
  series = {{{CompSysTech}}'17},
  pages = {203--210},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3134302.3134340},
  url = {https://doi.org/10.1145/3134302.3134340},
  urldate = {2022-10-22},
  abstract = {In this paper, we study requirements management practices in students' software development projects. The 12 projects studied applied iterative development models and agile practices. We analyze tools usage, methods, processes, common problems, risks, and challenges in requirements management. We also research changes in requirements statuses and conduct a more detailed analysis for status changes in three projects. As a result, we propose guidelines and suggestions to teachers and project managers based on our findings.},
  isbn = {978-1-4503-5234-5},
  keywords = {Requirements management,requirements statuses,student projects,tools},
  file = {C:\Users\ron\Zotero\storage\ITNM8EIR\Mäkiaho et al. - 2017 - Requirements Management in Students' Software Deve.pdf}
}

@inproceedings{maoPreliminaryFindingsDevSecOps2020,
  title = {Preliminary {{Findings}} about {{DevSecOps}} from {{Grey Literature}}},
  booktitle = {2020 {{IEEE}} 20th {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security}} ({{QRS}})},
  author = {Mao, Runfeng and Zhang, He and Dai, Qiming and Huang, Huang and Rong, Guoping and Shen, Haifeng and Chen, Lianping and Lu, Kaixiang},
  date = {2020-12},
  pages = {450--457},
  publisher = {{IEEE}},
  location = {{Macau, China}},
  doi = {10.1109/QRS51102.2020.00064},
  url = {https://ieeexplore.ieee.org/document/9282798/},
  urldate = {2022-06-20},
  eventtitle = {2020 {{IEEE}} 20th {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security}} ({{QRS}})},
  isbn = {978-1-72818-913-0},
  file = {C:\Users\ron\Zotero\storage\CCU9P4C5\Maodevsecops..pdf}
}

@inproceedings{mariaScrumSModelSafe2015,
  title = {{{ScrumS}}: A Model for Safe Agile Development},
  shorttitle = {{{ScrumS}}},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Management}} of Computational and Collective {{intElligence}} in {{Digital EcoSystems}}},
  author = {Maria, Rene Esteves and Rodrigues, Luiz Antonio and Pinto, Nelson Alves},
  date = {2015-10-25},
  pages = {43--47},
  publisher = {{ACM}},
  location = {{Caraguatatuba Brazil}},
  doi = {10.1145/2857218.2857225},
  url = {https://dl.acm.org/doi/10.1145/2857218.2857225},
  urldate = {2022-11-08},
  abstract = {The utilization of the Scrum method for software development offers major benefits to its users, such as the process acceleration and resources to deal with the instability of technological environments. Fast customer feedback and support for volatile requirement results in a higher product value, however it hinders the team in dealing appropriately with a critical aspect of every system, which is the information security. Whereas attacks have become more sophisticated that even simpler systems can be potential targets, so it is essential to treat software security within the agile method itself, in order to make it part of the process. Aiming to improve V\textbackslash VWHP¶V  TXDOLW\textbackslash{}   reliability, and security, this work proposes an accessory to the Scrum agile method named ScrumS, which adds specific security techniques for a risk analysis project.},
  eventtitle = {{{MEDES}} '15: {{The}} 7th {{International Conference}} on {{Management}} of Computational and Collective {{IntElligence}} in {{Digital EcoSystems}}},
  isbn = {978-1-4503-3480-8},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\K9IX7QFI\Maria et al. - 2015 - ScrumS a model for safe agile development.pdf}
}

@article{marieskaPerformanceKernelBased2011,
  title = {On Performance of Kernel Based and Embedded {{Real-Time Operating System}}: {{Benchmarking}} and Analysis},
  author = {Marieska, Mastura D and Hariyanto, Paul G and Fauzan, M Firda and Kistijantoro, Achmad Imam and Manaf, Afwarman},
  date = {2011},
  journaltitle = {International Conference on Advanced Computer Science and Information Systems},
  pages = {401--406},
  abstract = {There are many kinds of Real Time Operating System (RTOS) can be found at the present time. This research aims to benchmark and analyze two kinds of RTOS; kernel based and embedded RTOS. Benchmarking information about performance of a RTOS is really important in order to choose appropriate RTOS. We use Xenomai and RT Patch Linux for kernel based RTOS and eCos for embedded RTOS. Benchmarking is done by comparing four performance metrics and assessing performance of a test bed application. The four performance metrics are interrupt latency, task switching time, preemption time, and deadlock break time. Testing result shows that embedded RTOS is appropriate for executing one task application while kernel based RTOS is suitable to execute multitasking applications. Testing result also shows that the network packet processing of embedded RTOS is faster than kernel based RTOS.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\7C77XQ2D\Marieska et al. - 2011 - On performance of kernel based and embedded Real-T.pdf}
}

@online{marksProgrammerGuideText2020,
  title = {Programmer's {{Guide}} to {{Text Blocks}}},
  author = {Marks, Stuart and Laskey, Laskey},
  date = {2020-09-15},
  url = {https://docs.oracle.com/en/java/javase/15/text-blocks/index.html},
  urldate = {2022-05-24},
  organization = {{Programmer's Guide to Text Blocks}}
}

@inproceedings{marquesRequirementsTraceabilityProcess2015,
  title = {Towards a Requirements Traceability Process Centered on the Traceability Model},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Marques, Arthur and Ramalho, Franklin and Andrade, Wilkerson L.},
  date = {2015-04-13},
  series = {{{SAC}} '15},
  pages = {1364--1369},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2695664.2695776},
  url = {https://doi.org/10.1145/2695664.2695776},
  urldate = {2022-10-22},
  abstract = {Requirements traceability (RT) has been acknowledged as a valuable activity in the software development process. Its importance is reflected in different quality standards, which dictates that requirements should be traceable through the software development life-cycle. However, there is not a consensus about the traceability process and, as a consequence, RT practices cannot be unified across different organizational settings. Identifying common aspects to the RT process has been recognized as one of the grand challenges of RT. Even though, researches regarding improvements in requirements engineering (RE) neglect the traceability research scope. Therefore, is this paper, we propose and detail a RT process, specifying its workflow, actors, responsibilities and inputs/outputs as well as establishing contracts that govern the proposed process phases. To evaluate the proposed process, we have conducted an empirical experiment in a real project under development for the federal police of Brazil. We observed time, precision, recall and efficiency metrics and concluded that the proposed process improves the performance of RT practices. As a final remark, the proposed process contributes towards the discussion of common aspects of the RT process.},
  isbn = {978-1-4503-3196-8},
  keywords = {contracts,process,requirements traceability,requirements traceability process},
  file = {C:\Users\ron\Zotero\storage\HDMC4CXQ\Marques et al. - 2015 - Towards a requirements traceability process center.pdf}
}

@inproceedings{maximResamplingStatisticalTiming2012,
  title = {Re-Sampling for Statistical Timing Analysis of Real-Time Systems},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Real-Time}} and {{Network Systems}} - {{RTNS}} '12},
  author = {Maxim, Dorin and Houston, Mike and Santinelli, Luca and Bernat, Guillem and Davis, Robert I. and Cucu-Grosjean, Liliana},
  date = {2012},
  pages = {111},
  publisher = {{ACM Press}},
  location = {{Pont \&\#224; Mousson, France}},
  doi = {10.1145/2392987.2393001},
  url = {http://dl.acm.org/citation.cfm?doid=2392987.2393001},
  urldate = {2022-06-21},
  eventtitle = {The 20th {{International Conference}}},
  isbn = {978-1-4503-1409-1},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\W3B9KWTZ\Maxim et al. - 2012 - Re-sampling for statistical timing analysis of rea.pdf}
}

@book{mccumberAssessingManagingSecurity2005,
  title = {Assessing and {{Managing Security Risk}} in {{IT Systems}}, {{A Structured Methodology}}},
  author = {McCumber, John},
  date = {2005},
  publisher = {{Auerbach Publications}},
  isbn = {0-203-49042-8}
}

@inproceedings{mccumberInformationSystemsSecurity1992,
  title = {Information {{Systems Security}}: {{A Comprehensive Model}}},
  booktitle = {14th {{National Computer Security Conference}}},
  author = {McCumber, John},
  date = {1992-05},
  location = {{Washington, D.C.}},
  eventtitle = {14th {{National Computer Security Conference}}}
}

@article{mellor-crummeyAlgorithmsScalableSynchronization1991,
  title = {Algorithms for {{Scalable Synchronization}} on {{Shared-Memory Multiprocessors}}},
  author = {Mellor-Crummey, John M. and Scott, Michael L.},
  date = {1991-02},
  journaltitle = {ACM Transactions on Computer Systems},
  volume = {9},
  number = {1},
  pages = {21--65},
  doi = {103727.103729},
  url = {https://dl.acm.org/doi/10.1145/103727.103729},
  file = {C:\Users\ron\Zotero\storage\T4PKR8H4\103727.103729.pdf}
}

@inproceedings{mohammadiPatternsIdentificationTrust2016,
  title = {Patterns for Identification of Trust Concerns and Specification of Trustworthiness Requirements},
  booktitle = {Proceedings of the 21st {{European Conference}} on {{Pattern Languages}} of {{Programs}}},
  author = {Mohammadi, Nazila Gol and Heisel, Maritta},
  date = {2016-07-06},
  series = {{{EuroPlop}} '16},
  pages = {1--20},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3011784.3011819},
  url = {https://doi.org/10.1145/3011784.3011819},
  urldate = {2022-10-22},
  abstract = {Software systems enable performing today's complex business processes. Considering trustworthiness requirements in accordance to trust concerns of the end-users during the development of software systems is a critical issue. Particularly challenging are the cross-disciplinary factors that affect trust of the end-users. Furthermore, expertise in domain and requirements analysis are also required. The problem is even more complex considering current advances in technologies like cloud computing, because of the high distribution, huge amounts of data flows, and a large number of involved participants. It is essential for building a trustworthy software system to elicit and analyze trustworthiness requirements. Documenting these trustworthiness requirements along the other requirements that can achieve the construction of a trustworthy system requires eliciting and documenting the trustworthiness-related domain knowledge. Using goal and business process models, gained domain knowledge can be refined into functional or non-functional requirements for software development which contribute to the trustworthiness of the system under development. Our pattern-based approach helps to systematically refine and document the functional as well as non-functional requirements that satisfy the identified trustworthiness goals. In this paper, we develop patterns to aid the documentation of trustworthiness requirements. The provided patterns are used during the requirement engineering phase to support the requirements engineer in documenting trustworthiness requirements by using linguistic templates. Using our patterns, end-users are also supported in expressing their requirements (trust concern identification pattern) and understanding which problems exist and how the problems are addressed (trustworthiness requirement pattern). These patterns yield insights into the relevant trustworthiness requirements that address the corresponding trust concerns. The application of the introduced patterns helps the requirements engineer during the trustworthiness requirement documentation in an unambiguous, understandable, traceable and verifiable way. We illustrate the patterns by applying it on an application example from an ambient assisted living domain.},
  isbn = {978-1-4503-4074-8},
  keywords = {linguistic templates,patterns,requirements engineering,trust,trustworthiness requirements},
  file = {C:\Users\ron\Zotero\storage\M3AQJFXZ\Mohammadi and Heisel - 2016 - Patterns for identification of trust concerns and .pdf}
}

@inproceedings{moketarAutomatedCollaborativeRequirements2016,
  title = {An Automated Collaborative Requirements Engineering Tool for Better Validation of Requirements},
  booktitle = {Proceedings of the 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Moketar, Nor Aiza and Kamalrudin, Massila and Sidek, Safiah and Robinson, Mark and Grundy, John},
  date = {2016-08-25},
  series = {{{ASE}} 2016},
  pages = {864--869},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2970276.2970295},
  url = {https://doi.org/10.1145/2970276.2970295},
  urldate = {2022-10-22},
  abstract = {This demo introduces an automated collaborative requirements engineering tool, called TestMEReq, which is used to promote effective communication and collaboration between client-stakeholders and requirements engineers for better requirements validation. Our tool is augmented with real time communication and collaboration support to allow multiple stakeholders to collaboratively validate the same set of requirements. We have conducted a user study focusing on validating requirements using TestMEReq with a few groups of requirements engineers and client stakeholders. The study shows that our automated tool support is able to assist requirements engineers to effectively communicate with client-stakeholders to better validate the requirements virtually in real time. (Demo video: https://www.youtube.com/watch?v=7sWLOx-N4Jo).},
  isbn = {978-1-4503-3845-5},
  keywords = {Abstract test,communication and collaboration,Essential Use Cases,Essential User Interface,requirement-based testing,requirements validation},
  file = {C:\Users\ron\Zotero\storage\3Z7XYUVU\Moketar et al. - 2016 - An automated collaborative requirements engineerin.pdf}
}

@online{MoreFalsehoodsProgrammers,
  title = {More Falsehoods Programmers Believe about Time; "Wisdom of the Crowd" Edition},
  url = {https://infiniteundo.com/post/25509354022/more-falsehoods-programmers-believe-about-time},
  urldate = {2022-06-16},
  abstract = {A couple of days ago I decided to [write down some of the things I've learned about testing][testing\_post] over the course of the last [several years.][codeascraft] In the course of enumerating the...},
  langid = {american},
  organization = {{Infinite Undo}}
}

@inreference{MorrisWorm2023,
  title = {Morris Worm},
  booktitle = {Wikipedia},
  date = {2023-02-22T07:42:06Z},
  url = {https://en.wikipedia.org/w/index.php?title=Morris_worm&oldid=1140888391},
  urldate = {2023-06-05},
  abstract = {The Morris worm or Internet worm of November 2, 1988, is one of the oldest computer worms distributed via the Internet, and the first to gain significant mainstream media attention. It resulted in the first felony conviction in the US under the 1986 Computer Fraud and Abuse Act. It was written by a graduate student at Cornell University, Robert Tappan Morris, and launched on 8:30 pm November 2, 1988, from the Massachusetts Institute of Technology network.},
  langid = {english},
  annotation = {Page Version ID: 1140888391},
  file = {C:\Users\ron\Zotero\storage\4NCJ9HDR\Morris_worm.html}
}

@inproceedings{muchowInvestigationMigratingProprietary2015,
  title = {An Investigation of Migrating from Proprietary {{RTOS}} to Embedded {{Linux}}},
  booktitle = {Proceedings of the 11th {{International Symposium}} on {{Open Collaboration}}},
  author = {Muchow, Oscar and Ustarbowski, David and Hammouda, Imed},
  date = {2015-08-19},
  series = {{{OpenSym}} '15},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2788993.2789832},
  url = {https://doi.org/10.1145/2788993.2789832},
  urldate = {2022-06-07},
  abstract = {Embedded systems and the open source operating system Linux have been going hand in hand for a long time now. Companies using Linux for their embedded products are praising it for being time and cost efficient when it comes to performance and maintainability. Another solution for embedded systems is a Real-Time Operating System (RTOS). The goal of this this paper is to investigate whether a traditional proprietary RTOS can be substituted with embedded Linux, and if this kind of migration can lead to reduced licensing costs and increased general quality of the system. We used a qualitative research method for this case-study. The investigation was conducted with interviews as the main source of information. The result of this study is an empirical model we named 'Embedded Linux Adoption Model'. We concluded that in many cases a proprietary RTOS can be substituted with embedded Linux without affecting the critical needs of the system. The study also showed that many embedded system developers are very receptive to open source solutions and could think of contributing to the community.},
  isbn = {978-1-4503-3666-6},
  keywords = {adoption,embedded Linux,Linux,migration,RTOS},
  file = {C:\Users\ron\Zotero\storage\7K2K9HUN\2788993.2789832.pdf}
}

@online{muellerWhatDevOps2010,
  title = {What {{Is DevOps}}?},
  author = {Mueller, Ernest},
  date = {2010-08-02T22:01:33+00:00},
  url = {https://theagileadmin.com/what-is-devops/},
  urldate = {2022-06-22},
  abstract = {DevOps is a term for a group of concepts that, while not all new, have catalyzed into a movement and are rapidly spreading throughout the technical community.~ Like any new and popular term, people…},
  langid = {english},
  organization = {{the agile admin}},
  file = {C:\Users\ron\Zotero\storage\V3FYX962\what-is-devops.html}
}

@incollection{müller12HeadDrivenPhrase2019,
  title = {12. {{Head-Driven Phrase Structure Grammar}}},
  booktitle = {Current {{Approaches}} to {{Syntax}}},
  author = {Müller, Stefan and Machicao Y Priemer, Antonio},
  editor = {Kertész, András and Moravcsik, Edith and Rákosi, Csilla},
  date = {2019-05-06},
  pages = {317--360},
  publisher = {{De Gruyter}},
  doi = {10.1515/9783110540253-012},
  url = {https://www.degruyter.com/document/doi/10.1515/9783110540253-012/html},
  urldate = {2023-10-23},
  isbn = {978-3-11-054025-3}
}

@article{mullerDesignRequirementsIterative2017,
  title = {Design {{Requirements Iterative Process}} ({{DRIP}}) {{Tool Demonstration Concurrent Engineering}} of {{Design}}, {{Requirements}} and {{Knowledge}}},
  author = {Müller, Josef and Narasimhan, Prashanth Lakshmi and Gopalswamy, Swaminathan},
  date = {2017-05-10},
  journaltitle = {ACM SIGAda Ada Letters},
  shortjournal = {Ada Lett.},
  volume = {36},
  number = {2},
  pages = {60--68},
  issn = {1094-3641},
  doi = {10.1145/3092893.3092905},
  url = {https://doi.org/10.1145/3092893.3092905},
  urldate = {2022-10-22},
  abstract = {Requirements definition and design decisions are highly coupled for mechatronic systems, and heavily influenced by prior knowledge. Upfront engineering of requirements and design is often addressed by inefficient, ad-hoc iterative methods. We propose a methodology to perform concurrent engineering of high level requirements and design along with prior knowledge by using a "common constraint framework" to describe the requirements, design and knowledge precisely. Then an upfront symbolic simultaneous analysis of all the constraints allows us to identify infeasibilities. Next we define a design architecture that can be used to extend the above constraint framework to include temporal aspects leading to an ability to define low level requirements and test scenarios under which these requirements can be verified. Importantly the low level requirements and test scenarios can be specified independent of the final implementation. These provide the critical link between upfront requirements engineering process and downstream implementation verification. Finally we define "mappings" between implementations and the design architecture that allows definitions of executable tests in the implementation environment that in turn can be used to verify the low level requirements. We demonstrate the above methodologies using the tool DRIP.},
  keywords = {architecture,concurrent engineering of requirements and design with knowledge,constraint analysis framework,Domain Specific Language,infeasibility analysis,iterative engineering of requirements and design with knowledge,Language Work Bench,Projectional Editor,simulation based verification},
  file = {C:\Users\ron\Zotero\storage\22WARDZZ\Müller et al. - 2017 - Design Requirements Iterative Process (DRIP) Tool .pdf}
}

@book{müllerHeadDrivenPhraseStructure2021,
  title = {Head-{{Driven Phrase Structure Grammar}}: {{The}} Handbook ({{Volume}} 9)},
  shorttitle = {Head-{{Driven Phrase Structure Grammar}}},
  editor = {Müller, Stefan and Abeillé, Anne and Borsley, Robert D. and Koenig, Jean-Pierre},
  date = {2021},
  publisher = {{Language Science Press}},
  doi = {10.5281/zenodo.5543318},
  url = {https://library.oapen.org/handle/20.500.12657/52594},
  urldate = {2023-10-23},
  abstract = {Head-Driven Phrase Structure Grammar (HPSG) is a constraint-based or declarative approach to linguistic knowledge, which analyses all descriptive levels (phonology, morphology, syntax, semantics, pragmatics) with feature value pairs, structure sharing, and relational constraints. In syntax it assumes that expressions have a single relatively simple constituent structure. This volume provides a state-of-the-art introduction to the framework. Various chapters discuss basic assumptions and formal foundations, describe the evolution of the framework, and go into the details of the main syntactic phenomena. Further chapters are devoted to non-syntactic levels of description. The book also considers related fields and research areas (gesture, sign languages, computational linguistics) and includes chapters comparing HPSG with other frameworks (Lexical Functional Grammar, Categorial Grammar, Construction Grammar, Dependency Grammar, and Minimalism).},
  langid = {english},
  keywords = {bic Book Industry Communication::C Language::CF linguistics,Language Arts \& Disciplines,Linguistics},
  file = {C:\Users\ron\Zotero\storage\2R6V6VRW\external_content.pdf}
}

@inproceedings{najjarCapabilityMaturityModel2015,
  title = {Capability {{Maturity Model}} of {{Software Requirements Process}} and {{Integration}} ({{SRPCMMI}})},
  booktitle = {Proceedings of the {{International Conference}} on {{Intelligent Information Processing}}, {{Security}} and {{Advanced Communication}}},
  author = {Najjar, Sireen Kamal and Al-Sarayreh, Khalid T.},
  date = {2015-11-23},
  series = {{{IPAC}} '15},
  pages = {1--5},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2816839.2816856},
  url = {https://doi.org/10.1145/2816839.2816856},
  urldate = {2022-06-09},
  abstract = {Software requirement engineering (RE) process is one of the most important phases of the software development life cycle (SDLC) that affect its overall success, since it has a significant role in determining the software quality and software development process effectiveness, due to the increased consideration for Software requirement engineering process and process improvements at the SDLC life cycle. Several standards and RE improvements model were constructed on the aim at helping organizations in improving and assessing their RE process, however they suffer from several problems that limits their acceptance by the organization that are interest in the RE process improvement. The paper proposed a new requirement engineering process capability maturity model based on the capability maturity model integration for development (CMMI-DEV). The intention is to provide a generic maturity model that based on international standards and literature on software RE to help developing the area of RE process improvement.},
  isbn = {978-1-4503-3458-7},
  keywords = {CMMI,Maturity model,Process improvement,Requirement engineering},
  file = {C:\Users\ron\Zotero\storage\2A4UEHEG\2816839.2816856.pdf}
}

@inproceedings{najjarCapabilityMaturityModel2015a,
  title = {Capability {{Maturity Model}} of {{Software Requirements Process}} and {{Integration}} ({{SRPCMMI}})},
  booktitle = {Proceedings of the {{International Conference}} on {{Intelligent Information Processing}}, {{Security}} and {{Advanced Communication}}},
  author = {Najjar, Sireen Kamal and Al-Sarayreh, Khalid T.},
  date = {2015-11-23},
  series = {{{IPAC}} '15},
  pages = {1--5},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2816839.2816856},
  url = {https://doi.org/10.1145/2816839.2816856},
  urldate = {2022-10-22},
  abstract = {Software requirement engineering (RE) process is one of the most important phases of the software development life cycle (SDLC) that affect its overall success, since it has a significant role in determining the software quality and software development process effectiveness, due to the increased consideration for Software requirement engineering process and process improvements at the SDLC life cycle. Several standards and RE improvements model were constructed on the aim at helping organizations in improving and assessing their RE process, however they suffer from several problems that limits their acceptance by the organization that are interest in the RE process improvement. The paper proposed a new requirement engineering process capability maturity model based on the capability maturity model integration for development (CMMI-DEV). The intention is to provide a generic maturity model that based on international standards and literature on software RE to help developing the area of RE process improvement.},
  isbn = {978-1-4503-3458-7},
  keywords = {CMMI,Maturity model,Process improvement,Requirement engineering},
  file = {C:\Users\ron\Zotero\storage\QUDLVHUZ\Najjar and Al-Sarayreh - 2015 - Capability Maturity Model of Software Requirements.pdf}
}

@inproceedings{nakajimaCompositionKernelMulticore2010,
  title = {Composition {{Kernel}}: {{A Multi-core Processor Virtualization Layer}} for {{Highly Functional Embedded Systems}}},
  shorttitle = {Composition {{Kernel}}},
  booktitle = {2010 {{IEEE}} 16th {{Pacific Rim International Symposium}} on {{Dependable Computing}}},
  author = {Nakajima, Tatsuo and Kinebuchi, Yuki and Courbot, Alexandre and Shimada, Hiromasa and Lin, Tsung-Han and Mitake, Hitoshi},
  date = {2010-12},
  pages = {223--224},
  doi = {10.1109/PRDC.2010.11},
  abstract = {Multi-core processors are being increasingly adopted for embedded systems because they improve performance, power consumption and lower development cost. Composing multiple operating systems on a multi-core processor enhances the reusability of software when developing rich functional embedded systems. Multiple OS environments enable the product to use two versions of an operating system at the same time. In order to build multiple OS environments, a virtualization layer specialized for embedded systems is necessary, since most of processors for embedded systems support only two protection levels, and there is no hardware support for virtualization. In traditional approaches, an OS kernel runs at the user level to isolate the respective OS kernels to increase reliability, but this approach requires heavy modifications to the guest OSes if there is no proper hardware virtualization support that is rarely used in embedded systems. Therefore existing solutions are not preferred by the embedded system industry. In this paper, we propose a composition kernel where multiple OS kernels are running on top of a very thin hardware abstraction layer. A composition kernel can reduce the engineering cost of developing an embedded system by reusing existing OS kernels and application with minimum modification without assuming special hardware supports.},
  eventtitle = {2010 {{IEEE}} 16th {{Pacific Rim International Symposium}} on {{Dependable Computing}}},
  keywords = {Embedded systems,Embedded Systems,Kernel,Linux,Monitoring,Multi-core processor,Multicore processing,Program processors,Virtualization},
  file = {C:\Users\ron\Zotero\storage\2UZTI4TI\Composition_Kernel_A_Multi-core_Processor_Virtualization_Layer_for_Highly_Functional_Embedded_Systems.pdf}
}

@inproceedings{nakashimaDesignImplementationInterrupt2002,
  title = {Design and Implementation of Interrupt Packaging Mechanism},
  booktitle = {International {{Workshop}} on {{Innovative Architecture}} for {{Future Generation High-Performance Processors}} and {{Systems}}},
  author = {Nakashima, K. and Kusakabe, S. and Taniguchi, H. and Amamiya, M.},
  date = {2002-01},
  pages = {95--102},
  issn = {1537-3223},
  doi = {10.1109/IWIA.2002.1035023},
  abstract = {As the amount of data transferred between the main processing unit and peripheral devices increases, the frequency of interrupts from peripheral devices also increases. Thus, the efficiency of interrupt handling is one of the key issues that must be addressed to realize high performance computing environments. In conventional interrupt mechanisms, an interrupt handler consists of three parts: a pre-processing routine, a main-handler routine and a postprocessing routine. During a pre-processing routine, all register values are pushed to a stack, and values of the interrupt controller are changed so that the main-handler routine can execute with registers in a given interrupt priority. During a post-processing routine, values of interrupt controller are restored, and all register values are popped from the stack. The more interrupts occur, the more preprocessing and post-processing routine overhead must be tolerated In order to reduce interrupt overhead, we propose an interrupt packaging mechanism that packages main handlers of a series of interrupts and reduces the overhead of pre/post-processing. We have designed and implemented the interrupt packaging mechanism for interrupts from a Myrinet NIC (network interface card). In our evaluation, we have improved system performance by 6.07\%.},
  eventtitle = {International {{Workshop}} on {{Innovative Architecture}} for {{Future Generation High-Performance Processors}} and {{Systems}}},
  keywords = {Computer integrated manufacturing,Frequency,High performance computing,Information science,Multiprocessor interconnection networks,Network interfaces,Operating systems,Packaging,Registers,System performance},
  file = {C:\Users\ron\Zotero\storage\3KTHG7DA\Design_and_implementation_of_interrupt_packaging_mechanism.pdf}
}

@online{NaPiRE,
  title = {{{NaPiRE}}},
  url = {http://www.re-survey.org/#/home},
  urldate = {2022-10-27},
  file = {C:\Users\ron\Zotero\storage\HLUB2TNP\www.re-survey.org.html}
}

@report{nethercoteDynamicBinaryAnalysis2004,
  title = {Dynamic Binary Analysis and Instrumentation},
  author = {Nethercote, Nicholas},
  date = {2004},
  number = {UCAM-CL-TR-606},
  institution = {{University of Cambridge, Computer Laboratory}},
  url = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-606.html},
  urldate = {2022-06-21},
  langid = {english},
  file = {C\:\\Users\\ron\\Zotero\\storage\\KI4AKD5C\\Nethercote - 2004 - Dynamic binary analysis and instrumentation.pdf;C\:\\Users\\ron\\Zotero\\storage\\9DJXDGPF\\UCAM-CL-TR-606.html}
}

@book{newmarkTextbookTranslation1988,
  title = {A Textbook of Translation},
  author = {Newmark, Peter},
  date = {1988},
  publisher = {{Prentice-Hall International}},
  location = {{New York}},
  isbn = {978-0-13-912593-5},
  pagetotal = {292},
  keywords = {Translating and interpreting}
}

@inproceedings{nguyentranHazardAnalysisMethods2022,
  title = {Hazard {{Analysis Methods}} for {{Software Safety Requirements Engineering}}},
  booktitle = {2022 {{The}} 5th {{International Conference}} on {{Software Engineering}} and {{Information Management}} ({{ICSIM}})},
  author = {Nguyen Tran, Vu and Vu Tran, Long and Nguyen Tran, Viet and Ngoc Vu, Dao},
  date = {2022-01-21},
  series = {{{ICSIM}} 2022},
  pages = {11--18},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3520084.3520087},
  url = {https://doi.org/10.1145/3520084.3520087},
  urldate = {2022-10-22},
  abstract = {The rise of software-based system control in safety-critical systems has made software safety a critical part of a system safety program. The risk of catastrophic software system failure increases with the growth of safety-critical technologies in autonomous transportation systems, airplanes, traffic control systems, medical surgery equipment, nuclear power centers, power grids, human-assist robotics, and military weaponry. Developing software control in safety-critical systems is challenging because the control needs to be reliable and safe. High-profile system failures in recent years, such as the crashes of the 737 MAX airliners, are constant reminders of the risk of software failure in safety-critical systems. The software quality assurance approaches used in software development today are insufficient for created for assuring software reliability but not safety. Developing functionally safe software requires incorporating a risk-driven approach that focuses on hazard identification, hazard risk anticipation, and mitigation. Software safety methods adoption in practice and across mainstream computer science and software engineering curriculums is still limited. Heeding the call for more publications on the practice of software safety, we present an integrated approach to software safety requirements engineering (SSRE). Engineering safety requirements for software is one of the most important steps in building safe software. First, we provide an overview of SSRE. Then we describe three hazard analysis methods that can be incorporated into a software requirement engineering process. Finally, discuss how we combine these distinct methods into a single SSRE approach to support safety-critical systems development.},
  isbn = {978-1-4503-9551-9},
  keywords = {safety requirements engineering,software hazard analysis,software hazards,Software safety},
  file = {C:\Users\ron\Zotero\storage\SFKLLKEY\Nguyen Tran et al. - 2022 - Hazard Analysis Methods for Software Safety Requir.pdf}
}

@thesis{nicholsComputertoComputerBasedCommunication2022,
  type = {Master of Science},
  title = {Computer-to-{{Computer Based Communication Through Natural Language}}},
  author = {Nichols, Preston},
  date = {2022-12-21},
  institution = {{State University of New York Polytechnic Institute}},
  location = {{Utica, New York}},
  url = {http://hdl.handle.net/20.500.12648/8608},
  abstract = {While highly accurate and efficient computer-to-computer communication exists, the exploration of communication of computer models via natural language is still worth exploring. This paper explores the creation of a “Speaker” model which preforms image-to-text  operations and a “Listener” model which does the reverse text-to-image task. These models can be used together to form the basis of computer natural language communication not only in existing languages such as English but in completely new generated languages as well with the help of the “Rambler” model which combines the Speaker and Listener to preform the entire image-to-text-to-image process. By comparing the image on both sides of the process, how effective the communication is can be measured. While natural language-based computer communications will likely never be common place, it nonetheless poses some interesting and unique challenges.},
  pagetotal = {36},
  file = {C:\Users\ron\Downloads\FINAL_Preston_Nichols_Masters_Project.pdf}
}

@inproceedings{nistalaProcessPatternsRequirement2016,
  title = {Process Patterns for Requirement Consistency Analysis},
  booktitle = {Proceedings of the 21st {{European Conference}} on {{Pattern Languages}} of {{Programs}}},
  author = {Nistala, Padmalata V and Nori, Kesav V and Natarajan, Swaminathan},
  date = {2016-07-06},
  series = {{{EuroPlop}} '16},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3011784.3011809},
  url = {https://doi.org/10.1145/3011784.3011809},
  urldate = {2022-10-22},
  abstract = {In the requirement space, patterns are gaining prominence to capture the requirement knowledge for reuse and help identify requirements. The quality of requirement specification is critical for effective understanding and implementation of requirements. This paper presents a set of process patterns that use the concept of compositional traceability to analyze how well the requirement specifications or user stories have been formulated and to identify inconsistencies among its encompassing elements. We present two patterns that have been successfully applied and found useful to carry out consistency analysis on requirements and detect inconsistencies in requirements: Requirements coverage analysis and Requirements traceability analysis. These patterns can be applied to projects during requirements phase for review and analysis of stated requirements. The paper describes the patterns, discusses its implementation and results from a project case study.},
  isbn = {978-1-4503-4074-8},
  keywords = {process pattern,requirements analysis,requirements consistency,requirements coverage,requirements pattern,requirements quality,requirements traceability},
  file = {C:\Users\ron\Zotero\storage\GTH5N6KS\Nistala et al. - 2016 - Process patterns for requirement consistency analy.pdf}
}

@inproceedings{niuGrayLinksUse2016,
  title = {Gray Links in the Use of Requirements Traceability},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Niu, Nan and Wang, Wentao and Gupta, Arushi},
  date = {2016-11-01},
  series = {{{FSE}} 2016},
  pages = {384--395},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2950290.2950354},
  url = {https://doi.org/10.1145/2950290.2950354},
  urldate = {2022-10-22},
  abstract = {The value of traceability is in its use. How do different software engineering tasks affect the tracing of the same requirement? In this paper, we answer the question via an empirical study where we explicitly assign the participants into 3 trace-usage groups of one requirement: finding its implementation for verification and validation purpose, changing it within the original software system, and reusing it toward another application. The results uncover what we call "gray links"--around 20\% of the total traces are voted to be true links with respect to only one task but not the others. We provide a mechanism to identify such gray links and discuss how they can be leveraged to advance the research and practice of value-based requirements traceability.},
  isbn = {978-1-4503-4218-6},
  keywords = {gray links,requirements change,requirements reuse,software engineering task,Traceability,using traceability},
  file = {C:\Users\ron\Zotero\storage\P4JYI92J\Niu et al. - 2016 - Gray links in the use of requirements traceability.pdf}
}

@inproceedings{novikovDEVELOPINGEDUCATIONALSOFTWARE2020,
  title = {{{DEVELOPING EDUCATIONAL SOFTWARE TO RAISE LINGUISTIC AWARENESS}} ({{A CASE STUDY OF CREATING AND PROMOTING SOLRESOL TRANSLATOR ONLINE}})},
  author = {Novikov, Philipp and Kiseleva, Alexandra},
  date = {2020-03},
  pages = {6840--6844},
  location = {{Valencia, Spain}},
  doi = {10.21125/inted.2020.1821},
  url = {http://library.iated.org/view/NOVIKOV2020DEV},
  urldate = {2023-10-03},
  abstract = {In this research the authors share the experience of developing and promoting self-published educational software named Solresol: The Project, which is a non-commercial program that can be used for translation from English into the constructed language named Solresol and back. Solresol is a constructed language that predates the more popular a priori and a posteriori constructed languages designed for international communication such as Esperanto and Lojban. Despite the fact that Solresol is one of the lesser-known constructed languages, it still attracts attention of those who are interested in either linguistics or arts thanks to its simplicity and a wide variety of methods of expression, ranging from letters and numbers to colors and musical notes. The aim of the project implemented by the authors is to develop software that popularizes Solresol and makes the language more accessible to the general public. The software is powered by the cutting-edge technology of Unreal Engine 4 which is typically used in video games and visual design, which helps to put the emphasis on the aesthetics and makes it possible to implement the ideas of Francois Sudre, the creator of Solresol. This non-commercial software project started in 2014 and is still developing. Its creation helped to raise linguistic awareness, as evidenced by Google Analytics and other metrics. The use of such websites as Facebook, reddit, YouTube, etc. allowed us to accumulate more than 10 000 unique visitors interested in the project and made it one of the most popular resources on SolReSol over a short period of time, as evidenced by Google rankings. Moreover, it has been possible to build a network of those who are interested in Solresol, including artists, academics and constructed language enthusiasts. The experience shared by the authors of this research may be used as a blueprint for the academics who are willing to popularize their ideas and reach a significantly wider audience using the means of social media and search engine optimization.},
  eventtitle = {14th {{International Technology}}, {{Education}} and {{Development Conference}}}
}

@inproceedings{nurbojatmikoSlrIdentificationClassification2018,
  title = {Slr on {{Identification}} \& {{Classification}} of {{Non-Functional Requirements Attributes}}, and {{Its Representation}} in {{Functional Requirements}}},
  booktitle = {Proceedings of the 2018 2nd {{International Conference}} on {{Computer Science}} and {{Artificial Intelligence}}},
  author = {Nurbojatmiko and Budiardjo, Eko K. and Wibowo, Wahyu C.},
  date = {2018-12-08},
  series = {{{CSAI}} '18},
  pages = {151--157},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3297156.3297200},
  url = {https://doi.org/10.1145/3297156.3297200},
  urldate = {2022-10-22},
  abstract = {Software requirements are crucial. Tight relationship between functional and non-functional will determine the quality of requirements, in which ultimately determines the quality of the software being built. Non-Functional Requirements (NFR) often get less attention in the Requirements Engineering (RE) process. In NFR determination, it is necessary to know the attributes and determination criteria that will make easier to determine the relation to FR (Functional Requirements) related. Nowadays many software developers use the Agile Methodology, where the determination of NFR by means of interviews with its stakeholders. Considering the importance of NFR attributes, then the NFR attributes mapping, how to obtain NFR attributes, and how to represent them closely with FR. The research method uses the Prisma Framework. The results of this research determine the NFR attributes domains, identification of NFR attributes, classification of NFR attributes, NFR representation between FR and NFR attributes, are used to obtain application features.},
  isbn = {978-1-4503-6606-9},
  keywords = {Functional Requirements (FR),NFR attributes,Non-Functional Requirements (NFR),Requirements Engineering (RE)},
  file = {C:\Users\ron\Zotero\storage\52NZ6MT9\Nurbojatmiko et al. - 2018 - Slr on Identification & Classification of Non-Func.pdf}
}

@inbook{okrentBuddingLinguistsHow2020,
  title = {Budding Linguists and How to Find Them},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Okrent, Arika},
  date = {2020-08-12},
  pages = {27--31},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0003},
  url = {https://academic.oup.com/book/31973/chapter/267716978},
  urldate = {2023-08-24},
  abstract = {Even though people are rarely explicitly exposed to the basic ideas of linguistics in school, there is a certain type of kid who gravitates toward them regardless. That kid is a natural linguist, and there are a lot of them out there. Many linguists describe a common experience of finding excitement in the idea of language as a structure, and their discovery of that idea sometimes came through contact with fictional languages or through tinkering with their own creations. Language invention can play a role in guiding natural linguists to linguistics, and may be able to help better establish in our culture a linguistics-oriented love of language.},
  bookauthor = {Okrent, Arika},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@inproceedings{oliveiraantoninoParameterizedSafetyRequirements2015,
  title = {The Parameterized Safety Requirements Templates},
  booktitle = {Proceedings of the 8th {{International Symposium}} on {{Software}} and {{Systems Traceability}}},
  author = {Oliveira Antonino, Pablo and Trapp, Mario and Barbosa, Paulo and Sousa, Luana},
  date = {2015-05-16},
  series = {{{SST}} '15},
  pages = {29--35},
  publisher = {{IEEE Press}},
  location = {{Florence, Italy}},
  abstract = {Despite imposing strict recommendations to be considered during the specification of safety requirements, standards and regulations do not provide guidance to be used throughout the creation of these artifacts. In practice, each safety requirement specification has heterogeneous structures, usually based on the experience of the engineers involved in the specification process. Consequently, it becomes difficult to ensure that the standards' recommendations were considered, such as the existence of evidences that the requirements are properly traceable to other development artifacts such as architecture and failure propagation models. To address this challenge, we defined the Parameterized Safety Requirements Templates, which is a controlled natural language based approach to support engineers in elaborating the content description of safety requirements specifications, ensuring that elements of the architectural design and of the failure propagation models are explicitly considered throughout the textual description of the safety requirements, and are therefore properly traced. The Parameterized Safety Requirements Templates have been used in different domains such as automotive, avionics, and medical devices, and have proven to be effective in improving artifact traceability. In this paper, we present their usage in the context of an industrial Automated External Defibrillator system.},
  keywords = {architecture,failure propagation model,safety requirement,traceability},
  file = {C:\Users\ron\Zotero\storage\MWBD8KL4\Oliveira Antonino et al. - 2015 - The parameterized safety requirements templates.pdf}
}

@inproceedings{oliveiraProtectingKernelCode2009,
  title = {Protecting {{Kernel Code}} and {{Data}} with a {{Virtualization-Aware Collaborative Operating System}}},
  booktitle = {2009 {{Annual Computer Security Applications Conference}}},
  author = {de Oliveira, Daniela Alvim Seabra and Wu, S. Felix},
  date = {2009-12},
  pages = {451--460},
  issn = {1063-9527},
  doi = {10.1109/ACSAC.2009.49},
  abstract = {The traditional virtual machine usage model advocates placing security mechanisms in a trusted VM layer and letting the untrusted guest OS run unaware of the presence of virtualization. In this work we challenge this traditional model and propose a collaboration approach between a virtualization-aware operating system and a VM layer to prevent tampering against kernel code and data. Our integrity model is a relaxed version of Biba's and the main idea is to have all attempted writes into kernel code and data segments checked for validity at VM level. The OS-VM collaboration bridges the semantic gap between tracing low integrity objects at OS-level (files, processes, modules, allocated areas) and architecture-level (memory and registers). We have implemented this approach in a proof-of-concept prototype and have successfully tested it against 6 rootkits (including a non-control data attack) and 4 real-world benign LKM/drivers. All rootkits were prevented from corrupting kernel space and no false positive was triggered for benign modules. Performance measurements show that the average overhead to the VM for the OS-VM communication is low (7\%, CPU benchmarks). The greatest overhead is caused by the memory monitoring module inside the VM: 1.38X alone and 1.46X when combined with the OS-VM communication. For OS microbenchmarks the slowdown for the OS-VM communication was 1.16X on average.},
  eventtitle = {2009 {{Annual Computer Security Applications Conference}}},
  keywords = {Bridges,collaboration,Collaboration,Collaborative work,Data security,integrity,Kernel,kernel code and data,operating system,Operating systems,Protection,Registers,rootkits,security,virtual machine,Virtual machining,Virtual manufacturing},
  file = {C:\Users\ron\Zotero\storage\QMZCXZ83\Protecting_Kernel_Code_and_Data_with_a_Virtualization-Aware_Collaborative_Operating_System.pdf}
}

@inproceedings{onumoEmpiricalStudyCultural2017,
  title = {An {{Empirical Study}} of {{Cultural Dimensions}} and {{Cybersecurity Development}}},
  booktitle = {2017 {{IEEE}} 5th {{International Conference}} on {{Future Internet}} of {{Things}} and {{Cloud}} ({{FiCloud}})},
  author = {Onumo, Aristotle and Cullen, Andrea and Ullah-Awan, Irfan},
  date = {2017-08},
  pages = {70--76},
  doi = {10.1109/FiCloud.2017.41},
  abstract = {The purpose of the present study is to empirically investigate whether national culture has an impact on cybersecurity development. We used methods of correlation and hierarchical regression to analyse two sets of indices; the global cybersecurity index of 2015 and Hofstede cultural dimension index. The research discovered that there exist a significant correlation between cybersecurity development and the cultural dimensions as defined by Hofstede cultural theory. Five cultural dimensions were used in the study; power distance, masculinity/femininity, individualism/collectivism, uncertainty avoidance, long term/short term orientation, and the research found out that individualism and long term orientation were significantly correlated with cybersecurity development. These findings have strategic implications in helping government and decision makers fashion out relevant policies and programmes while taking into cognisance the cultural factors in the improvement of the cyberwellness profile and the development of strategic cybersecurity. Implications and recommendations for future work are further discussed.},
  eventtitle = {2017 {{IEEE}} 5th {{International Conference}} on {{Future Internet}} of {{Things}} and {{Cloud}} ({{FiCloud}})},
  keywords = {Computer security,Correlation,Cultural differences,Cybersecurity policies,Electronic government,Indexes,ITU,National Culture,Strategic Cybersecurity,Uncertainty},
  file = {C\:\\Users\\ron\\Zotero\\storage\\NV9QKRQT\\Onumo et al. - 2017 - An Empirical Study of Cultural Dimensions and Cybe.pdf;C\:\\Users\\ron\\Zotero\\storage\\SFXN2YH5\\8114465.html}
}

@incollection{ostromPrivateCommonProperty2007,
  title = {Private and {{Common Property Rights}}.},
  booktitle = {Private and {{Common Property Rights}}.},
  author = {Ostrom, E and Hess, C},
  editor = {Bouckaert, B},
  date = {2007},
  publisher = {{Ostrom}},
  location = {{Northampton, MA}}
}

@article{osullivanMakingSenseRevisioncontrol2009,
  title = {Making {{Sense}} of {{Revision-control Systems}}: {{Whether}} Distributed or Centralized, All Revision-Control Systems Come with Complicated Sets of Tradeoffs. {{How}} Do You Find the Best Match between Tool and Team?},
  shorttitle = {Making {{Sense}} of {{Revision-control Systems}}},
  author = {O'Sullivan, Bryan},
  date = {2009-08},
  journaltitle = {Queue},
  shortjournal = {Queue},
  volume = {7},
  number = {7},
  pages = {30--40},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/1594204.1595636},
  url = {https://dl.acm.org/doi/10.1145/1594204.1595636},
  urldate = {2023-04-25},
  abstract = {Modern software is tremendously complicated, and the methods that teams use to manage its development reflect this complexity. Though many organizations use revision-control software to track and manage the complexity of a project as it evolves, the topic of how to make an informed choice of revision-control tools has received scant attention. Until fairly recently, the world of revision control was moribund, so there was simply not much to say on this subject.},
  langid = {english}
}

@inproceedings{pakdeetrakulwongOntologyBasedMultiAgentSystem2015,
  title = {An {{Ontology-Based Multi-Agent System}} to {{Support Requirements Traceability}} in {{Multi-Site Software Development Environment}}},
  booktitle = {Proceedings of the {{ASWEC}} 2015 24th {{Australasian Software Engineering Conference}}},
  author = {Pakdeetrakulwong, Udsanee and Wongthongtham, Pornpit and Khan, Naveed},
  date = {2015-09-28},
  series = {{{ASWEC}} ' 15 {{Vol}}. {{II}}},
  pages = {96--100},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2811681.2811700},
  url = {https://doi.org/10.1145/2811681.2811700},
  urldate = {2022-10-22},
  abstract = {With the advent of the Internet, many software companies have adopted the multi-site software development approach that enables project team members to work across multiple sites. A globally dispersed project offers several advantages; however, it creates additional challenges in regard to communication, coordination and information sharing and can lead to the failure of software projects. We propose a conceptual architecture for an ontology-based multi-agent system to provide active support to access software engineering domain knowledge and to recommend software development project captured in the Software Engineering Ontology. The Software Engineering Ontology is used as a communication framework to enable knowledge sharing and reuse while the multi-agent provides the autonomy and the reactive and proactive features to the multi-site software development environment. In this paper we focus on utilizing the proposed framework to support automated requirements traceability tasks. When there is a change in requirement, software agents work cooperatively to trace and identify potentially affected software artifacts and notify the relevant team members to make them aware of any changes.},
  isbn = {978-1-4503-3796-0},
  keywords = {multi-agent system,multi-site software development,Requirements traceability,software engineering ontology},
  file = {C:\Users\ron\Zotero\storage\FSU3MT9G\Pakdeetrakulwong et al. - 2015 - An Ontology-Based Multi-Agent System to Support Re.pdf}
}

@article{palupiTECHNIQUESTRASNSLATIONUSES2021,
  title = {{{THE TECHNIQUES OF TRASNSLATION AND USES THE V DIAGRAM SYSTEM BY NEWMARK}}},
  author = {Palupi, Muji Endah},
  date = {2021-03},
  journaltitle = {Journal of English Language and literature},
  volume = {6},
  number = {1},
  pages = {81--94},
  doi = {10.37110/jell.v6i1.118},
  url = {https://pdfs.semanticscholar.org/97b7/9191e1b3255a02ad520f977db503b6b24a63.pdf},
  file = {C:\Users\ron\Zotero\storage\5PDF5E9X\9191e1b3255a02ad520f977db503b6b24a63.pdf}
}

@inproceedings{panEasyModellingVerification2019,
  title = {Easy {{Modelling}} and {{Verification}} of {{Unpredictable}} and {{Preemptive Interrupt-Driven Systems}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Pan, Minxue and Chen, Shouyu and Pei, Yu and Zhang, Tian and Li, Xuandong},
  date = {2019-05},
  pages = {212--222},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICSE.2019.00037},
  url = {https://ieeexplore.ieee.org/document/8812085/},
  urldate = {2022-06-11},
  abstract = {The widespread real-time and embedded systems are mostly interrupt-driven because their heavy interaction with the environment is often initiated by interrupts. With the interrupt arrival being unpredictable and the interrupt handling being preemptive, a large number of possible system behaviours are generated, which makes the correctness assurance of such systems difficult and costly. Model checking is considered to be one of the effective methods for exhausting behavioural state space for correctness. However, existing modelling approaches for interrupt-driven systems are based on either calculus or automata theory, and have a steep learning curve. To address this problem, we propose a new modelling language called interrupt sequence diagram (ISD). By extending the popular UML sequence diagram notations, the ISD supports the modelling of interrupts’ essential features visually and concisely. We also propose an automatabased semantics for ISD, based on which ISD can be transformed to a subset of hybrid automata so as to leverage the abundant off-the-shelf checkers. Experiments on examples from both realworld and existing literature were conducted, and the results demonstrate our approach’s usability and effectiveness.},
  eventtitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  isbn = {978-1-72810-869-8},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\KD8ZS44P\Pan et al. - 2019 - Easy Modelling and Verification of Unpredictable a.pdf}
}

@inproceedings{pannittoCALaMoConstructionistAssessment2023,
  title = {{{CALaMo}}: A {{Constructionist Assessment}} of {{Language Models}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Construction Grammars}} and {{NLP}} ({{CxGs}}+{{NLP}}, {{GURT}}/{{SyntaxFest}} 2023)},
  author = {Pannitto, Ludovica and Herbelot, Aurélie},
  date = {2023-03},
  pages = {21--30},
  publisher = {{Association for Computational Linguistics}},
  location = {{Washington DC}},
  url = {https://aclanthology.org/2023.cxgsnlp-1.3},
  abstract = {This paper presents a novel framework for evaluating Neural Language Models\{'\} linguistic abilities using a constructionist approach. Not only is the usage-based model in line with the underlying stochastic philosophy of neural architectures, but it also allows the linguist to keep meaning as a determinant factor in the analysis. We outline the framework and present two possible scenarios for its application.},
  eventtitle = {The {{First International Workshop}} on {{Construction Grammars}} and {{NLP}} ({{CxGs}}+{{NLP}}, {{GURT}}/{{SyntaxFest}} 2023)},
  file = {C\:\\Users\\ron\\Zotero\\storage\\CK3JCFBJ\\2023.cxgsnlp-1.3.pdf;C\:\\Users\\ron\\Zotero\\storage\\U2Z9IH3W\\2023.cxgsnlp-1.3.mov}
}

@inbook{pearsonUsingLanguageInvention2020,
  title = {Using Language Invention to Teach Typology and Cross-Linguistic Universals},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Pearson, Matt},
  date = {2020-08-12},
  pages = {86--106},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0007},
  url = {https://academic.oup.com/book/31973/chapter/267717327},
  urldate = {2023-08-24},
  abstract = {This chapter outlines a group project where students learn about language typology by creating a naturalistic constructed language. Students learn about cross-linguistic variation in natural languages (in areas such as phoneme inventory, word order, and case alignment), and then determine which grammatical properties their invented language will have. Decisions are made at random by spinning a wheel. Attached to the wheel is a pie chart, where the size of each slice represents the percentage of the world’s languages possessing a given setting for some structural parameter or combination of parameters. Crucially, each decision constrains subsequent decisions in accordance with known implicational universals. For instance, in determining whether the language has prepositions or postpositions, the pie chart is adjusted based on the order of verb and object in the language, as decided by a previous spin of the wheel.},
  bookauthor = {Pearson, Matt},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@inproceedings{peixotoSpecifyingPrivacyRequirements2018,
  title = {Specifying Privacy Requirements with Goal-Oriented Modeling Languages},
  booktitle = {Proceedings of the {{XXXII Brazilian Symposium}} on {{Software Engineering}}},
  author = {Peixoto, Mariana Maia and Silva, Carla},
  date = {2018-09-17},
  series = {{{SBES}} '18},
  pages = {112--121},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3266237.3266270},
  url = {https://doi.org/10.1145/3266237.3266270},
  urldate = {2022-10-22},
  abstract = {Context: Privacy of personal data is a growing concern regarding users of software systems. In this sense, the literature reports that in order to avoid privacy breaches, there must be systematic approaches to specify privacy requirements from the early activities of software development. Objective: Motivated by this situation, this paper presents a framework of privacy modeling capabilities that must be addressed by requirements modeling languages to better support privacy specification. The capabilities will be used to compare three goal-oriented modeling languages (i*, NFR-Framework and Secure-Tropos). Method: The framework was created with basis on a conceptual foundation and a conceptual model of privacy built from an analysis of a standard, a regulation, guidelines and other bibliographical sources related to privacy. A health care example is used to illustrate how the framework can be used to compare the chosen modeling languages. Results: Fourteen privacy modeling capabilities were defined in the framework and it was observed that the analyzed modeling languages do not fully support them. Conclusions: The proposed framework contributes towards the consolidation of a privacy conceptual foundation that can be used to evaluate modeling languages for privacy in Requirements Engineering. The comparison performed by using this framework indicates Secure-Tropos as the most complete language to model privacy among the analyzed goal-oriented modeling languages.},
  isbn = {978-1-4503-6503-1},
  keywords = {goal-oriented languages,privacy,requirements engineering,requirements modeling},
  file = {C:\Users\ron\Zotero\storage\K4P8YNCN\Peixoto and Silva - 2018 - Specifying privacy requirements with goal-oriented.pdf}
}

@article{perryVintCerfMistakes2023,
  entrysubtype = {magazine},
  title = {Vint {{Cerf}} on 3 {{Mistakes He Made}} in {{TCP}}/{{IP}}},
  author = {Perry, Tekla S},
  date = {2023-05-07},
  journaltitle = {IEEE Spectrum},
  volume = {60},
  number = {5},
  url = {https://spectrum.ieee.org/vint-cerf-mistakes},
  file = {C:\Users\ron\Downloads\05_Spectrum_2023.pdf}
}

@inbook{petersonArtistryLanguageInvention2020,
  title = {Artistry in Language Invention: {{Conlang}} Pedagogy and the Instructor as Authority},
  shorttitle = {Artistry in Language Invention},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Peterson, David J.},
  date = {2020-08-12},
  pages = {251--282},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0015},
  url = {https://academic.oup.com/book/31973/chapter/267718337},
  urldate = {2023-08-24},
  abstract = {Courses in language construction are often taught with an eye to generating interest in the linguistics major from undergraduate students who might otherwise be uninterested in taking an introductory course. With the focus primarily on linguistics, the artistry of language invention is often lost. This chapter lays out a method of teaching and evaluating language construction that is rigorous, informed by linguistics, and takes language creation seriously as an art form. Specifically, this chapter argues for the importance of instruction in naturalistic language creation. Two forms of naturalism are introduced and contrasted: weak naturalism (based on statistical typological patterns) and strong naturalism (based on simulating natural linguistic evolution). The chapter closes with an extended example of a homework assignment employing some of the ideas introduced in the chapter, plus a word of caution about the use of linguistic theory in a course on language construction.},
  bookauthor = {Peterson, David J.},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@book{petersonArtLanguageInvention2015,
  title = {The Art of Language Invention: From {{Horse-Lords}} to {{Dark Elves}}, the Words behind World-Building},
  shorttitle = {The Art of Language Invention},
  author = {Peterson, David J.},
  date = {2015},
  publisher = {{Penguin Books}},
  location = {{New York, New York}},
  isbn = {978-0-14-312646-1},
  pagetotal = {292},
  keywords = {{Languages, Artificial}}
}

@article{petitGraphbasedReentrancyfreeSemantic2023,
  title = {On {{Graph-based Reentrancy-free Semantic Parsing}}},
  author = {Petit, Alban and Corro, Caio},
  date = {2023-06-29},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {703--722},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00570},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00570/116614/On-Graph-based-Reentrancy-free-Semantic-Parsing},
  urldate = {2023-08-14},
  abstract = {Abstract              We propose a novel graph-based approach for semantic parsing that resolves two problems observed in the literature: (1) seq2seq models fail on compositional generalization tasks; (2) previous work using phrase structure parsers cannot cover all the semantic parses observed in treebanks. We prove that both MAP inference and latent tag anchoring (required for weakly-supervised learning) are NP-hard problems. We propose two optimization algorithms based on constraint smoothing and conditional gradient to approximately solve these inference problems. Experimentally, our approach delivers state-of-the-art results on GeoQuery, Scan, and Clevr, both for i.i.d. splits and for splits that test for compositional generalization.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\CPHMBTVZ\tacl_a_00570.pdf}
}

@inproceedings{pimentelGamifiedRequirementsInspection2018,
  title = {A Gamified Requirements Inspection Process for Goal Models},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Pimentel, João and Santos, Emanuel and Pereira, Tarcisio and Ferreira, Daniel and Castro, Jaelson},
  date = {2018-04-09},
  series = {{{SAC}} '18},
  pages = {1300--1307},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3167132.3167272},
  url = {https://doi.org/10.1145/3167132.3167272},
  urldate = {2022-10-22},
  abstract = {Despite its potential benefits, requirements inspection is an often overlooked activity. When it is performed, it is usually in an unstructured, ad hoc fashion. This is particularly the case for requirements documented as i* goal models, considering that there is no inspection process designed specifically to address this kind of models. In this paper, we propose a gamified requirements inspection process that goes beyond the straightforward application of game mechanics. The Ring-i process (Requirements Inspection Gamified process for i* Models) allows stakeholders to verify i* models together, in a playful environment. Empirical evaluation with students in a Requirements Engineering course provided indications that the process is not only useful but also fun and easy to use.},
  isbn = {978-1-4503-5191-1},
  keywords = {gamification,goal modeling,requirements engineering,requirements inspection,social modeling,software engineering,verification and validation},
  file = {C:\Users\ron\Zotero\storage\J65J3NVG\Pimentel et al. - 2018 - A gamified requirements inspection process for goa.pdf}
}

@online{Pluralistic09Nov,
  title = {Pluralistic: 09 {{Nov}} 2022 {{Delegating}} Trust Is Really, Really, Really Hard (Infosec Edition) – {{Pluralistic}}: {{Daily}} Links from {{Cory Doctorow}}},
  shorttitle = {Pluralistic},
  url = {https://pluralistic.net/2022/11/09/infosec-blackpill/},
  urldate = {2022-11-10},
  langid = {american},
  file = {C:\Users\ron\Zotero\storage\5KSQGJBW\infosec-blackpill.html}
}

@book{poibeauMachineTranslation2017,
  title = {Machine Translation},
  author = {Poibeau, Thierry},
  date = {2017},
  series = {The {{MIT Press}} Essential Knowledge Series},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-53421-5},
  pagetotal = {285},
  keywords = {History,Machine translating}
}

@inproceedings{porterOperatingSystemTransactions2009,
  title = {Operating {{System Transactions}}},
  booktitle = {Proceedings of the {{ACM SIGOPS}} 22nd Symposium on {{Operating}} Systems Principles},
  author = {Porter, Donald E. and Hofmann, Owen S. and Rossbach, Christopher J. and Benn, Alexander and Witchel, Emmett},
  date = {2009-10-11},
  series = {{{SOSP}} '09},
  pages = {161--176},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1629575.1629591},
  url = {https://doi.org/10.1145/1629575.1629591},
  urldate = {2022-06-24},
  abstract = {Applications must be able to synchronize accesses to operating system resources in order to ensure correctness in the face of concurrency and system failures. System transactions allow the programmer to specify updates to heterogeneous system resources with the OS guaranteeing atomicity, consistency, isolation, and durability (ACID). System transactions efficiently and cleanly solve persistent concurrency problems that are difficult to address with other techniques. For example, system transactions eliminate security vulnerabilities in the file system that are caused by time-of-check-to-time-of-use (TOCTTOU) race conditions. System transactions enable an unsuccessful software installation to roll back without disturbing concurrent, independent updates to the file system. This paper describes TxOS, a variant of Linux 2.6.22 that implements system transactions. TxOS uses new implementation techniques to provide fast, serializable transactions with strong isolation and fairness between system transactions and non-transactional activity. The prototype demonstrates that a mature OS running on commodity hardware can provide system transactions at a reasonable performance cost. For instance, a transactional installation of OpenSSH incurs only 10\% overhead, and a non-transactional compilation of Linux incurs negligible overhead on TxOS. By making transactions a central OS abstraction, TxOS enables new transactional services. For example, one developer prototyped a transactional ext3 file system in less than one month.},
  isbn = {978-1-60558-752-3},
  keywords = {operating systems,race conditions,transactional memory,transactions,txos},
  file = {C:\Users\ron\Zotero\storage\SZ4IMV2X\Porter et al. - 2009 - Operating System Transactions.pdf}
}

@inproceedings{pucciDeviceControlAbstractions1992,
  title = {Device Control Abstractions for Workstation Operating System},
  booktitle = {[1992] {{Proceedings Third Workshop}} on {{Workstation Operating Systems}}},
  author = {Pucci, M.F.},
  date = {1992},
  pages = {34--38},
  publisher = {{IEEE Comput. Soc. Press}},
  location = {{Key Biscayne, FL, USA}},
  doi = {10.1109/WWOS.1992.275692},
  url = {http://ieeexplore.ieee.org/document/275692/},
  urldate = {2022-05-15},
  abstract = {The efficient and flexible control of hardware devices has become more critical as conventional systems begin to address multimedia applications. Unpredicatable workloads on a general purpose workstation operating system will cause disturbances in process scheduling that become visible to the user. While it is usually just a minor annoyance if a program takes a little longer to compile, it can be a major disruption if a video sequence displayed in a window system drops frames or loses lip synchronization. Device control and general purpose computing have inherently different characteristics that require unique treatment. Systems that combine real-time and general purpose characteristics in a single operating system have met with mixed success. The demands on a unified system tend to pull in opposite and conflicting directions.},
  eventtitle = {[1992] {{Third Workshop}} on {{Workstation Operating Systems}}},
  isbn = {978-0-8186-2555-8},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\4L8ZFJU5\Pucci - 1992 - Device control abstractions for workstation operat.pdf}
}

@book{punskeLanguageInventionLinguistics2020,
  title = {Language Invention in Linguistics Pedagogy},
  editor = {Punske, Jeffrey and Sanders, Nathan and Fountain, Amy},
  date = {2020},
  edition = {First edition},
  publisher = {{Oxford University Press}},
  location = {{Oxford}},
  abstract = {"This volume brings together multiple emerging strands of interest in language and linguistics. First is increasing attention on pedagogical scholarship in linguistics, signaled by the 2013 addition to the flagship journal Language of a series on Teaching Linguistics (see for example Sanders 2016) and by many recent panels and workshops on pedagogy at linguistics conferences around the world. Additionally, public outreach has gained greater prominence in the field, with linguists becoming more active and engaged with the public on social media and in podcasts. There has also been an increase in broader public interest in constructed languages (conlangs) and how to build them, indicated by the popularity of conlangs in film and television (e.g. Star Trek, Avatar, and Game of Thrones) and by the success of relevant books (e.g. Okrent 2010; Rosenfelder 2010; Peterson 2014, 2015). This volume showcases a variety of methods which instructors can use to tap into this public interest in conlangs and conlanging to reach a broader student population, increase their engagement with course material, deepen their understanding of linguistics and its interdisciplinary relationships, and provide opportunities for public outreach. Using language invention as a pedagogical tool is an innovative way to capitalize on the effectiveness of many modern educational approaches, such as problem-based learning, collaborative learning, and active learning, especially for a diverse cohort of learners. The methods and materials presented in this volume help cultivate students' understanding of language, linguistic diversity, linguistic analysis, and the power of creativity"--Publisher's description},
  isbn = {978-0-19-256543-3},
  langid = {english},
  annotation = {OCLC: 1197630196}
}

@inproceedings{qahtaniImpactCovid19Pandemic2022,
  title = {Impact of the {{Covid-19}} Pandemic on the Requirement Engineering Process in Small Development Projects: {{A}} Case Study},
  shorttitle = {Impact of the {{Covid-19}} Pandemic on the Requirement Engineering Process in Small Development Projects},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Information Communication}} and {{Management}}},
  author = {Qahtani, Abdulrahman Mohammed},
  date = {2022-07-13},
  series = {{{ICICM}} '22},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3551690.3551691},
  url = {https://doi.org/10.1145/3551690.3551691},
  urldate = {2022-10-22},
  abstract = {Requirement engineering (RE) is crucial for any software development project. It plays a vital role in the development lifecycle, as it lays the foundation for all subsequent development steps. Ensuring proper collection of project requirements makes the subsequent process, including design, development and testing, easier. The software industry has paid great attention to RE, including tools and techniques, starting with requirement elicitation. In the last couple of years, the Covid-19 pandemic has impacted processes, as meeting clients and stakeholders face-to-face and visiting them in their workplace has been made more difficult by social distancing, changing the techniques for requirement elicitation. Most software development companies rely on distant communication and alternative approaches to collect customers’ requirements to understand their needs, so they can reflect the reality in the work and try to persuade the customers that the developed solution meets their requirements. This study presents a case study of three educational projects at three different universities with different development requirements. The study compares the process before and during the pandemic to identify the challenges and features of the shift which occurred and measure its impact on number of requirements completed in both periods, as well as its impact on development project resources. This study's findings indicate increased requirements during the elicitation process and changes in human resources because of eliminating the practitioners over distance communication during the Covid-19 pandemic.},
  isbn = {978-1-4503-9649-3},
  keywords = {Case study,Covid-19,Requirement engineering,Software projects},
  file = {C:\Users\ron\Zotero\storage\WHPEXF4Z\Qahtani - 2022 - Impact of the Covid-19 pandemic on the requirement.pdf}
}

@unpublished{qaissauneeMcCumberCubeCIA2021,
  title = {The {{McCumber Cube}} and {{CIA Triad}}},
  author = {Qaissaunee, Mike and Sands, John and Sands, Susan},
  date = {2021},
  url = {https://www.ncyte.net/faculty/cybersecurity-curriculum/college-curriculum/interactive-lessons/the-mccumber-cube-and-cia-triad}
}

@misc{qualcomminc.Snapdragon835Mobile,
  title = {Snapdragon 835 {{Mobile Platform}}},
  author = {Qualcomm, Inc.},
  url = {https://www.qualcomm.com/products/application/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-835-mobile-platform}
}

@article{raghavanSOCIETALIMPACTSALGORITHMIC,
  title = {{{THE SOCIETAL IMPACTS OF ALGORITHMIC DECISION-MAKING}}},
  author = {Raghavan, Manish},
  pages = {481},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\JPEAHZAR\Raghavan - THE SOCIETAL IMPACTS OF ALGORITHMIC DECISION-MAKIN.pdf}
}

@article{raghunathanOpenSourceClosed2005,
  title = {Open {{Source Versus Closed Source}}: {{Software Quality}} in {{Monopoly}} and {{Competitive Markets}}},
  shorttitle = {Open {{Source Versus Closed Source}}},
  author = {Raghunathan, S. and Prasad, A. and Mishra, B.K. and Chang, H.},
  date = {2005-11},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  shortjournal = {IEEE Trans. Syst., Man, Cybern. A},
  volume = {35},
  number = {6},
  pages = {903--918},
  issn = {1083-4427},
  doi = {10.1109/TSMCA.2005.853493},
  url = {http://ieeexplore.ieee.org/document/1519032/},
  urldate = {2022-05-23},
  abstract = {The open source model of software development has received substantial attention in the industry and popular media; nevertheless, critics frequently contend that open source softwares are inferior in quality compared to closed source softwares because of lack of incentives and project management, while proponents argue the opposite. This paper examines this quality debate by modeling and analyzing software quality, demand, profitability, and welfare under open and closed source environments in monopoly and competitive markets. The results show no dominant quality advantage of one method over another under all circumstances. Both open source and closed source qualities decrease in a competitive market. Conditions under which each method can generate higher quality software are examined.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\KBU7GCBY\Raghunathan et al. - 2005 - Open Source Versus Closed Source Software Quality.pdf}
}

@inproceedings{rahimiEvolvingSoftwareTrace2019,
  title = {Evolving Software Trace Links between Requirements and Source Code},
  booktitle = {Proceedings of the 10th {{International Workshop}} on {{Software}} and {{Systems Traceability}}},
  author = {Rahimi, Mona and Cleland-Huang, Jane},
  date = {2019-05-27},
  series = {{{SST}} '19},
  pages = {12},
  publisher = {{IEEE Press}},
  location = {{Montreal, Quebec, Canada}},
  doi = {10.1109/SST.2019.00012},
  url = {https://doi.org/10.1109/SST.2019.00012},
  urldate = {2022-10-22},
  abstract = {Maintaining trace links in response to continuous changes occurring in software systems is arduous. In this paper, we present a Trace Link Evolver (TLE) to automatically evolve source-to-requirement trace links according to underlying changes in the system. TLE depends on a set of heuristics coupled with refactoring detection tools and information retrieval algorithms to detect predefined change scenarios that occur across contiguous versions of a software system. Our evaluations show that considering both structural and semantic changes leads to more accurate trace link evolution.},
  keywords = {evolution,maintenance,traceability},
  file = {C:\Users\ron\Zotero\storage\WN5ECRRW\Rahimi and Cleland-Huang - 2019 - Evolving software trace links between requirements.pdf}
}

@inproceedings{rahmidewiSoftwareRequirementRelatedInformation2021,
  title = {Software {{Requirement-Related Information Extraction}} from {{Online News}} Using {{Domain Specificity}} for {{Requirements Elicitation}}: {{How}} the System Analyst Can Get Software Requirements without Constrained by Time and Stakeholder Availability},
  shorttitle = {Software {{Requirement-Related Information Extraction}} from {{Online News}} Using {{Domain Specificity}} for {{Requirements Elicitation}}},
  booktitle = {2021 10th {{International Conference}} on {{Software}} and {{Computer Applications}}},
  author = {Rahmi Dewi, Mutia and Kharisma Raharjana, Indra and Siahaan, Daniel and Fatichah, Chastine},
  date = {2021-02-23},
  series = {{{ICSCA}} 2021},
  pages = {81--87},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3457784.3457796},
  url = {https://doi.org/10.1145/3457784.3457796},
  urldate = {2022-10-22},
  abstract = {Stakeholder involvement is vital in the requirements elicitation process. However, a lack of stakeholder involvement in the project often occurs. Various data sources, such as business processes and software documentation, will help determine what stakeholders need. The limitation of these documents is that they are technical and do not contain numerous stakeholder's needs. This paper aims to extract information related to software requirements from online news using domain specificity. The online news comprises elements that compose the user story, namely: the aspect of who (stakeholder), the aspect of what (functional), and the aspect of why (reason). User stories are standard practices used to capture and write functional software requirements in agile software development. This paper proposes to use domain specificity to extract aspects of what from online news to increase understanding of domain knowledge, especially software functionality. We use SRS documents and online news as data sources. Based on these datasets, domain specificity is calculated to produce software specific vocabulary. POS tags are being used to extract software requirements-related information from online news. When examining two datasets, our approach improved precision and recall with average values of 0.56 and 0.579.},
  isbn = {978-1-4503-8882-5},
  keywords = {online news,POS tagging,software requirement-related information,software-specific vocabulary,user stories},
  file = {C:\Users\ron\Zotero\storage\4E34GIN3\Rahmi Dewi et al. - 2021 - Software Requirement-Related Information Extractio.pdf}
}

@inproceedings{rahmidewiSoftwareRequirementRelatedInformation2021a,
  title = {Software {{Requirement-Related Information Extraction}} from {{Online News}} Using {{Domain Specificity}} for {{Requirements Elicitation}}: {{How}} the System Analyst Can Get Software Requirements without Constrained by Time and Stakeholder Availability},
  shorttitle = {Software {{Requirement-Related Information Extraction}} from {{Online News}} Using {{Domain Specificity}} for {{Requirements Elicitation}}},
  booktitle = {2021 10th {{International Conference}} on {{Software}} and {{Computer Applications}}},
  author = {Rahmi Dewi, Mutia and Kharisma Raharjana, Indra and Siahaan, Daniel and Fatichah, Chastine},
  date = {2021-02-23},
  pages = {81--87},
  publisher = {{ACM}},
  location = {{Kuala Lumpur Malaysia}},
  doi = {10.1145/3457784.3457796},
  url = {https://dl.acm.org/doi/10.1145/3457784.3457796},
  urldate = {2022-10-22},
  abstract = {Stakeholder involvement is vital in the requirements elicitation process. However, a lack of stakeholder involvement in the project often occurs. Various data sources, such as business processes and software documentation, will help determine what stakeholders need. The limitation of these documents is that they are technical and do not contain numerous stakeholder’s needs. This paper aims to extract information related to software requirements from online news using domain specificity. The online news comprises elements that compose the user story, namely: the aspect of who (stakeholder), the aspect of what (functional), and the aspect of why (reason). User stories are standard practices used to capture and write functional software requirements in agile software development. This paper proposes to use domain specificity to extract aspects of what from online news to increase understanding of domain knowledge, especially software functionality. We use SRS documents and online news as data sources. Based on these datasets, domain specificity is calculated to produce software specific vocabulary. POS tags are being used to extract software requirements-related information from online news. When examining two datasets, our approach improved precision and recall with average values of 0.56 and 0.579.},
  eventtitle = {{{ICSCA}} 2021: 2021 10th {{International Conference}} on {{Software}} and {{Computer Applications}}},
  isbn = {978-1-4503-8882-5},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\6Y5WEBJJ\Rahmi Dewi et al. - 2021 - Software Requirement-Related Information Extractio.pdf}
}

@inproceedings{rakitaContextAwareSmartphoneApplications2018,
  title = {Context-{{Aware Smartphone Applications Based}} on {{Generic Context Supervision Services}}},
  booktitle = {2018 26th {{Telecommunications Forum}} ({{TELFOR}})},
  author = {Rakita, Bojan and Bundalo, Zlatko and Bundalo, Dusanka and Sajic, Mirko},
  date = {2018-11},
  pages = {420--425},
  publisher = {{IEEE}},
  location = {{Belgrade}},
  doi = {10.1109/TELFOR.2018.8611924},
  url = {https://ieeexplore.ieee.org/document/8611924/},
  urldate = {2022-05-15},
  abstract = {Mobile phones have the abilities to use contextual information from a variety of different sources. Primarily, the contextual information can be collected from the history of mobile device usage, personal user information and sensor readings. The generalization of such collected information and data, as well as their processing, can lead to numerous benefits, where the most notable is the software reusability and maintainability. The more the types of data are similar it means that generalization is more realistic. This paper describes principle of development of a context-aware applications based on a generic solution for contextual information control for Android operating system mobile based devices. In this paper the focus is on creating supervisor framework which provides higher level of information for clients based on basic information conditions and their combinations. It was developed, implemented and described one practical Android based smartphone application that provides to the user personalized view on his/her meetings and tasks within the organization.},
  eventtitle = {2018 26th {{Telecommunications Forum}} ({{TELFOR}})},
  isbn = {978-1-5386-7171-9},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\QBYM25GZ\Rakita et al. - 2018 - Context-Aware Smartphone Applications Based on Gen.pdf}
}

@artwork{randalmonroeDependency,
  title = {Dependency},
  author = {{Randal Monroe}}
}

@online{randalmonroeXkcdDependency,
  title = {Xkcd: {{Dependency}}},
  author = {{Randal Monroe}},
  url = {https://xkcd.com/2347/},
  urldate = {2022-05-24},
  file = {C:\Users\ron\Zotero\storage\BR937JA7\2347.html}
}

@inproceedings{rathAnalyzingRequirementsTraceability2018,
  title = {Analyzing Requirements and Traceability Information to Improve Bug Localization},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Rath, Michael and Lo, David and Mäder, Patrick},
  date = {2018-05-28},
  series = {{{MSR}} '18},
  pages = {442--453},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3196398.3196415},
  url = {https://doi.org/10.1145/3196398.3196415},
  urldate = {2022-10-22},
  abstract = {Locating bugs in industry-size software systems is time consuming and challenging. An automated approach for assisting the process of tracing from bug descriptions to relevant source code benefits developers. A large body of previous work aims to address this problem and demonstrates considerable achievements. Most existing approaches focus on the key challenge of improving techniques based on textual similarity to identify relevant files. However, there exists a lexical gap between the natural language used to formulate bug reports and the formal source code and its comments. To bridge this gap, state-of-the-art approaches contain a component for analyzing bug history information to increase retrieval performance. In this paper, we propose a novel approach TraceScore that also utilizes projects' requirements information and explicit dependency trace links to further close the gap in order to relate a new bug report to defective source code files. Our evaluation on more than 13,000 bug reports shows, that TraceScore significantly outperforms two state-of-the-art methods. Further, by integrating TraceScore into an existing bug localization algorithm, we found that TraceScore significantly improves retrieval performance by 49\% in terms of mean average precision (MAP).},
  isbn = {978-1-4503-5716-6},
  keywords = {bug localization,machine learning,requirements traceability,software maintenance,traceability recovery,version history},
  file = {C:\Users\ron\Zotero\storage\ZNS23B9D\Rath et al. - 2018 - Analyzing requirements and traceability informatio.pdf}
}

@article{raymondCathedralBazaar,
  title = {The {{Cathedral}} and the {{Bazaar}}},
  author = {Raymond, Eric S},
  pages = {18},
  abstract = {I anatomize a successful open-source project, fetchmail, that was run as a deliberate test of the surprising theories about software engineering suggested by the history of Linux. I discuss these theories in terms of two fundamentally different development styles, the “cathedral” model of most of the commercial world versus the “bazaar” model of the Linux world. I show that these models derive from opposing assumptions about the nature of the software-debugging task. I then make a sustained argument from the Linux experience for the proposition that “Given enough eyeballs, all bugs are shallow”, suggest productive analogies with other self-correcting systems of selfish agents, and conclude with some exploration of the implications of this insight for the future of software.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\4YZ8N38X\Raymond - The Cathedral and the Bazaar.pdf}
}

@online{RedHawkLinux2022,
  title = {{{RedHawk Linux}}},
  date = {2022-04-25},
  url = {https://concurrent-rt.com/products/software/redhawk-linux/},
  urldate = {2022-06-18},
  abstract = {The most reliable real-time operating system available today.},
  langid = {american},
  organization = {{Concurrent Real-Time}},
  file = {C:\Users\ron\Zotero\storage\2NRK7MJA\redhawk-linux.html}
}

@article{rocheAdoptingDevOpsPractices2013,
  title = {Adopting {{DevOps}} Practices in Quality Assurance},
  author = {Roche, James},
  date = {2013-11},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {56},
  number = {11},
  pages = {38--43},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2524713.2524721},
  url = {https://dl.acm.org/doi/10.1145/2524713.2524721},
  urldate = {2022-06-20},
  abstract = {Merging the art and science of software development.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\GQR33UJA\dev_ops_p38-roche.pdf}
}

@legislation{rockefellerBillProvideOngoing2013,
  title = {A Bill to Provide for an Ongoing, Voluntary Public-Private Partnership to Improve Cybersecurity, and to Strengthen Cybersecurity Research and Development, Workforce Development and Education, and Public Awareness and Preparedness, and for Other Purposes.},
  shorttitle = {Cybersecurity {{Enhancement Act}} of 2014},
  editora = {Rockefeller, John D and Thune, John},
  editoratype = {collaborator},
  date = {2013/2014},
  journaltitle = {Public Law 113-274},
  number = {S. 1353},
  url = {https://www.congress.gov/bill/113th-congress/senate-bill/1353/text}
}

@inproceedings{rodriguezUserSystemStories2021,
  title = {User and {{System Stories}}: {{An Agile Approach}} for {{Managing Requirements}} in {{AOSE}}},
  shorttitle = {User and {{System Stories}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  author = {Rodriguez, Sebastian and Thangarajah, John and Winikoff, Michael},
  date = {2021-05-03},
  series = {{{AAMAS}} '21},
  pages = {1064--1072},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  location = {{Richland, SC}},
  abstract = {The agile software development life cycle is widely used in industry today due to its highly flexible and iterative processes that facilitate rapid prototyping. There has been recent work in bringing concepts and processes from agile methodologies to agent-oriented software engineering (AOSE). We contribute to this effort by presenting in this paper a novel approach to capturing requirements of agent systems in AOSE using and extending agile concepts. In this paper, we propose to adopt and extend the well-known concept of User Stories to facilitate the development of agent systems. We introduce a novel concept, System Story, that defines requirements from the perspective of the system. These System Stories are refinements of User Stories and provide more intuitive mappings to agent concepts in the design and implementation. We show how our approach allows better traceability of requirements between stories and the different software development artifacts. We validate our proposal with a feature-based comparison to recent related work, and a preliminary user evaluation based on a drone simulation of a simple search and rescue case study.},
  isbn = {978-1-4503-8307-3},
  keywords = {agile methodologies,AOSE,engineering MAS},
  file = {C:\Users\ron\Zotero\storage\3S99ZPSS\Rodriguez et al. - 2021 - User and System Stories An Agile Approach for Man.pdf}
}

@book{rosenfelderLanguageConstructionKit2010,
  title = {The Language Construction Kit},
  author = {Rosenfelder, Mark},
  date = {2010},
  publisher = {{Yonagu Books}},
  location = {{Chicago}},
  abstract = {Create plausible and realistic languages for role-playing games, fantasy and science fiction, movies or video games, or international communication},
  isbn = {978-0-9844700-0-6},
  pagetotal = {292},
  annotation = {OCLC: ocn639971902}
}

@inreference{RubikCube2022,
  title = {Rubik's {{Cube}}},
  booktitle = {Wikipedia},
  date = {2022-11-19T08:42:45Z},
  url = {https://en.wikipedia.org/w/index.php?title=Rubik%27s_Cube&oldid=1122727745},
  urldate = {2022-11-20},
  abstract = {The Rubik's Cube is a 3-D combination puzzle originally invented in 1974 by Hungarian sculptor and professor of architecture Ernő Rubik. Originally called the Magic Cube, the puzzle was licensed by Rubik to be sold by Ideal Toy Corp in 1980 via businessman Tibor Laczi and Seven Towns founder Tom Kremer. The cube was released internationally in 1980 and became one of the most recognized icons in popular culture. It won the 1980 German Game of the Year special award for Best Puzzle. As of January 2009, 350 million cubes had been sold worldwide, making it the world's bestselling puzzle game and bestselling toy.On the original classic Rubik's Cube, each of the six faces was covered by nine stickers, each of one of six solid colours: white, red, blue, orange, green, and yellow. Some later versions of the cube have been updated to use coloured plastic panels instead, which prevents peeling and fading.  Since 1988, the arrangement of colours has been standardised with white opposite yellow, blue opposite green, and orange  opposite red, and the red, white, and blue arranged clockwise in that order. On early cubes, the position of the colours varied from cube to cube. An internal pivot mechanism enables each face to turn independently, thus mixing up the colours. For the puzzle to be solved, each face must be returned to have only one colour. Similar puzzles have now been produced with various numbers of sides, dimensions, and stickers, not all of them by Rubik. Although the Rubik's Cube reached its height of mainstream popularity in the 1980s, it is still widely known and used. Many speedcubers continue to practise it and similar puzzles, and compete for the fastest times in various categories. Since 2003, the World Cube Association, the international governing body of the Rubik's Cube, has organised competitions worldwide and recognises world records.},
  langid = {english},
  annotation = {Page Version ID: 1122727745},
  file = {C:\Users\ron\Zotero\storage\SS2MNJIP\Rubik's_Cube.html}
}

@video{ruhlHowLanguagesSteal2023,
  entrysubtype = {video},
  title = {How {{Languages Steal Words From Each Other}}},
  editora = {Ruhl, Molly and Scott, Tom and McCulloch and Scott, Tom},
  editoratype = {scriptwriter},
  date = {2023-09-18},
  url = {https://youtu.be/TFpzps-DCb0?si=YRBTq28CBmWqZWRI}
}

@inproceedings{saitoVisualizingEffectsRequirements2016,
  title = {Visualizing the Effects of Requirements Evolution},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering Companion}}},
  author = {Saito, Shinobu and Iimura, Yukako and Tashiro, Hirokazu and Massey, Aaron K. and Antón, Annie I.},
  date = {2016-05-14},
  series = {{{ICSE}} '16},
  pages = {152--161},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2889160.2889237},
  url = {https://doi.org/10.1145/2889160.2889237},
  urldate = {2022-10-22},
  abstract = {Changes to software requirements occur throughout the software life cycle. Requirements engineers who maintain software systems in regulated environments must identify the affected artifacts when requirements change. This identification is critical to: (a) ensure continued compliance with regulations, and (b) accurately estimate budget requests. Previously, we introduced Requirements Evolution Charts (REC) to provide a visual representation of requirements evolution history. An REC is generated from the issue tickets in which requirements engineers record changes to requirements artifacts. Herein, we examine whether a REC helps software engineers conduct an impact analysis. Thirty experienced NTT requirements engineers with no prior domain knowledge identified the impact of seven requirements changes in a large-scale system governed by Japanese laws and regulations. They were divided into two groups of fifteen engineers. The REC group employed the REC to aid their efforts; the Non-REC group conducted their impact analysis without the REC. Participants in both groups identified which of the 139 artifacts were affected based on a history of 108 issue tickets. Our study reveals that engineers in the REC group identified the affected artifacts more accurately and quickly than the Non-REC group, suggesting that the REC is a valuable tool for examining the impact of requirements evolution.},
  isbn = {978-1-4503-4205-6},
  keywords = {change requirements,impact analysis,issue ticket,requirements evolution},
  file = {C:\Users\ron\Zotero\storage\EPFV5TPX\Saito et al. - 2016 - Visualizing the effects of requirements evolution.pdf}
}

@article{salahPerformanceAnalysisComparison2007,
  title = {Performance Analysis and Comparison of Interrupt-Handling Schemes in Gigabit Networks},
  author = {Salah, K. and El-Badawi, K. and Haidari, F.},
  date = {2007-11},
  journaltitle = {Computer Communications},
  shortjournal = {Computer Communications},
  volume = {30},
  number = {17},
  pages = {3425--3441},
  issn = {01403664},
  doi = {10.1016/j.comcom.2007.06.013},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0140366407002691},
  urldate = {2022-06-14},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\YCT7LXZ7\Salah et al. - 2007 - Performance analysis and comparison of interrupt-h.pdf}
}

@inbook{sandersInterdisciplinarityConlangsMoving2020,
  title = {The Interdisciplinarity of Conlangs: {{Moving}} beyond Linguistics},
  shorttitle = {The Interdisciplinarity of Conlangs},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Sanders, Nathan and Schreyer, Christine},
  date = {2020-08-12},
  pages = {169--185},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0011},
  url = {https://academic.oup.com/book/31973/chapter/267717815},
  urldate = {2023-08-24},
  abstract = {This chapter discusses the pedagogical and cognitive benefits that interdisciplinarity has for both students and instructors when paired with intentional pedagogy, in the context of courses that use language construction to teach linguistics. The chapter presents four case studies from such courses, addressing how a language is shaped by the anatomy of its users, the physical environment it is transmitted through, and the surrounding culture of its users. As these case studies demonstrate, language construction provides opportunities to explore the links that language has with other fields of study, such as biology, physics, and culture, highlighting the inherent interdisciplinarity of linguistics and anthropology.},
  bookauthor = {Sanders, Nathan and Schreyer, Christine},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@inbook{sandersPrimerConstructedLanguages2020,
  title = {A Primer on Constructed Languages},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Sanders, Nathan},
  date = {2020-08-12},
  pages = {6--26},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0002},
  url = {https://academic.oup.com/book/31973/chapter/267716880},
  urldate = {2023-08-24},
  abstract = {This chapter outlines the history of language construction, beginning with the earliest recorded examples of linguistic creativity and continuing with the first true constructed languages from the Middle Ages up through the Renaissance and Enlightenment, when language construction was guided largely by religious and philosophical concerns. The chapter continues by exploring more recent history, when language construction was guided more by practical goals to unite humanity. At the same time, language construction as an art form was also being developed, most notably by J. R. R. Tolkien, who set the stage for the modern era of artistic language construction requiring specialized knowledge, talent, and hard work. The chapter also discusses the emerging role of language construction as a tool for language revitalization and concludes with a summary of terms and concepts that are important to the study of constructed languages.},
  bookauthor = {Sanders, Nathan},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@inproceedings{santosIncrementalArchitecturalRequirements2018,
  title = {Incremental Architectural Requirements for Agile Modeling: A Case Study within a Scrum Project},
  shorttitle = {Incremental Architectural Requirements for Agile Modeling},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Agile Software Development}}: {{Companion}}},
  author = {Santos, Nuno and Pereira, Jaime and Morais, Francisco and Barros, Júlio and Ferreira, Nuno and Machado, Ricardo J.},
  date = {2018-05-21},
  pages = {1--4},
  publisher = {{ACM}},
  location = {{Porto Portugal}},
  doi = {10.1145/3234152.3234166},
  url = {https://dl.acm.org/doi/10.1145/3234152.3234166},
  urldate = {2022-10-22},
  abstract = {Models may be used as a shared understanding when the project is composed by distributed agile teams. This paper describes the applicability of a process for modeling a small set of requirements and a candidate architecture for a distributed Scrum teams context, addressing the use of such models within the delivery of user stories included in a team backlog and inter-team coordination.},
  eventtitle = {{{XP}} '18 {{Companion}}: 19th {{International Conference}} on {{Agile Software Development}}},
  isbn = {978-1-4503-6422-5},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\GABZBSDX\Santos et al. - 2018 - Incremental architectural requirements for agile m.pdf}
}

@report{schaefferNationalInformationAssurance2010,
  type = {CNSS Instructions},
  title = {National {{Information Assurance}} ({{IA}}) {{Glossary}}},
  author = {Schaeffer, Richard and {Committee on National Security Systems}},
  date = {2010-04-26},
  number = {4009},
  url = {https://www.dni.gov/files/NCSC/documents/nittf/CNSSI-4009_National_Information_Assurance.pdf},
  file = {C:\Users\ron\Zotero\storage\MX8A8CJX\Schaeffer and Committee on National Security Systems - 2010 - National Information Assurance (IA) Glossary.pdf}
}

@article{schilitDisseminatingActiveMap1994,
  title = {Disseminating Active Map Information to Mobile Hosts},
  author = {Schilit, B.N. and Theimer, M.M.},
  date = {1994-09},
  journaltitle = {IEEE Network},
  shortjournal = {IEEE Network},
  volume = {8},
  number = {5},
  pages = {22--32},
  issn = {0890-8044},
  doi = {10.1109/65.313011},
  url = {http://ieeexplore.ieee.org/document/313011/},
  urldate = {2022-05-15},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\GUQ4T6VK\Schilit and Theimer - 1994 - Disseminating active map information to mobile hos.pdf}
}

@article{schreyerConstructedLanguages2021,
  title = {Constructed {{Languages}}},
  author = {Schreyer, Christine},
  date = {2021-10-21},
  journaltitle = {Annual Review of Anthropology},
  shortjournal = {Annu. Rev. Anthropol.},
  volume = {50},
  number = {1},
  pages = {327--344},
  issn = {0084-6570, 1545-4290},
  doi = {10.1146/annurev-anthro-101819-110152},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-anthro-101819-110152},
  urldate = {2023-08-21},
  abstract = {Constructed languages, also known as conlangs, are languages that have been purposefully created for either real-world or fictional speakers. Within this article, I provide a summary of the language creation process and how the community of conlangers, people who make languages, come to know each other's work, as well as how language creation assignments are being adopted within university classrooms. I also explore the role of the language creator in bringing a community of speakers into existence through the invention of a language. I discuss whether speakers of a constructed language are part of a community of practice or a speech community and the implications for this distinction within anthropology. I also describe conscripts, or constructed orthographies, as well as the relationship between endangered languages and constructed languages, how invented worlds can create real-world shifts in worldview, and suggestions for new directions in research linking anthropology and constructed languages.},
  langid = {english}
}

@report{seangallagherSourceForgeGrabsGIMP,
  title = {{{SourceForge}} Grabs {{GIMP}} for {{Windows}}’ Account, Wraps Installer in Bundle-Pushing Adware [{{Updated}}]},
  author = {{Sean Gallagher}},
  url = {https://arstechnica.com/information-technology/2015/05/sourceforge-grabs-gimp-for-windows-account-wraps-installer-in-bundle-pushing-adware/}
}

@article{seangallagherSourceForgeGrabsGIMPa,
  entrysubtype = {magazine},
  title = {{{SourceForge}} Grabs {{GIMP}} for {{Windows}}’ Account, Wraps Installer in Bundle-Pushing Adware [{{Updated}}]},
  author = {Sean Gallagher},
  journaltitle = {Ars Technica},
  url = {https://arstechnica.com/information-technology/2015/05/sourceforge-grabs-gimp-for-windows-account-wraps-installer-in-bundle-pushing-adware/}
}

@article{sebastianMalayalamNaturalLanguage2023,
  title = {Malayalam {{Natural Language Processing}}: {{Challenges}} in {{Building}} a {{Phrase-Based Statistical Machine Translation System}}},
  shorttitle = {Malayalam {{Natural Language Processing}}},
  author = {Sebastian, Mary Priya and G., Santhosh Kumar},
  date = {2023-04-06},
  journaltitle = {ACM Transactions on Asian and Low-Resource Language Information Processing},
  shortjournal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  volume = {22},
  number = {4},
  pages = {117:1--117:51},
  issn = {2375-4699},
  doi = {10.1145/3579163},
  url = {https://doi.org/10.1145/3579163},
  urldate = {2023-08-14},
  abstract = {Statistical Machine Translation (SMT) is a preferred Machine Translation approach to convert the text in a specific language into another by automatically learning translations using a parallel corpus. SMT has been successful in producing quality translations in many foreign languages, but there are only a few works attempted in South Indian languages. The article discusses on experiments conducted with SMT for Malayalam language and analyzes how the methods defined for SMT in foreign languages affect a Dravidian language, Malayalam. The baseline SMT model does not work for Malayalam due to its unique characteristics like agglutinative nature and morphological richness. Hence, the challenge is to identify where precisely the SMT model has to be modified such that it adapts the challenges of the language peculiarity into the baseline model and give better translations for English to Malayalam translation. The alignments between English and Malayalam sentence pairs, subjected to the training process in SMT, plays a crucial role in producing quality output translation. Therefore, this work focuses on improving the translation model of SMT by refining the alignments between English–Malayalam sentence pairs. The phrase alignment algorithms align the verb and noun phrases in the sentence pairs and develop a new set of alignments for the English–Malayalam sentence pairs. These alignment sets refine the alignments formed from Giza++ produced as a result of EM training algorithm. The improved Phrase-Based SMT model trained using these refined alignments resulted in better translation quality, as indicated by the AER and BLUE scores.},
  keywords = {alignments,Dravidian language,Machine Translation,Malayalam,Natural Language Processing,Statistical Machine Translation},
  file = {C:\Users\ron\Zotero\storage\ZVA7V26D\3579163.pdf}
}

@inproceedings{senellartOpenNMTSystemDescription2018,
  title = {{{OpenNMT System Description}} for {{WNMT}} 2018: 800 Words/Sec on a Single-Core {{CPU}}},
  shorttitle = {{{OpenNMT System Description}} for {{WNMT}} 2018},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Neural Machine Translation}} and {{Generation}}},
  author = {Senellart, Jean and Zhang, Dakun and Wang, Bo and Klein, Guillaume and Ramatchandirin, Jean-Pierre and Crego, Josep and Rush, Alexander},
  date = {2018},
  pages = {122--128},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/W18-2715},
  url = {http://aclweb.org/anthology/W18-2715},
  urldate = {2023-10-21},
  eventtitle = {Proceedings of the 2nd {{Workshop}} on {{Neural Machine Translation}} and {{Generation}}},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\5LAWL8L2\Senellart et al. - 2018 - OpenNMT System Description for WNMT 2018 800 word.pdf}
}

@article{shahResolvingAmbiguitiesNatural2015,
  title = {Resolving {{Ambiguities}} in {{Natural Language Software Requirements}}: {{A Comprehensive Survey}}},
  shorttitle = {Resolving {{Ambiguities}} in {{Natural Language Software Requirements}}},
  author = {Shah, Unnati S. and Jinwala, Devesh C.},
  date = {2015-09-14},
  journaltitle = {ACM SIGSOFT Software Engineering Notes},
  shortjournal = {SIGSOFT Softw. Eng. Notes},
  volume = {40},
  number = {5},
  pages = {1--7},
  issn = {0163-5948},
  doi = {10.1145/2815021.2815032},
  url = {https://doi.org/10.1145/2815021.2815032},
  urldate = {2022-06-09},
  abstract = {Requirements Engineering is one of the most vital activities in the entire Software Development Life Cycle. The success of the software is largely dependent on how well the users' requirements have been understood and converted into appropriate functionalities in the software. Typically, the users convey their requirements in natural language statements that initially appear easy to state. However, being stated in natural language, the statement of requirements often tends to suffer from misinterpretations and imprecise inferences. As a result, the requirements specified thus, may lead to ambiguities in the software specifications. One can indeed find numerous approaches that deal with ensuring precise requirement specifications. Naturally, an obvious approach to deal with ambiguities in natural language software specifications is to eliminate ambiguities altogether i.e. to use formal specifications. However, the formal methods have been observed to be cost-effective largely for the development of mission-critical software. Due to the technical sophistication required, these are yet to be accepted in the mainstream. Hence, the other alternative is to let the ambiguities exist in the natural language requirements but deal with the same using proven techniques viz. using approaches based on machine learning, knowledge and ontology to resolve them. One can indeed find numerous automated and semi-automated tools to resolve specific types of natural language software requirement ambiguities. However, to the best of our knowledge there is no published literature that attempts to compare and contrast the prevalent approaches to deal with ambiguities in natural language software requirements. Hence, in this paper, we attempt to survey and analyze the prevalent approaches that attempt to resolve ambiguities in natural language software requirements. We focus on presenting a state-of-the-art survey of the currently available tools for ambiguity resolution. The objective of this paper is to disseminate, dissect and analyze the research work published in the area, identify metrics for a comparative evaluation and eventually do the same. At the end, we identify open research issues with an aim to spark new interests and developments in this field.},
  keywords = {Ambiguity,Natural Language Processing,Requirements Engineering},
  file = {C:\Users\ron\Zotero\storage\23BF27S3\2815021.2815032.pdf}
}

@inproceedings{shanFacilitatingInterapplicationInteractions2012,
  title = {Facilitating Inter-Application Interactions for {{OS-level}} Virtualization},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN}}/{{SIGOPS}} Conference on {{Virtual Execution Environments}} - {{VEE}} '12},
  author = {Shan, Zhiyong and Wang, Xin and Chiueh, Tzi-cker and Meng, Xiaofeng},
  date = {2012},
  pages = {75},
  publisher = {{ACM Press}},
  location = {{London, England, UK}},
  doi = {10.1145/2151024.2151036},
  url = {http://dl.acm.org/citation.cfm?doid=2151024.2151036},
  urldate = {2022-06-21},
  eventtitle = {The 8th {{ACM SIGPLAN}}/{{SIGOPS}} Conference},
  isbn = {978-1-4503-1176-2},
  langid = {english}
}

@online{sharmaBIGSabotageFamous2022,
  title = {{{BIG}} Sabotage: {{Famous}} Npm Package Deletes Files to Protest {{Ukraine}} War},
  author = {Sharma, Ax},
  date = {2022-03-17},
  url = {https://www.bleepingcomputer.com/news/security/big-sabotage-famous-npm-package-deletes-files-to-protest-ukraine-war/},
  urldate = {2022-05-24},
  file = {C:\Users\ron\Zotero\storage\F4UKT79Q\big-sabotage-famous-npm-package-deletes-files-to-protest-ukraine-war.html}
}

@online{sharmaPopularPythonPHP2022,
  type = {Computer News},
  title = {Popular {{Python}} and {{PHP}} Libraries Hijacked to Steal {{AWS}} Keys},
  author = {Sharma, Ax},
  date = {2022-05-24},
  url = {https://www.bleepingcomputer.com/news/security/popular-python-and-php-libraries-hijacked-to-steal-aws-keys/},
  organization = {{BleepingComputer}}
}

@inproceedings{shiinaLightweightPreemptiveUserlevel2021,
  title = {Lightweight Preemptive User-Level Threads},
  booktitle = {Proceedings of the 26th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Shiina, Shumpei and Iwasaki, Shintaro and Taura, Kenjiro and Balaji, Pavan},
  date = {2021-02-17},
  series = {{{PPoPP}} '21},
  pages = {374--388},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3437801.3441610},
  url = {https://doi.org/10.1145/3437801.3441610},
  urldate = {2022-06-15},
  abstract = {Many-to-many mapping models for user- to kernel-level threads (or "M:N threads") have been extensively studied for decades as a lightweight substitute for current Pthreads implementations that provide a simple one-to-one mapping ("1:1 threads"). M:N threads derive performance from their ability to allow users to context switch between threads and control their scheduling entirely in user space with no kernel involvement. This same ability, however, causes M:N threads to lose the kernel-provided ability of implicit OS preemption---threads have to explicitly yield control for other threads to be scheduled. Hence, programs over nonpreemptive M:N threads can cause core starvation, loss of prioritization, and, sometimes, deadlock unless programs are written to explicitly yield in proper places. This paper explores two techniques for M:N threads to efficiently achieve implicit preemption similar to 1:1 threads: signal-yield and KLT-switching. Overheads of these techniques, with our optimizations, can be less than 1\% compared with nonpreemptive M:N threads. Our evaluation with three applications demonstrates that our preemption techniques for M:N threads improve core utilization and enhance the performance by utilizing lightweight context switching and flexible scheduling of M:N threads.},
  isbn = {978-1-4503-8294-6},
  keywords = {deadlock,multithreading,preemption,priority,user-level threads},
  file = {C:\Users\ron\Zotero\storage\ZDPC8EUJ\3437801.3441610.pdf}
}

@inproceedings{silvaRequirementsEngineeringProcess2019,
  title = {A {{Requirements Engineering Process}} for {{IoT Systems}}},
  booktitle = {Proceedings of the {{XVIII Brazilian Symposium}} on {{Software Quality}}},
  author = {Silva, Danyllo and Gonçalves, Taisa Guidini and da Rocha, Ana Regina C.},
  options = {useprefix=true},
  date = {2019-10-28},
  series = {{{SBQS}}'19},
  pages = {204--209},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3364641.3364664},
  url = {https://doi.org/10.1145/3364641.3364664},
  urldate = {2022-10-22},
  abstract = {Nowadays there is a great interest in IoT systems and many applications take advantage of this technology. The elicitation, specification and management of requirements for IoT systems present new challenges to requirements engineering. There is a lack of systematic approaches to the development of IoT applications and more specifically for IoT-based requirements engineering. To fill this gap this paper presents the definition of a Requirements Engineering process for IoT systems. This process is a tailored and harmonized version of the following processes of ISO IEC/IEEE 12207:2017 aiming to accomplish the needs of IoT systems: Business or Mission Analysis process, Stakeholder Needs and Requirements Definition process and System/Software Requirements Definition process.},
  isbn = {978-1-4503-7282-4},
  keywords = {Internet of Things,ISO/IEC/IEEE 12207,process,Requirements engineering},
  file = {C:\Users\ron\Zotero\storage\IJ4M4JVE\Silva et al. - 2019 - A Requirements Engineering Process for IoT Systems.pdf}
}

@article{simcoIntroductionBenchmarks,
  title = {Introduction to {{Benchmarks}}},
  author = {Simco, Dr Greg E},
  journaltitle = {Operating Systems},
  pages = {11},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\5TINXJ8S\Simco - Introduction to Benchmarks.pdf}
}

@article{simcoIntroductionSystemPerformance,
  title = {Introduction to {{System Performance Evaluation}}},
  author = {Simco, Dr Greg E},
  pages = {20},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\Y9262Y86\Simco - Introduction to System Performance Evaluation.pdf}
}

@article{simcoPerformanceEvaluationInternet2001,
  title = {Performance Evaluation and the {{Internet}} 2 Performance Initiative},
  author = {Simco, Greg},
  date = {2001-04},
  journaltitle = {The Internet and Higher Education},
  shortjournal = {The Internet and Higher Education},
  volume = {4},
  number = {2},
  pages = {125--136},
  issn = {10967516},
  doi = {10.1016/S1096-7516(01)00056-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1096751601000562},
  urldate = {2022-06-02},
  abstract = {Performance is a key factor in the development and execution of modern computer systems and networks. In order to understand the performance of a computer system, the elements of the system must be defined as well as the evaluation strategy. The Internet 2 end-to-end performance initiative is focused on performance measurement, analysis, and improvements that lead to a standard set of network capabilities and limitations. By using current Internet 2 project groups, the end-to-end performance initiative is a collaborative effort that will maximize current infrastructure performance and provide guidelines for future systems. The focal points of this initiative are: network applications, end computer and operating systems, and the supporting network. As a result of this performance initiative, a performance evaluation and review framework will provide direction for current and future network development. D 2001 Elsevier Science Inc. All rights reserved.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\E4AHMI6F\Simco - 2001 - Performance evaluation and the Internet 2 performa.pdf}
}

@inproceedings{siqueiraUsingBDDSBVR2017,
  title = {Using {{BDD}} and {{SBVR}} to {{Refine Business Goals}} into an {{Event-B Model}}: {{A Research Idea}}},
  shorttitle = {Using {{BDD}} and {{SBVR}} to {{Refine Business Goals}} into an {{Event-B Model}}},
  booktitle = {2017 {{IEEE}}/{{ACM}} 5th {{International FME Workshop}} on {{Formal Methods}} in {{Software Engineering}} ({{FormaliSE}})},
  author = {Siqueira, Fabio Levy and de Sousa, Thiago C. and Muniz Silva, Paulo S.},
  options = {useprefix=true},
  date = {2017-05},
  pages = {31--36},
  publisher = {{IEEE}},
  location = {{Buenos Aires}},
  doi = {10.1109/FormaliSE.2017.5},
  url = {https://ieeexplore.ieee.org/document/7967990/},
  urldate = {2022-10-22},
  abstract = {The transition from a requirements document to a formal specification in Event-B is usually manual and ad-hoc. In order to bridge this gap, we propose a method based on Behavior-Driven Development, an agile approach, and that uses a structured natural language conformant to the formalism of the Semantics of Business Vocabulary and Business Rules (SBVR) standard. This method will successively refine a list of high-level business goals into an Event-B model using transformations. In this paper we present our research idea, describing the steps of this method and showing an example based on the Train System scenario described by Abrial.},
  eventtitle = {2017 {{IEEE}}/{{ACM}} 5th {{International FME Workshop}} on {{Formal Methods}} in {{Software Engineering}} ({{FormaliSE}})},
  isbn = {978-1-5386-0422-9},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\GUD59MPG\Siqueira et al. - 2017 - Using BDD and SBVR to Refine Business Goals into a.pdf}
}

@book{souppayaSecureSoftwareDevelopment2022,
  title = {Secure {{Software Development Framework}} ({{SSDF}}) {{Version}} 1.1},
  author = {Souppaya, Murugiah and Scarfone, Karen and Dodson, Donna},
  date = {2022-02},
  series = {{{NIST Special Publication}}},
  volume = {800--218},
  publisher = {{NIST}},
  url = {https://doi.org/10.6028/NIST.SP.800-218},
  pagetotal = {36},
  file = {C:\Users\ron\Zotero\storage\FTIEZ9H9\NIST.SP.800-218.pdf}
}

@article{spinellisEvolutionUnixSystem2021,
  title = {Evolution of the {{Unix System Architecture}}: {{An Exploratory Case Study}}},
  shorttitle = {Evolution of the {{Unix System Architecture}}},
  author = {Spinellis, Diomidis and Avgeriou, Paris},
  date = {2021-06-01},
  journaltitle = {IEEE Transactions on Software Engineering},
  shortjournal = {IIEEE Trans. Software Eng.},
  volume = {47},
  number = {6},
  pages = {1134--1163},
  issn = {0098-5589, 1939-3520, 2326-3881},
  doi = {10.1109/TSE.2019.2892149},
  url = {https://ieeexplore.ieee.org/document/8704965/},
  urldate = {2022-05-20},
  abstract = {Unix has evolved for almost five decades, shaping modern operating systems, key software technologies, and development practices. Studying the evolution of this remarkable system from an architectural perspective can provide insights on how to manage the growth of large, complex, and long-lived software systems. Along main Unix releases leading to the FreeBSD lineage we examine core architectural design decisions, the number of features, and code complexity, based on the analysis of source code, reference documentation, and related publications. We report that the growth in size has been uniform, with some notable outliers, while cyclomatic complexity has been religiously safeguarded. A large number of Unix-defining design decisions were implemented right from the very early beginning, with most of them still playing a major role. Unix continues to evolve from an architectural perspective, but the rate of architectural innovation has slowed down over the system’s lifetime. Architectural technical debt has accrued in the forms of functionality duplication and unused facilities, but in terms of cyclomatic complexity it is systematically being paid back through what appears to be a self-correcting process. Some unsung architectural forces that shaped Unix are the emphasis on conventions over rigid enforcement, the drive for portability, a sophisticated ecosystem of other operating systems and development organizations, and the emergence of a federated architecture, often through the adoption of third-party subsystems. These findings have led us to form an initial theory on the architecture evolution of large, complex operating system software.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\DW2R6RZ9\Spinellis and Avgeriou - 2021 - Evolution of the Unix System Architecture An Expl.pdf}
}

@inproceedings{spinkEfficientAsynchronousInterrupt2016,
  title = {Efficient Asynchronous Interrupt Handling in a Full-System Instruction Set Simulator},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN}}/{{SIGBED Conference}} on {{Languages}}, {{Compilers}}, {{Tools}}, and {{Theory}} for {{Embedded Systems}}},
  author = {Spink, Tom and Wagstaff, Harry and Franke, Björn},
  date = {2016-06-13},
  series = {{{LCTES}} 2016},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2907950.2907953},
  url = {https://doi.org/10.1145/2907950.2907953},
  urldate = {2022-06-13},
  abstract = {Instruction set simulators (ISS) have many uses in embedded software and hardware development and are typically based on dynamic binary translation (DBT), where frequently executed regions of guest instructions are compiled into host instructions using a just-in-time (JIT) compiler. Full-system simulation, which necessitates handling of asynchronous interrupts from e.g. timers and I/O devices, complicates matters as control flow is interrupted unpredictably and diverted from the current region of code. In this paper we present a novel scheme for handling of asynchronous interrupts, which integrates seamlessly into a region-based dynamic binary translator. We first show that our scheme is correct, i.e. interrupt handling is not deferred indefinitely, even in the presence of code regions comprising control flow loops. We demonstrate that our new interrupt handling scheme is efficient as we minimise the number of inserted checks. Interrupt handlers are also presented to the JIT compiler and compiled to native code, further enhancing the performance of our system. We have evaluated our scheme in an ARM simulator using a region-based JIT compilation strategy. We demonstrate that our solution reduces the number of dynamic interrupt checks by 73\%, reduces interrupt service latency by 26\% and improves throughput of an I/O bound workload by 7\%, over traditional per-block schemes.},
  isbn = {978-1-4503-4316-9},
  keywords = {Asynchronous interrupt handling,Dynamic binary translation,Full system simulation,Region-based just-in-time compilation},
  file = {C:\Users\ron\Zotero\storage\AW94KDZ6\2907950.2907953.pdf}
}

@inproceedings{srinivasanFirmRealtimeSystem1998,
  title = {A Firm Real-Time System Implementation Using Commercial off-the-Shelf Hardware and Free Software},
  booktitle = {Proceedings. {{Fourth IEEE Real-Time Technology}} and {{Applications Symposium}} ({{Cat}}. {{No}}.{{98TB100245}})},
  author = {Srinivasan, B. and Pather, S. and Hill, R. and Ansari, F. and Niehaus, D.},
  date = {1998-06},
  pages = {112--119},
  doi = {10.1109/RTTAS.1998.683194},
  abstract = {The emergence of multimedia and high-speed networks has expanded the class of applications that combine the timing requirements of hard real-time applications with the need for operating system services typically available only on soft-real time or time-sharing systems. These applications, which the authors describe as firm real-time, currently have no widely-available, low-cost operating system to support them. They discuss modifications they have made to the popular Linux operating system that give it the ability to support the comparatively stringent timing requirements of these applications, while still giving them access to the full range of Linux services. Using their firm real-time system as a basis, they have developed the ATM Reference Traffic System (ARTS) that is capable of recording and accurately reproducing packet-level ATM traffic streams with timing resolution in microseconds. The effectiveness of this application, as well as the comparative ease with which it was developed illustrate the performance and utility of the system.},
  eventtitle = {Proceedings. {{Fourth IEEE Real-Time Technology}} and {{Applications Symposium}} ({{Cat}}. {{No}}.{{98TB100245}})},
  keywords = {Application software,Costs,Displays,Hardware,Linux,Multimedia systems,Operating systems,Real time systems,Subspace constraints,Timing},
  file = {C:\Users\ron\Zotero\storage\UPCJMVFD\A_firm_real-time_system_implementation_using_commercial_off-the-shelf_hardware_and_free_software.pdf}
}

@inproceedings{straubSoftwareEngineeringFirst2020,
  title = {Software {{Engineering}}: {{The First Line}} of {{Defense}} for {{Cybersecurity}}},
  author = {Straub, Jeremy},
  date = {2020-10-16},
  pages = {1--5},
  location = {{Beijing, China}},
  doi = {10.1109/ICSESS49938.2020.9237715},
  eventtitle = {{{IEEE}} 11th {{International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  file = {C:\Users\ron\Zotero\storage\Q8RNBV3N\Software_Engineering_The_First_Line_of_Defense_for_Cybersecurity.pdf}
}

@inproceedings{suoCharacterizingNetworkingPerformance2021,
  title = {Characterizing Networking Performance and Interrupt Overhead of Container Overlay Networks},
  booktitle = {Proceedings of the 2021 {{ACM Southeast Conference}}},
  author = {Suo, Kun and Shi, Yong and Lee, Ahyoung and Baidya, Sabur},
  date = {2021-04-15},
  series = {{{ACM SE}} '21},
  pages = {93--99},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3409334.3452040},
  url = {https://doi.org/10.1145/3409334.3452040},
  urldate = {2022-06-13},
  abstract = {Containers, an emerging service to manage and deploy applications into isolated boxes, are quickly increasing in popularity in the cloud and edge computing. In order to provide connectivity among multiple hosts, cloud providers adopt overlay networks, which not only impose significant overhead in throughput and latency in containerized applications, but also consume more CPU resources of the system. Through profiling and code analysis, this paper reveals that the overwhelming interrupts, as well as its load imbalance in the kernel processing contribute to the inefficiency of the container overlay networks. Specifically, every packet in container networks might raise multiple software interrupts compared to that in VM networks. Our results indicate that the container network throughput drops 2/3 and the tail latency increases more than 37 times if the interrupt overhead is not well optimized.},
  isbn = {978-1-4503-8068-3},
  keywords = {container,interrupt,network,overlay,performance},
  file = {C\:\\Users\\ron\\Zotero\\storage\\37NWZ5AG\\pldi22main-p152-p-archive.zip;C\:\\Users\\ron\\Zotero\\storage\\6MHGBXZU\\3409334.3452040.pdf}
}

@inreference{SwadeshList,
  title = {Swadesh {{List}}},
  booktitle = {Wikipedia},
  url = {https://en.wikipedia.org/wiki/Swadesh_list}
}

@book{tambouratzisMachineTranslationMinimal2017,
  title = {Machine {{Translation}} with {{Minimal Reliance}} on {{Parallel Resources}}},
  author = {Tambouratzis, George and Sofianopoulos, Sokratis and Vassiliou, Marina},
  date = {2017},
  series = {{{SpringerBriefs}} in {{Statistics}}},
  edition = {1st ed. 2017},
  publisher = {{Springer International Publishing : Imprint: Springer}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-63107-3},
  abstract = {This book provides a unified view on a new methodology for Machine Translation (MT). This methodology extracts information from widely available resources (extensive monolingual corpora) while only assuming the existence of a very limited parallel corpus, thus having a unique starting point to Statistical Machine Translation (SMT). In this book, a detailed presentation of the methodology principles and system architecture is followed by a series of experiments, where the proposed system is compared to other MT systems using a set of established metrics including BLEU, NIST, Meteor and TER. Additionally, a free-to-use code is available, that allows the creation of new MT systems. The volume is addressed to both language professionals and researchers. Prerequisites for the readers are very limited and include a basic understanding of the machine translation as well as of the basic tools of natural language processing},
  isbn = {978-3-319-63107-3},
  pagetotal = {1},
  keywords = {Mathematical statistics,Natural language processing (Computer science),Natural Language Processing (NLP),Pattern recognition,Pattern Recognition,Probability and Statistics in Computer Science,Statistics,Statistics and Computing/Statistics Programs},
  file = {C:\Users\ron\Zotero\storage\89MWMKFY\Tambouratzis et al. - 2017 - Machine Translation with Minimal Reliance on Paral.pdf}
}

@artwork{thomasMajorLevelsLinguistic2010,
  title = {Major Levels of Linguistic Structure. {{Morphology}} Is Shown Encompassed by Syntax, and Encompassing Phonology.},
  author = {Thomas, James J},
  editora = {Cook, Kristin A. and McSush},
  editoratype = {collaborator},
  date = {2010-01-03},
  url = {https://en.wikipedia.org/wiki/Morphology_(linguistics)#/media/File:Major_levels_of_linguistic_structure.svg}
}

@software{thompsonPolyGlot,
  title = {{{PolyGlot}}},
  author = {Thompson, Draque},
  url = {https://github.com/DraqueT/PolyGlot},
  abstract = {Welcome to PolyGlot, an open source tool set that helps to build constructed languages! PolyGlot is a project that I have been working on for some time, and it continues to become more stable and capable with each release. If you have ideas, find bugs, or have any comments at all, please feel free to email me at draquemail@gmail.com or post them on PolyGlot's issues page!}
}

@inproceedings{togashiConcurrencyGoJava2014,
  title = {Concurrency in {{Go}} and {{Java}}: {{Performance}} Analysis},
  shorttitle = {Concurrency in {{Go}} and {{Java}}},
  booktitle = {2014 4th {{IEEE International Conference}} on {{Information Science}} and {{Technology}}},
  author = {Togashi, Naohiro and Klyuev, Vitaly},
  date = {2014-04},
  pages = {213--216},
  issn = {2164-4357},
  doi = {10.1109/ICIST.2014.6920368},
  abstract = {Go is a new programming language developed by Google. Although it is still young compared to other programming languages, it already has modern and powerful features inherited from existing programming languages, and some of these are similar to Java. Go is designed for quick time development. Concurrency is the one of the main its features. In this paper, we analyze the performance of Go, and compare it with Java from two aspects: compile time and concurrency. There are many studies about the performance analysis and comparison of programming languages, but only a few publications investigate Go. Some of Go performance evaluation are based on the experimental release of Go. To analyze concurrency features, we implement simple matrix multiplication programs in both Go and Java. Java implementation uses Java Thread, and Go implementation uses Gor-outine and Channel. From the experiment, Go derived better performance than Java in both compile time and concurrency. Moreover, Go code shows the ease of concurrent programming. Go is still young, but we are convinced that Go will become the mainstream.},
  eventtitle = {2014 4th {{IEEE International Conference}} on {{Information Science}} and {{Technology}}},
  keywords = {Benchmark testing,Concurrency,Concurrent computing,Educational institutions,Evaluation,Go,Java,Message systems,Performance,Programming},
  file = {C:\Users\ron\Zotero\storage\C5VXVFZQ\Concurrency_in_Go_and_Java_Performance_analysis.pdf}
}

@report{tommilaControlledNaturalLanguage2014,
  title = {Controlled Natural Language Requirements in the Design and Analysis of Safety Critical {{I}}\&{{C}} Systems},
  author = {Tommila, Teemu and Pakonen, Antti},
  date = {2014-02-26},
  institution = {{SAFIR2014 Reference group 2}},
  location = {{Espoo, Finland}},
  abstract = {In order to be effectively communicated to all relevant stakeholders, requirements need to be easily understandable, unambiguous, and verifiable. This research report studies the possibilities of textual requirement templates in specifying and analysing safety critical instrumentation and control systems. We use a Controlled Natural Language (CNL) in the shape of a restricted vocabulary and templates for “standardised” requirement statements, where terms of the particular application can be filled-in. While the template-based approach applies to systems engineering in general, our particular interest is in model checking, a formal method to exhaustively show that a system model fulfils its stated functional requirements. Model checking is based on strict requirement formalisation using temporal logic languages, which calls for specific expertise. With natural language templates, oft-used requirement constructs can be re-used without having to work with complex temporal logic expressions. In this report we first discuss the theoretical foundations of system modelling and natural language constructs, and then review related research on methods and tools. On the basis of the literature and VTT’s previous experiences in model checking, we suggest a first set of templates intended for model checking function block based control applications and list desired features for a requirement authoring tool based on the templates. Topics for further research include processing of chains of events that are too complex for practical expression in writing, as well as mechanisms for integrating the suggested tool concept into existing model checking, system development, and requirement management tools.},
  file = {C:\Users\ron\Zotero\storage\9EMT3XCP\VTT-R-01067-14.pdf}
}

@online{traillClickLanguagesClicks,
  title = {Click Languages | {{Clicks}}, {{Khoisan}}, {{Bushmen}} | {{Britannica}}},
  author = {Traill, Anthony},
  url = {https://www.britannica.com/topic/click-languages},
  urldate = {2023-10-31},
  abstract = {Click languages, a group of languages found only in Africa in which clicks function as normal consonants. The sole report outside Africa of a language using clicks involves the special case of Damin, a ritual vocabulary of the Lardil of northern Queensland, Australia. While clicks are an extensive},
  langid = {english}
}

@online{u/seduConlangConstructionSoftware2009,
  type = {Reddit thread},
  title = {Conlang {{Construction Software}}?},
  author = {{u/Sedu}},
  date = {2009-12-16},
  url = {https://www.reddit.com/r/conlangs/comments/2278s5/conlang_construction_software/},
  organization = {{Reddit r/conlangs}}
}

@online{uclpsychology&languagesciencesSAMPAComputerReadable,
  title = {{{SAMPA}} - Computer Readable Phonetic Alphabet},
  author = {{UCL Psychology \& Language Sciences}},
  url = {https://www.phon.ucl.ac.uk/home/sampa/index.html},
  abstract = {SAMPA (Speech Assessment Methods Phonetic Alphabet) is a machine-readable phonetic alphabet. It was originally developed under the ESPRIT project 1541, SAM (Speech Assessment Methods) in 1987-89 by an international group of phoneticians, and was applied in the first instance to the European Communities languages Danish, Dutch, English, French, German, and Italian (by 1989); later to Norwegian and Swedish (by 1992); and subsequently to Greek, Portuguese, and Spanish (1993). Under the BABEL project, it has now been extended to Bulgarian, Estonian, Hungarian, Polish, and Romanian (1996). Under the aegis of COCOSDA it is hoped to extend it to cover many other languages (and in principle all languages). On the initiative of the OrienTel project, Arabic, Hebrew, and Turkish have been added. Other recent additions: Cantonese, Croatian, Czech, Russian, Slovenian, Thai.}
}

@article{unitedstatescybersecurityandinfrastructuresecurityagencyCaseMemorySafe2023,
  title = {The {{Case}} for {{Memory Safe Roadmaps}}},
  author = {{United States Cybersecurity and Infrastructure Security Agency} and {United States National Security Agency} and {United States Federal Bureau of Investigation} and {Australian Signals Directorate’s Australian Cyber Security Centre} and {Canadian Centre for Cyber Security} and {United Kingdom National Cyber Security Centre} and {New Zealand National Cyber Security Centre} and {Computer Emergency Response Team New Zealand}},
  date = {2023-12},
  url = {https://www.cisa.gov/sites/default/files/2023-12/The-Case-for-Memory-Safe-Roadmaps-508c.pdf?is=a58931661cb3480751fa50fd2e230ca1a136aba33b45ae1f93e1bd1d11674995},
  abstract = {Memory safety vulnerabilities are the most prevalent type of disclosed software vulnerability. They are a class of well-known and common coding errors that malicious actors routinely exploit. These vulnerabilities represent a major problem for the software industry as they cause manufacturers to continually release security updates and their customers to continually patch. These vulnerabilities persist despite software manufacturers historically expending significant resources attempting to reduce their prevalence and impact through various methods, including analyzing, patching, publishing new code and investing in training programs for developers. Customer organizations expend significant resources responding to these vulnerabilities through onerous patch management programs and incident response activities. Memory safe programming languages (MSLs) can eliminate memory safety vulnerabilities. Therefore, transitioning to MSLs would likely greatly lessen the need to invest in activities aimed at reducing these vulnerabilities or minimizing their impact. Additionally, investments to migrate unsafe codebases to MSLs would pay long-term dividends in the form of safer products—defraying some of the upfront cost of transitioning to MSLs. The U.S. Cybersecurity and Infrastructure Security Agency (CISA), National Security Agency (NSA), Federal Bureau of Investigation (FBI), and the cybersecurity authorities of Australia, Canada, the United Kingdom, and New Zealand  (hereafter referred to as the authoring agencies) jointly developed this guidance as part of our collective Secure by Design campaign. With this guidance, the authoring agencies urge senior executives at every software manufacturer to reduce customer risk by prioritizing design and development practices that implement MSLs. Additionally, the agencies urge software manufacturers to create and publish memory safe roadmaps that detail how they will eliminate memory safety vulnerabilities in their products. By publishing memory safe roadmaps, manufacturers will signal to customers that they are taking ownership of security outcomes, embracing radical transparency, and taking a top-down approach to developing secure products—key Secure by Design tenets. This guidance provides manufacturers with steps to create memory safe roadmaps and implement changes to eliminate memory safety  vulnerabilities from their products. Eliminating this vulnerability class should be seen as a business imperative likely requiring participation from many departments. The authoring agencies urge executives to lead from the top by publicly identifying senior staff who will drive publication of their roadmap and assist with realigning resources as needed.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\9RBDC7Z7\The Case for Memory Safe Roadmaps.pdf}
}

@misc{unitedstatesdepartmentofdefenseDepartmentDefenceInterface2018,
  title = {Department {{Of Defence Interface Standard Digital Time Division Command}}/{{Response Multiplex Data Bus}}},
  shorttitle = {{{MIL-STD-1553C}}},
  author = {{United States Department of Defense}},
  date = {2018-02-28},
  abstract = {This standard establishes requirements for digital, command/response, time division multiplexing (data bus) techniques. It encompasses the data bus line and its interface electronics, and also defines the concept of operation and information flow on the multiplex data bus and the electrical and functional formats to be employed.},
  organization = {{United States Department of Defense}},
  file = {C:\Users\ron\Zotero\storage\2HT2C29K\United States Department of Defense - 2018 - DEPARTMENT OF DEFENSE INTERFACE STANDARD DIGITAL T.pdf}
}

@incollection{vallina-rodriguezCaseContextAwareResources2012,
  title = {The {{Case}} for {{Context-Aware Resources Management}} in {{Mobile Operating Systems}}},
  booktitle = {Mobile {{Context Awareness}}},
  author = {Vallina-Rodriguez, Narseo and Crowcroft, Jon},
  editor = {Lovett, Tom and O'Neill, Eamonn},
  date = {2012},
  pages = {97--113},
  publisher = {{Springer London}},
  location = {{London}},
  doi = {10.1007/978-0-85729-625-2_6},
  url = {http://link.springer.com/10.1007/978-0-85729-625-2_6},
  urldate = {2022-05-15},
  abstract = {Efficient management of mobile resources from an energy perspective in modern smart-phones is paramount nowadays. Today’s mobile phones are equipped with a wide range of sensing, computational, storage and communication resources. The diverse range of sensors such as microphones, cameras, accelerometers, gyroscopes, GPS, digital compass and proximity sensors allow mobile apps to be context-aware whereas the ability to have connectivity almost everywhere has bootstrapped the birth of rich and interactive mobile applications and the integration of cloud services. However, the intense use of those resources can easily be translated into power-hungry applications. The way users interact with their mobile handsets and the availability of mobile resources is context dependent. Consequently, understanding how users interact with their applications and integrating context-aware resources management techniques in the core features of a mobile operating system can provide benefits such as energy savings and usability. This chapter describes how context drives the way users interact with their handsets and how it determines the availability and state of hardware resources in order to explain different contextaware resources management systems and the different attempts to incorporate this feature in mobile operating systems.},
  isbn = {978-0-85729-624-5 978-0-85729-625-2},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\9HTM24Z4\Vallina-Rodriguez and Crowcroft - 2012 - The Case for Context-Aware Resources Management in.pdf}
}

@inproceedings{vanlamsweerdeObjectOrientationGoal2004,
  title = {From {{Object Orientation}} to {{Goal Orientation}}: {{A Paradigm Shift}} for {{Requirements Engineering}}},
  shorttitle = {From {{Object Orientation}} to {{Goal Orientation}}},
  booktitle = {Radical {{Innovations}} of {{Software}} and {{Systems Engineering}} in the {{Future}}},
  author = {van Lamsweerde, Axel and Letier, Emmanuel},
  editor = {Wirsing, Martin and Knapp, Alexander and Balsamo, Simonetta},
  options = {useprefix=true},
  date = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {325--340},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24626-8_23},
  abstract = {Requirements engineering (RE) is concerned with the elicitation of the objectives to be achieved by the system envisioned, the operationalization of such objectives into specifications of services and constraints, the assignment of responsibilities for the resulting requirements to agents such as humans, devices and software, and the evolution of such requirements over time and across system families. Getting high-quality requirements is difficult and critical. Recent surveys have confirmed the growing recognition of RE as an area of primary concern in software engineering research and practice. The paper reviews the important limitations of OO modeling and formal specification technology when applied to this early phase of the software lifecycle. It argues that goals are an essential abstraction for eliciting, elaborating, modeling, specifying, analyzing, verifying, negotiating and documenting robust and conflict-free requirements. A safety injection system for a nuclear power plant is used as a running example to illustrate the key role of goals while engineering requirements for high assurance systems.},
  isbn = {978-3-540-24626-8},
  langid = {english},
  keywords = {Goal-oriented requirements engineering,high assurance systems,lightweight formal methods,safety,specification building process},
  file = {C:\Users\ron\Zotero\storage\WFISWEDD\van Lamsweerde and Letier - 2004 - From Object Orientation to Goal Orientation A Par.pdf}
}

@article{vardaDatadrivenCrosslingualSyntax2023,
  title = {Data-Driven {{Cross-lingual Syntax}}: {{An Agreement Study}} with {{Massively Multilingual Models}}},
  shorttitle = {Data-Driven {{Cross-lingual Syntax}}},
  author = {Varda, Andrea Gregor De and Marelli, Marco},
  date = {2023-06-01},
  journaltitle = {Computational Linguistics},
  volume = {49},
  number = {2},
  pages = {261--299},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00472},
  url = {https://direct.mit.edu/coli/article/49/2/261/114515/Data-driven-Cross-lingual-Syntax-An-Agreement},
  urldate = {2023-08-14},
  abstract = {Abstract              Massively multilingual models such as mBERT and XLM-R are increasingly valued in Natural Language Processing research and applications, due to their ability to tackle the uneven distribution of resources available for different languages. The models’ ability to process multiple languages relying on a shared set of parameters raises the question of whether the grammatical knowledge they extracted during pre-training can be considered as a data-driven cross-lingual grammar. The present work studies the inner workings of mBERT and XLM-R in order to test the cross-lingual consistency of the individual neural units that respond to a precise syntactic phenomenon, that is, number agreement, in five languages (English, German, French, Hebrew, Russian). We found that there is a significant overlap in the latent dimensions that encode agreement across the languages we considered. This overlap is larger (a) for long- vis-à-vis short-distance agreement and (b) when considering XLM-R as compared to mBERT, and peaks in the intermediate layers of the network. We further show that a small set of syntax-sensitive neurons can capture agreement violations across languages; however, their contribution is not decisive in agreement processing.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\SXCGK96X\coli_a_00472.pdf}
}

@inproceedings{vasilevSelectiveProcessInstrumentation2016,
  title = {Selective Process Instrumentation in Virtual Machine},
  booktitle = {Companion {{Proceedings}} of the 15th {{International Conference}} on {{Modularity}}},
  author = {Vasilev, Ivan},
  date = {2016-03-14},
  series = {{{MODULARITY Companion}} 2016},
  pages = {20},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2892664.2892703},
  url = {https://doi.org/10.1145/2892664.2892703},
  urldate = {2022-06-20},
  abstract = {System instrumentation is widely used in different analyze tools, but they are usually hard to maintain and rather slow. To solve this problems we present our approach to instrumentation with multi-leveled plugins and instrumentation of individual processes.},
  isbn = {978-1-4503-4033-5},
  keywords = {Process instrumentation,related plugins,virtual machine},
  file = {C:\Users\ron\Zotero\storage\ULA2YKX5\Vasilev - 2016 - Selective process instrumentation in virtual machi.pdf}
}

@inproceedings{vernerPredictingGoodRequirements2006,
  title = {Predicting Good Requirements for In-House Development Projects},
  booktitle = {Proceedings of the 2006 {{ACM}}/{{IEEE}} International Symposium on {{Empirical}} Software Engineering},
  author = {Verner, June and Cox, Karl and Bleistein, Steven J.},
  date = {2006-09-21},
  series = {{{ISESE}} '06},
  pages = {154--163},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1159733.1159758},
  url = {https://doi.org/10.1145/1159733.1159758},
  urldate = {2022-06-09},
  abstract = {We surveyed software practitioners regarding software development practices used during recent projects. Five categories of questions broadly related to requirements were asked: the sponsor, customer/users, requirements issues, the project manager and project management, and the development process. Relationships between project factors and good requirements were investigated. We developed requirements prediction equations by dividing our response data into two data sets. Using binary logistic regression on each set we tested the equations developed. Such analysis provides us with insight into which variables are significant predictors of good requirements. The best predictors were 1) the customers/users had a high level of confidence in the development team, with 2) risks were controlled and managed by the project manager.},
  isbn = {978-1-59593-218-1},
  keywords = {management,prediction,requirements,risk management},
  file = {C:\Users\ron\Zotero\storage\QEBJJM9F\1159733.1159758.pdf}
}

@inproceedings{vidalSysMLToolRequirements2018,
  title = {{{SysML}} as a {{Tool}} for {{Requirements Traceability}} in {{Mechatronic Design}}},
  booktitle = {Proceedings of the 2018 4th {{International Conference}} on {{Mechatronics}} and {{Robotics Engineering}}},
  author = {Vidal, Enrique J. and Villota, Elizabeth R.},
  date = {2018-02-07},
  series = {{{ICMRE}} 2018},
  pages = {146--152},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3191477.3191494},
  url = {https://doi.org/10.1145/3191477.3191494},
  urldate = {2022-10-22},
  abstract = {Modern mechatronic systems have to deal with the ever increasing requirements due to a higher number of demands and their complexity. Those demands are summarized in the requirements list. Classic design methodologies do not directly relate a system or part of it to the requirements list. As a result, by failing to ensure that the qualification of requirements is traceable, when a problem arises, design engineers lose the ability to track back and quickly locate one or more not fulfilled requirements. Our work shows how requirements traceability for mechatronic design can be achieved by employing a Model-Based Systems Engineering (MBSE) methodology and associated tool, Systems Modeling Language (SysML). SysML is a general purpose multiview language for systems modeling capable of linking requirements to the system elements by capturing textual requirements and placing them in the design models. Furthermore, SysML can be coupled to other tools including spreadsheets and design and simulation software, such as Matlab or Modelica, enabling requirements verification. An electro-mechanical actuator (EMA) -an airplane surface actuatoris chosen as a mechatronic system example. By joining SysML and Matlab/Simulink, it was possible to trace requirements for the EMA mechatronic design and hence verify compliance with requirements throughout the design.},
  isbn = {978-1-4503-6365-5},
  keywords = {Mechatronic Design,Requirements,SysML,Traceability},
  file = {C:\Users\ron\Zotero\storage\VGRH3SPI\Vidal and Villota - 2018 - SysML as a Tool for Requirements Traceability in M.pdf}
}

@article{vilelaRetrospectiveAnalysisSAC2016,
  title = {A Retrospective Analysis of {{SAC}} Requirements: Engineering Track},
  shorttitle = {A Retrospective Analysis of {{SAC}} Requirements},
  author = {Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno},
  date = {2016-08-29},
  journaltitle = {ACM SIGAPP Applied Computing Review},
  shortjournal = {SIGAPP Appl. Comput. Rev.},
  volume = {16},
  number = {2},
  pages = {26--41},
  issn = {1559-6915},
  doi = {10.1145/2993231.2993234},
  url = {https://doi.org/10.1145/2993231.2993234},
  urldate = {2022-10-22},
  abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
  keywords = {relevance,requirements engineering,retrospective,SAC,scoping study,symposium on applied computing,systematic mapping study,trends},
  file = {C:\Users\ron\Zotero\storage\76PZUDTW\Vilela et al. - 2016 - A retrospective analysis of SAC requirements engi.pdf}
}

@inproceedings{vishwakarmaPortingSystematicTesting2014,
  title = {Porting and Systematic Testing of an Embedded {{RTOS}}},
  booktitle = {International {{Conference}} on {{Computing}} and {{Communication Technologies}}},
  author = {Vishwakarma, Anil K and Suresh, K.V and Singh, U.K},
  date = {2014-12},
  pages = {1--4},
  doi = {10.1109/ICCCT2.2014.7066752},
  abstract = {The use of a Real-Time Operating System (RTOS) is now quite common in embedded systems because of the requirement for multi-tasking. We present the details of our port of the popular MicroC/OS-II RTOS to an avionics platform based on a PowerPC 7410 processor with multiple peripherals including RS422, MIL1553, digital IO and timers. Also, the systematic testing that we have carried out to validate the port is also presented. Further, detailed performance measurements have been carried out, and we also compare the performance of our MicroC/OS-II port with that of RT-Linux on the same platform.},
  eventtitle = {International {{Conference}} on {{Computing}} and {{Communication Technologies}}},
  keywords = {Assembly,Context,Embedded,MicroC/OS,Porting,Ports (Computers),Real-time systems,Registers,RTOS,Switches,Testing},
  file = {C\:\\Users\\ron\\Zotero\\storage\\JPTS87NH\\Vishwakarma et al. - 2014 - Porting and systematic testing of an embedded RTOS.pdf;C\:\\Users\\ron\\Zotero\\storage\\8T2KXXHF\\7066752.html}
}

@inproceedings{wakankarExperienceTestingRigorous2013,
  title = {Experience with Testing and Rigorous Program Analysis for Qualification of {{RTOS}}},
  booktitle = {Proceedings of the 6th {{India Software Engineering Conference}}},
  author = {Wakankar, A. and Khan, Arindam and Aravamuthan, G. and Bhattacharjee, A. K.},
  date = {2013-02-21},
  series = {{{ISEC}} '13},
  pages = {83--89},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2442754.2442767},
  url = {https://doi.org/10.1145/2442754.2442767},
  urldate = {2022-06-07},
  abstract = {Real Time Operating System (RTOS) is a critical component of embedded systems. International standards such as IEC60880 used for development of Instrumentation and Control (I\&C) system in nuclear power plants require rigorous qualification of all software components. In this paper, we describe our experience in qualification of ESOS; an in-house configured RTOS from a commercial RTOS available with source code. The qualification activities include static \& dynamic analysis, timing analysis and rigorous program analysis. We discuss how rigorous program analysis was used to uncover a subtle bug in the implementation.},
  isbn = {978-1-4503-1987-4},
  keywords = {program analysis,program testing and analysis,RTOS,safety qualification,testing},
  file = {C:\Users\ron\Zotero\storage\IGZ6BGN3\2442754.2442767.pdf}
}

@online{WatchLiveInterrupts,
  title = {Watch {{Live Interrupts}} | {{Linux Journal}}},
  url = {https://www.linuxjournal.com/content/watch-live-interrupts},
  urldate = {2022-06-14}
}

@video{watkinsConlangingArtCrafting2017,
  type = {Documentary},
  entrysubtype = {film},
  title = {Conlanging: {{The Art}} of {{Crafting Tongues}}},
  editor = {Watkins, Britton},
  editortype = {director},
  editora = {Peterson, David J. and Schreyer, Christine and Salo, David and Frommer, Paul and Okrand, Marc},
  editoratype = {collaborator},
  date = {2017-08},
  abstract = {A deep dive into the hidden world of constructed languages and the fascinating people who make them.},
  langid = {english}
}

@online{watt2013,
  author = {Watt, Simon},
  date = {2013-07},
  url = {https://docs.oracle.com/database/121/SQPUG/title.htm},
  organization = {{SQL*Plus® User's Guide and Reference}}
}

@inbook{wells-jensenExtraterrestrialMessageConstruction2020,
  title = {Extraterrestrial Message Construction: {{Guidelines}} for the Use of Xenolinguistics in the Classroom},
  shorttitle = {Extraterrestrial Message Construction},
  booktitle = {Language {{Invention}} in {{Linguistics Pedagogy}}},
  author = {Wells-Jensen, Sheri and Spallinger, Kimberly},
  date = {2020-08-12},
  pages = {239--250},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oso/9780198829874.003.0014},
  url = {https://academic.oup.com/book/31973/chapter/267718236},
  urldate = {2023-08-24},
  abstract = {This chapter presents a set of exercises ready for use in the classroom, in which students use basic pattern recognition skills to solve an “alien” message that includes both numbers and other elements. The other elements define relations among the numbers, and students determine how those relations can be mapped to words in a human language—whatever the language of instruction. Rationale for the utility of this approach is discussed, as are ways to modify the instructions to present the exercises to student populations with different levels of sophistication in math and computation, and different levels of linguistic training. The exercise and rationale exemplify one way in which constructed languages can be used to introduce key concepts in linguistics for students from a variety of academic disciplines.},
  bookauthor = {Wells-Jensen, Sheri and Spallinger, Kimberly},
  isbn = {978-0-19-882987-4 978-0-19-186835-1},
  langid = {english}
}

@standard{wellsComputercodingIPAProposed2016,
  title = {Computer-Coding the {{IPA}}: A Proposed Extension of {{SAMPA}}},
  author = {Wells},
  date = {2016-03-16},
  file = {C:\Users\ron\Zotero\storage\ZM52CCNM\Wells - 2016 - Computer-coding the IPA a proposed extension of S.pdf}
}

@online{WhatCybersecurityCISA2021,
  title = {What Is {{Cybersecurity}}? | {{CISA}}},
  shorttitle = {What Is {{Cybersecurity}}?},
  date = {2021-02-01},
  url = {https://www.cisa.gov/news-events/news/what-cybersecurity},
  urldate = {2023-05-22},
  abstract = {Defending yourself against cyberattacks starts with understanding the risks associated with cyber activity, what some of the basic cybersecurity terms mean, and},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\ZSN4EYIE\what-cybersecurity.html}
}

@inproceedings{wheelerQthreadsAPIProgramming2008,
  title = {Qthreads: {{An API}} for Programming with Millions of Lightweight Threads},
  shorttitle = {Qthreads},
  booktitle = {2008 {{IEEE International Symposium}} on {{Parallel}} and {{Distributed Processing}}},
  author = {Wheeler, Kyle B. and Murphy, Richard C. and Thain, Douglas},
  date = {2008-04},
  pages = {1--8},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2008.4536359},
  abstract = {Large scale hardware-supported multithreading, an attractive means of increasing computational power, benefits significantly from low per-thread costs. Hardware support for lightweight threads is a developing area of research. Each architecture with such support provides a unique interface, hindering development for them and comparisons between them. A portable abstraction that provides basic lightweight thread control and synchronization primitives is needed. Such an abstraction would assist in exploring both the architectural needs of large scale threading and the semantic power of existing languages. Managing thread resources is a problem that must be addressed if massive parallelism is to be popularized. The qthread abstraction enables development of large-scale multithreading applications on commodity architectures. This paper introduces the qthread API and its Unix implementation, discusses resource management, and presents performance results from the HPCCG benchmark.},
  eventtitle = {2008 {{IEEE International Symposium}} on {{Parallel}} and {{Distributed Processing}}},
  keywords = {Computer architecture,Costs,Hardware,Laboratories,Large-scale systems,Multithreading,Parallel processing,Programming profession,Resource management,Yarn},
  file = {C:\Users\ron\Zotero\storage\TDV5F5WK\Qthreads_An_API_for_programming_with_millions_of_lightweight_threads.pdf}
}

@inproceedings{wibowoRequirementsTraceabilityOntology2020,
  title = {Requirements {{Traceability Ontology}} to {{Support Requirements Management}}},
  booktitle = {Proceedings of the {{Australasian Computer Science Week Multiconference}}},
  author = {Wibowo, Adi and Davis, Joseph},
  date = {2020-02-04},
  series = {{{ACSW}} '20},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3373017.3373038},
  url = {https://doi.org/10.1145/3373017.3373038},
  urldate = {2022-10-22},
  isbn = {978-1-4503-7697-6},
  keywords = {Requirement traceability,requirements management,traceability ontology},
  file = {C:\Users\ron\Zotero\storage\3BQWAX3F\Wibowo and Davis - 2020 - Requirements Traceability Ontology to Support Requ.pdf}
}

@online{windows-driver-contentWindowsPerformanceToolkit,
  title = {Windows {{Performance Toolkit}}},
  author = {{windows-driver-content}},
  url = {https://docs.microsoft.com/en-us/windows-hardware/test/wpt/},
  urldate = {2022-06-14},
  abstract = {Included in the Windows Assessment and Deployment Kit, the Windows Performance Toolkit consists of performance monitoring tools that produce in-depth performance profiles of Windows operating systems and applications. (index)},
  langid = {american}
}

@article{winkfieldStudyEvolutionSecure2018,
  title = {A {{Study}} of the {{Evolution}} of {{Secure Software Development Architectures}}},
  author = {Winkfield, Leah and Hu, Yen-Hung and Hoppa, Mary Ann},
  date = {2018-08-30},
  journaltitle = {Journal of The Colloquium for Information Systems Security Education},
  volume = {6},
  number = {1},
  pages = {19--19},
  issn = {2641-4554},
  url = {https://cisse.info/journal/index.php/cisse/article/view/86},
  urldate = {2022-06-20},
  abstract = {Emerging technologies such as containers, microservices, DevOps, Agile software development life cycle (SDLC), and cloud-native applications have gained popularity and traction in the industry and among enterprises. These modern application technologies and architectures are being adopted because they enable greater flexibility, scalability, portability and more rapid development. Consequently, how to build and maintain secure applications and systems is being reevaluated. Since the total responsibility is now larger and more complex, the application developer role is expanding to include greater security obligations and concerns. This paper explores the evolution of software development architectures and consequent implications on security, to better understand the technology landscape driving this change and its impacts on application development. To remain competitive, organization must be prepared to invest in ongoing training of their developers in the latest best practices. To remain relevant, higher education must adapt curriculums to prepare future professionals in the appropriate cybersecurity and secure coding practices to match the development shifts observed in industry.},
  langid = {english},
  keywords = {Agile SDLC,Container,DevOps,Microservice,Secure Software Development},
  file = {C:\Users\ron\Zotero\storage\KJ4HNNI7\Winkfield2018.pdf}
}

@inproceedings{wnukAgileLeanMetrics2017,
  title = {Agile and Lean Metrics Associated with Requirements Engineering},
  booktitle = {Proceedings of the 27th {{International Workshop}} on {{Software Measurement}} and 12th {{International Conference}} on {{Software Process}} and {{Product Measurement}}},
  author = {Wnuk, Krzysztof and Maddila, Kalyan Chakravarthy},
  date = {2017-10-25},
  series = {{{IWSM Mensura}} '17},
  pages = {33--40},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3143434.3143437},
  url = {https://doi.org/10.1145/3143434.3143437},
  urldate = {2022-10-22},
  abstract = {Despite the continuously increasing importance of Agile and Lean in software development, the number of studies that investigate Requirements Engineering (RE) related aspects remains low. In this paper, we report the results from a literature review about Agile and Lean requirements engineering. By performing a systematic mapping literature review, we identified 22 metrics in 18 publications. We analyzed the identified papers based on research methodology, rigor and relevance and other external attributes. We also map the identified metrics on the abstracted model for Agile and Lean development. We conclude that requirements-associated metrics are underrepresented in the literature and most of the metrics focuses on the time aspect rather than the quality aspect.},
  isbn = {978-1-4503-4853-9},
  keywords = {agile and lean,literature review,metrics or measures,requirements engineering},
  file = {C:\Users\ron\Zotero\storage\R6T34876\Wnuk and Maddila - 2017 - Agile and lean metrics associated with requirement.pdf}
}

@article{xiaoLanguageModelingSyntaxBased2011,
  title = {Language {{Modeling}} for {{Syntax-Based Machine Translation Using Tree Substitution Grammars}}: {{A Case Study}} on {{Chinese-English Translation}}},
  shorttitle = {Language {{Modeling}} for {{Syntax-Based Machine Translation Using Tree Substitution Grammars}}},
  author = {Xiao, Tong and Zhu, Jingbo and Zhu, Muhua},
  date = {2011-12-01},
  journaltitle = {ACM Transactions on Asian Language Information Processing},
  shortjournal = {ACM Transactions on Asian Language Information Processing},
  volume = {10},
  number = {4},
  pages = {18:1--18:29},
  issn = {1530-0226},
  doi = {10.1145/2025384.2025386},
  url = {https://doi.org/10.1145/2025384.2025386},
  urldate = {2023-08-14},
  abstract = {The poor grammatical output of Machine Translation (MT) systems appeals syntax-based approaches within language modeling. However, previous studies showed that syntax-based language modeling using (Context-Free) Treebank Grammars was not very helpful in improving BLEU scores for Chinese-English machine translation. In this article we further study this issue in the context of Chinese-English syntax-based Statistical Machine Translation (SMT) where Synchronous Tree Substitution Grammars (STSGs) are utilized to model the translation process. In particular, we develop a Tree Substitution Grammar-based language model for syntax-based MT, and present three methods to efficiently integrate the proposed language model into MT decoding. In addition, we design a simple and effective method to adapt syntax-based language models for MT tasks. We demonstrate that the proposed methods are able to benefit a state-of-the-art syntax-based MT system. On the NIST Chinese-English MT evaluation corpora, we finally achieve an improvement of 0.6 BLEU points over the baseline.},
  keywords = {Machine translation,syntax-based language model,tree substitution grammar},
  file = {C:\Users\ron\Zotero\storage\6IVEPU68\2025384.2025386.pdf}
}

@inproceedings{xiongEnhancingLanguageModels2011,
  title = {Enhancing Language Models in Statistical Machine Translation with Backward N-Grams and Mutual Information Triggers},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} - {{Volume}} 1},
  author = {Xiong, Deyi and Zhang, Min and Li, Haizhou},
  date = {2011-06-19},
  series = {{{HLT}} '11},
  pages = {1288--1297},
  publisher = {{Association for Computational Linguistics}},
  location = {{USA}},
  abstract = {In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline.},
  isbn = {978-1-932432-87-9},
  file = {C:\Users\ron\Zotero\storage\SB3WAUHM\2002472.2002633.pdf}
}

@inproceedings{xoxaImplementationsFileLocking2015,
  title = {Implementations of {{File Locking Mechanisms}}, {{Linux}} and {{Windows}}},
  booktitle = {2015 12th {{International Conference}} on {{Information Technology}} - {{New Generations}}},
  author = {Xoxa, Nevila and Mehilli, Aldiger and Tafa, Igli and Fejzaj, Julian},
  date = {2015-04},
  pages = {756--757},
  doi = {10.1109/ITNG.2015.130},
  abstract = {The world of Operating Systems has evolved a lot, especially these late years, mainly due to technology in hardware, but also because of ever growing user and market requirements. Anyway, some basic concepts and implementations are still stuck around like they're irreplaceable. File locking is one of them. The point is, why in a range of more than 15 years, file locking mechanisms have little or not at all evolved, while the technology in general or OS-s particularly have changed a lot? That's why we'll discuss File locking mechanisms in Windows and Linux, and possibly find a better solution combining Linux Advisory Locking and Windows Mandatory Locking. The goal is to make this mechanism robust, and more important: cross-OS portable.},
  eventtitle = {2015 12th {{International Conference}} on {{Information Technology}} - {{New Generations}}},
  keywords = {Advisory Locking,Concurrent computing,Concurrent File Access,cross-OS locking mechanism,File systems,File-Locking mechanism,Instruction sets,Java,JVM-to-native locking,Kernel,Linux,Mandatory Locking,portable locking,Thread-level file-locking},
  file = {C\:\\Users\\ron\\Zotero\\storage\\JAZAGIEK\\Xoxa et al. - 2015 - Implementations of File Locking Mechanisms, Linux .pdf;C\:\\Users\\ron\\Zotero\\storage\\I6Z59YD8\\7113569.html}
}

@inproceedings{xuHandsonDigitalForensic2023,
  title = {A {{Hands-on Digital Forensic Lab}} to {{Investigate Morris Worm Attack}}},
  booktitle = {Proceedings of the 54th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 2},
  author = {Xu, Eric and Xu, Alex S. and Ferreira, Danny and Deng, Lin},
  date = {2023-03-06},
  series = {{{SIGCSE}} 2023},
  pages = {1283},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3545947.3576229},
  url = {https://dl.acm.org/doi/10.1145/3545947.3576229},
  urldate = {2023-06-05},
  abstract = {We have developed a hands-on digital forensic lab to investigate the Morris Worm attack. In the poster, after the attack, we demonstrate a systematic approach to reconstructing the attack scenario by analyzing the worm's running processes, the networking communication used by running processes, and metadata of the files left on victims' machines.},
  isbn = {978-1-4503-9433-8},
  keywords = {cybersecurity,digital forensics,morris worm attack},
  file = {C:\Users\ron\Zotero\storage\U8G5PY5I\Xu et al. - 2023 - A Hands-on Digital Forensic Lab to Investigate Mor.pdf}
}

@inproceedings{yaghmourMeasuringCharacterizingSystem2000,
  title = {Measuring and Characterizing System Behavior Using Kernel-Level Event Logging},
  booktitle = {Proceedings of 2000 {{USENIX Annual Technical Conference}}},
  author = {Yaghmour, Karmin and Dagenais, Michael R.},
  date = {2000-06},
  publisher = {{USENIX}},
  location = {{San Diego, CA}},
  url = {https://dl.acm.org/doi/10.5555/1267724.1267726},
  abstract = {Analyzing the dynamic behavior and performance of complex software systems is diffcult. Currently available systems either analyze each process in isolation, only provide system level cumulative statistics, or provide a fixed and limited number of process group related statistics. The Linux Trace Toolkit (LTT) introduced here provides a novel, modular, and extensible way of recording and analyzing complete system behavior. Because all significant system events are recorded, it is possible to analyze any desired subset of the running processes, and for instance distinguish between the time spent waiting for some relevant event (data from disk or another process) versus time spent waiting for some unrelated process to use up its time slice. Despite the extensive information gathered, experimental results show that the LTT time and memory overhead is minimal ({$<$} 2:5\% when observing core kernel events). Moreover, due to the LTT and Linux kernel modularity and open source code availability, the system is easily extended both in terms of system events gathered, and of later post-processing and graphical presentation.},
  eventtitle = {{{ATEC}} '00},
  file = {C:\Users\ron\Zotero\storage\BBKJZBGX\Yaghmour and Dagenais - 2000 - Measuring and characterizing system behavior using.pdf}
}

@inproceedings{yanuarifianiAutomatingBusinessProcess2019,
  title = {Automating {{Business Process Model Generation}} from {{Ontology-based Requirements}}},
  booktitle = {Proceedings of the 2019 8th {{International Conference}} on {{Software}} and {{Computer Applications}}},
  author = {Yanuarifiani, Amarilis Putri and Chua, Fang-Fang and Chan, Gaik-Yee},
  date = {2019-02-19},
  series = {{{ICSCA}} '19},
  pages = {205--209},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3316615.3316683},
  url = {https://doi.org/10.1145/3316615.3316683},
  urldate = {2022-10-22},
  abstract = {Requirements elicitation process faces major challenges about how stakeholders can easily verify requirements. Requirements document allows developers to visualize requirements using modeling language to ensure stakeholders have the same perspective as them. It is also effective to give presentations to stakeholders about how business processes will be carried out after the requirements are implemented. Issues are raised in building requirements modeling as business users generally do not have enough knowledge to build requirements models in specific notations. Transforming requirements (natural language) into semi-formal notation (BPMN) manually lead to inconsistency of elements structure. The need to automatically generate requirements model become crucial because it will be the basis for the programming process. Existing studies are mostly concerned on auto-completion of modeling language using domain ontology as basic knowledge, and let the stakeholders building initial requirements model with limited knowledge. The idea of this paper is to propose a methodology for building business process model in semi-formal language (BPMN) to represent future business processes using ontology approach. This research continues from previous study which transform requirements list into requirements ontology to formalize the elements such as problem, actor and process. By using requirements ontology as input, rule-based mapping method is proposed to map ontology instances to BPMN elements.},
  isbn = {978-1-4503-6573-4},
  keywords = {auto-generate BPMN,ontology-based requirements,Semi-formal modeling},
  file = {C:\Users\ron\Zotero\storage\AYQK6KZK\Yanuarifiani et al. - 2019 - Automating Business Process Model Generation from .pdf}
}

@online{YoctoProjectIt,
  title = {Yocto {{Project}} – {{It}}'s Not an Embedded {{Linux}} Distribution – It Creates a Custom One for You},
  url = {https://www.yoctoproject.org/},
  urldate = {2022-06-18},
  langid = {american},
  file = {C:\Users\ron\Zotero\storage\6VXQ4RHT\www.yoctoproject.org.html}
}

@inproceedings{yonejiUnifiedHardwareAbstraction2018,
  title = {Unified Hardware Abstraction Layer with Device Masquerade},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Yoneji, Iori and Fukai, Takaaki and Shinagawa, Takahiro and Kato, Kazuhiko},
  date = {2018-04-09},
  pages = {1102--1108},
  publisher = {{ACM}},
  location = {{Pau France}},
  doi = {10.1145/3167132.3167250},
  url = {https://dl.acm.org/doi/10.1145/3167132.3167250},
  urldate = {2022-05-15},
  abstract = {Device drivers are a major concern for existing and new operating systems (OSs) in terms of the development cost and code quality. Unfortunately, the OS-dependent nature of device drivers makes their reuse or unification complicated because the execution environments of device drivers are tightly coupled with the hosting OS kernel. Previous studies of porting device drivers from major OSs suffer from various conflicts and engineering cost, and unmodified reuse of device drivers with virtual machines (VMs) incurs non-negligible overhead. In this paper, we present the design and implementation of a unified hardware abstraction layer that uses a thin hypervisor, on which an OS kernel runs with a single device driver for each device class, thereby reducing the development costs of device drivers for OSs that run on bare-metal machines. Our key technique is device masquerade; rather than virtualizing devices with fat software layers, a thin hypervisor converts physical devices into standardized abstract devices with minimum efforts. We exploit a de facto standard interface to hardware devices that allows clean separation of the abstraction layer implementation from OS kernels and easy deployment in practical use. To reduce virtualization overhead, the hypervisor supports only a single VM and allows pass-through access to already standardized devices such as interrupt controllers. The experimental results confirmed that the performance of our system was comparable to that on a bare-metal machine without any hypervisors.},
  eventtitle = {{{SAC}} 2018: {{Symposium}} on {{Applied Computing}}},
  isbn = {978-1-4503-5191-1},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\WM6IY8EH\Yoneji et al. - 2018 - Unified hardware abstraction layer with device mas.pdf}
}

@inproceedings{yoshinoRequirementsTraceabilityManagement2020,
  title = {Requirements {{Traceability Management Support Tool}} for {{UML Models}}},
  booktitle = {Proceedings of the 2020 9th {{International Conference}} on {{Software}} and {{Computer Applications}}},
  author = {Yoshino, Kaito and Matsuura, Saeko},
  date = {2020-02-18},
  series = {{{ICSCA}} 2020},
  pages = {163--166},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3384544.3384586},
  url = {https://doi.org/10.1145/3384544.3384586},
  urldate = {2022-10-22},
  abstract = {Traceability between system requirements and the artifacts produced by the software development process contributes to the development of reliable software. This paper proposes a method of ensuring traceability by automatically generating an initial sequence diagram from an activity diagram; the activity diagram not only defines user-system interactions, but it also describes the feasibility of use cases via data modeling in a class diagram. These sequence diagrams evolve through changes that are made to the particulars of a system design; these sequence diagrams can be used to generate source code. As a result, the requirements can be traced to the program codes.},
  isbn = {978-1-4503-7665-5},
  keywords = {Requirements,Requirements Traceability,UML modeling},
  file = {C:\Users\ron\Zotero\storage\ZEUHRJ9N\Yoshino and Matsuura - 2020 - Requirements Traceability Management Support Tool .pdf}
}

@inproceedings{yuModellingReasoningSupport1997,
  title = {Towards Modelling and Reasoning Support for Early-Phase Requirements Engineering},
  booktitle = {Proceedings of {{ISRE}} '97: 3rd {{IEEE International Symposium}} on {{Requirements Engineering}}},
  author = {Yu, E.S.K.},
  date = {1997-01},
  pages = {226--235},
  doi = {10.1109/ISRE.1997.566873},
  abstract = {Requirements are usually understood as stating what a system is supposed to do, as apposed to how it should do it. However, understanding the organizational context and rationales (the "Whys") that lead up to systems requirements can be just as important for the ongoing success of the system. Requirements modelling techniques can be used to help deal with the knowledge and reasoning needed in this earlier phase of requirements engineering. However most existing requirements techniques are intended more for the later phase of requirements engineering, which focuses on completeness, consistency, and automated verification of requirements. In contrast, the early phase aims to model and analyze stakeholder interests and how they might be addressed, or compromised, by various system-and-environment alternatives. This paper argues, therefore, that a different kind of modelling and reasoning support is needed for the early phase. An outline of the i* framework is given as an example of a step in this direction. Meeting scheduling is used as a domain example.},
  eventtitle = {Proceedings of {{ISRE}} '97: 3rd {{IEEE International Symposium}} on {{Requirements Engineering}}},
  keywords = {Acoustical engineering,Knowledge engineering,Personnel,Processor scheduling},
  file = {C\:\\Users\\ron\\Zotero\\storage\\3N48YU45\\Yu - 1997 - Towards modelling and reasoning support for early-.pdf;C\:\\Users\\ron\\Zotero\\storage\\QNEGCHDW\\566873.html}
}

@article{yuNoun2VerbProbabilisticFrame2022,
  title = {{{Noun2Verb}}: {{Probabilistic Frame Semantics}} for {{Word Class Conversion}}},
  shorttitle = {{{Noun2Verb}}},
  author = {Yu, Lei and Xu, Yang},
  date = {2022-12-01},
  journaltitle = {Computational Linguistics},
  volume = {48},
  number = {4},
  pages = {783--818},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00447},
  url = {https://direct.mit.edu/coli/article/48/4/783/111537/Noun2Verb-Probabilistic-Frame-Semantics-for-Word},
  urldate = {2023-08-14},
  abstract = {Abstract              Humans can flexibly extend word usages across different grammatical classes, a phenomenon known as word class conversion. Noun-to-verb conversion, or denominal verb (e.g., to Google a cheap flight), is one of the most prevalent forms of word class conversion. However, existing natural language processing systems are impoverished in interpreting and generating novel denominal verb usages. Previous work has suggested that novel denominal verb usages are comprehensible if the listener can compute the intended meaning based on shared knowledge with the speaker. Here we explore a computational formalism for this proposal couched in frame semantics. We present a formal framework, Noun2Verb, that simulates the production and comprehension of novel denominal verb usages by modeling shared knowledge of speaker and listener in semantic frames. We evaluate an incremental set of probabilistic models that learn to interpret and generate novel denominal verb usages via paraphrasing. We show that a model where the speaker and listener cooperatively learn the joint distribution over semantic frame elements better explains the empirical denominal verb usages than state-of-the-art language models, evaluated against data from (1) contemporary English in both adult and child speech, (2) contemporary Mandarin Chinese, and (3) the historical development of English. Our work grounds word class conversion in probabilistic frame semantics and bridges the gap between natural language processing systems and humans in lexical creativity.},
  langid = {english},
  file = {C:\Users\ron\Zotero\storage\IE7VQCQH\coli_a_00447.pdf}
}

@article{zadehFuzzySets1965,
  title = {Fuzzy {{Sets}}},
  author = {Zadeh, Lofti A},
  date = {1965},
  journaltitle = {Information and Control},
  volume = {8},
  number = {3},
  pages = {338--353}
}

@inproceedings{zhangAutomaticGenerationLoad2011,
  title = {Automatic Generation of Load Tests},
  booktitle = {Proceedings of the 2011 26th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Zhang, Pingyu and Elbaum, Sebastian and Dwyer, Matthew B.},
  date = {2011-11-06},
  series = {{{ASE}} '11},
  pages = {43--52},
  publisher = {{IEEE Computer Society}},
  location = {{USA}},
  doi = {10.1109/ASE.2011.6100093},
  url = {https://doi.org/10.1109/ASE.2011.6100093},
  urldate = {2022-06-18},
  abstract = {Load tests aim to validate whether system performance is acceptable under peak conditions. Existing test generation techniques induce load by increasing the size or rate of the input. Ignoring the particular input values, however, may lead to test suites that grossly mischaracterize a system's performance. To address this limitation we introduce a mixed symbolic execution based approach that is unique in how it 1) favors program paths associated with a performance measure of interest, 2) operates in an iterative-deepening beam-search fashion to discard paths that are unlikely to lead to high-load tests, and 3) generates a test suite of a given size and level of diversity. An assessment of the approach shows it generates test suites that induce program response times and memory consumption several times worse than the compared alternatives, it scales to large and complex inputs, and it exposes a diversity of resource consuming program behavior.},
  isbn = {978-1-4577-1638-6},
  file = {C:\Users\ron\Zotero\storage\BFCLTQJR\Zhang et al. - 2011 - Automatic generation of load tests.pdf}
}

@article{zhangChiralityLogicGates2022,
  title = {Chirality Logic Gates},
  author = {Zhang, Yi and Wang, Yadong and Dai, Yunyun and Bai, Xueyin and Hu, Xuerong and Du, Luojun and Hu, Hai and Yang, Xiaoxia and Li, Diao and Dai, Qing and Hasan, Tawfique and Sun, Zhipei},
  date = {2022-12-09},
  journaltitle = {Science Advances},
  volume = {8},
  number = {49},
  pages = {eabq8246},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/sciadv.abq8246},
  url = {https://www.science.org/doi/10.1126/sciadv.abq8246},
  urldate = {2022-12-14},
  abstract = {The ever-growing demand for faster and more efficient data transfer and processing has brought optical computation strategies to the forefront of research in next-generation computing. Here, we report a universal computing approach with the chirality degree of freedom. By exploiting the crystal symmetry–enabled well-known chiral selection rules, we demonstrate the viability of the concept in bulk silica crystals and atomically thin semiconductors and create ultrafast ({$<$}100-fs) all-optical chirality logic gates (XNOR, NOR, AND, XOR, OR, and NAND) and a half adder. We also validate the unique advantages of chirality gates by realizing multiple gates with simultaneous operation in a single device and electrical control. Our first demonstrations of logic gates using chiral selection rules suggest that optical chirality could provide a powerful degree of freedom for future optical computing.},
  file = {C:\Users\ron\Zotero\storage\Q8RFTUCL\Zhang et al. - 2022 - Chirality logic gates.pdf}
}

@inproceedings{zhangCompositionalLoadTest2012,
  title = {Compositional Load Test Generation for Software Pipelines},
  booktitle = {Proceedings of the 2012 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Zhang, Pingyu and Elbaum, Sebastian and Dwyer, Matthew B.},
  date = {2012-07-15},
  series = {{{ISSTA}} 2012},
  pages = {89--99},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2338965.2336764},
  url = {https://doi.org/10.1145/2338965.2336764},
  urldate = {2022-06-18},
  abstract = {Load tests validate whether a system’s performance is acceptable under extreme conditions. Traditional load testing approaches are black-box, inducing load by increasing the size or rate of the input. Symbolic execution based load testing techniques complement traditional approaches by enabling the selection of precise input values. However, as the programs under analysis or their required inputs increase in size, the analyses required by these techniques either fail to scale up or sacrifice test effectiveness. We propose a new approach that addresses this limitation by performing load test generation compositionally. It uses existing symbolic execution based techniques to analyze the performance of each system component in isolation, summarizes the results of those analyses, and then performs an analysis across those summaries to generate load tests for the whole system. In its current form, the approach can be applied to any system that is structured in the form of a software pipeline. A study of the approach revealed that it can generate effective load tests for Unix and XML pipelines while outperforming state-of-the-art techniques.},
  isbn = {978-1-4503-1454-1},
  file = {C:\Users\ron\Zotero\storage\CMYT8LEG\Zhang et al. - 2012 - Compositional load test generation for software pi.pdf}
}

@article{zhaoNaturalLanguageProcessing2021,
  title = {Natural {{Language Processing}} for {{Requirements Engineering}}: {{A Systematic Mapping Study}}},
  shorttitle = {Natural {{Language Processing}} for {{Requirements Engineering}}},
  author = {Zhao, Liping and Alhoshan, Waad and Ferrari, Alessio and Letsholo, Keletso J. and Ajagbe, Muideen A. and Chioasca, Erol-Valeriu and Batista-Navarro, Riza T.},
  date = {2021-04-17},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {3},
  pages = {55:1--55:41},
  issn = {0360-0300},
  doi = {10.1145/3444689},
  url = {https://doi.org/10.1145/3444689},
  urldate = {2022-10-22},
  abstract = {Natural Language Processing for Requirements Engineering (NLP4RE) is an area of research and development that seeks to apply natural language processing (NLP) techniques, tools, and resources to the requirements engineering (RE) process, to support human analysts to carry out various linguistic analysis tasks on textual requirements documents, such as detecting language issues, identifying key domain concepts, and establishing requirements traceability links. This article reports on a mapping study that surveys the landscape of NLP4RE research to provide a holistic understanding of the field. Following the guidance of systematic review, the mapping study is directed by five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of tool development, and the usage of NLP technologies. Our main results are as follows: (i) we identify a total of 404 primary studies relevant to NLP4RE, which were published over the past 36 years and from 170 different venues; (ii) most of these studies (67.08\%) are solution proposals, assessed by a laboratory experiment or an example application, while only a small percentage (7\%) are assessed in industrial settings; (iii) a large proportion of the studies (42.70\%) focus on the requirements analysis phase, with quality defect detection as their central task and requirements specification as their commonly processed document type; (iv) 130 NLP4RE tools (i.e., RE specific NLP tools) are extracted from these studies, but only 17 of them (13.08\%) are available for download; (v) 231 different NLP technologies are also identified, comprising 140 NLP techniques, 66 NLP tools, and 25 NLP resources, but most of them—particularly those novel NLP techniques and specialized tools—are used infrequently; by contrast, commonly used NLP technologies are traditional analysis techniques (e.g., POS tagging and tokenization), general-purpose tools (e.g., Stanford CoreNLP and GATE) and generic language lexicons (WordNet and British National Corpus). The mapping study not only provides a collection of the literature in NLP4RE but also, more importantly, establishes a structure to frame the existing literature through categorization, synthesis and conceptualization of the main theoretical concepts and relationships that encompass both RE and NLP aspects. Our work thus produces a conceptual framework of NLP4RE. The framework is used to identify research gaps and directions, highlight technology transfer needs, and encourage more synergies between the RE community, the NLP one, and the software and systems practitioners. Our results can be used as a starting point to frame future studies according to a well-defined terminology and can be expanded as new technologies and novel solutions emerge.},
  keywords = {natural language processing (NLP),Requirements engineering (RE),software engineering (SE),systematic mapping study,systematic review},
  file = {C:\Users\ron\Zotero\storage\V38RW6JF\Zhao et al. - 2021 - Natural Language Processing for Requirements Engin.pdf}
}

@article{zhouHeadDrivenPhraseStructure2019,
  title = {Head-{{Driven Phrase Structure Grammar Parsing}} on {{Penn Treebank}}},
  author = {Zhou, Junru and Zhao, Hai},
  date = {2019},
  doi = {10.48550/ARXIV.1907.02684},
  url = {https://arxiv.org/abs/1907.02684},
  urldate = {2023-10-23},
  abstract = {Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 96.33 F1 of constituent parsing and 97.20\textbackslash\% UAS of dependency parsing on PTB.},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C:\Users\ron\Zotero\storage\5I8BR294\1907.02684.pdf}
}

@book{ziloginc.Z8000CPUTechnical1982,
  title = {Z8000 {{CPU Technical Manual}}},
  author = {{Zilog, Inc.}},
  date = {1982},
  url = {http://www.bitsavers.org/components/zilog/z8000/Z8000_CPUrefMan_1982.pdf},
  file = {C\:\\Users\\ron\\Zotero\\storage\\C6KBHSTH\\Z8000_CPUrefMan_1982.pdf;C\:\\Users\\ron\\Zotero\\storage\\QMM9YXKM\\Z8000Tech.pdf}
}
